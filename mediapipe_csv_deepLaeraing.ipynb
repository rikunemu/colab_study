{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mediapipe_csv_deepLaeraing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqScAydBdqDBODWBr+vkDK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikunemu/colab_study/blob/main/mediapipe_csv_deepLaeraing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12_PCa7i1Bzr"
      },
      "source": [
        "#  mediapipeで取った特徴点を分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rptBUegwvb1"
      },
      "source": [
        "90分対策  \n",
        "F12を開いて以下のスクリプトをコンソールで実行  \n",
        "function KeepClicking(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(KeepClicking,600000)\n",
        "\n",
        "  \n",
        "10分ごとに接続"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQzvztFP513g",
        "outputId": "9a8fa93e-c3a0-412b-af93-68e1bcc38e97"
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import os\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import utils as np_utils\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3KcXQxqltwZ"
      },
      "source": [
        "## csvデータ読み込み\n",
        "train,test,validそれぞれ7感情の特徴点を記録"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYz-88dFlO4e"
      },
      "source": [
        "fp_train='/content/drive/MyDrive/data分析/Mediapipe/csvtraintotal.csv'\n",
        "df_train=pd.read_csv(fp_train)\n",
        "fp_test='/content/drive/MyDrive/data分析/Mediapipe/csvtesttotal.csv'\n",
        "df_test=pd.read_csv(fp_test)\n",
        "fp_valid='/content/drive/MyDrive/data分析/Mediapipe/csvvalidtotal.csv'\n",
        "df_valid=pd.read_csv(fp_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "OnuDyefiX3F1",
        "outputId": "84b63127-f3ac-4ede-c3be-5bdc2259e83f"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "      <td>8108</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "      <td>8109</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "      <td>8110</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>8111</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "      <td>8112</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.470799  0.682127 -0.080930  ... -0.045183           0        0\n",
              "1      0.469567  0.799870 -0.017644  ... -0.206077           3        0\n",
              "2      0.711111  0.717369 -0.049849  ... -0.146556           4        0\n",
              "3      0.483527  0.809326 -0.011760  ... -0.195280           5        0\n",
              "4      0.061688  0.692855 -0.050553  ... -0.058228           6        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "82190  0.436367  0.787778 -0.124726  ... -0.051585        8108        6\n",
              "82191  0.456891  0.704518 -0.093469  ... -0.061855        8109        6\n",
              "82192  0.502264  0.729350 -0.124422  ... -0.034920        8110        6\n",
              "82193  0.539922  0.733853 -0.116346  ... -0.023272        8111        6\n",
              "82194  0.508446  0.805487 -0.070251  ... -0.153780        8112        6\n",
              "\n",
              "[82195 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "IwOLXOUD1NKy",
        "outputId": "51e7c3b8-11cf-4435-9928-b66e6fcd8ac3"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.516601</td>\n",
              "      <td>0.585682</td>\n",
              "      <td>-0.128171</td>\n",
              "      <td>0.354741</td>\n",
              "      <td>0.407085</td>\n",
              "      <td>-0.017539</td>\n",
              "      <td>0.303413</td>\n",
              "      <td>0.437223</td>\n",
              "      <td>-0.022323</td>\n",
              "      <td>0.385631</td>\n",
              "      <td>0.474988</td>\n",
              "      <td>-0.077427</td>\n",
              "      <td>0.192280</td>\n",
              "      <td>0.117430</td>\n",
              "      <td>0.133606</td>\n",
              "      <td>0.227670</td>\n",
              "      <td>0.153081</td>\n",
              "      <td>0.075319</td>\n",
              "      <td>0.261869</td>\n",
              "      <td>0.196746</td>\n",
              "      <td>0.025099</td>\n",
              "      <td>0.384131</td>\n",
              "      <td>0.734853</td>\n",
              "      <td>-0.092528</td>\n",
              "      <td>0.409821</td>\n",
              "      <td>0.185824</td>\n",
              "      <td>-0.000665</td>\n",
              "      <td>0.388524</td>\n",
              "      <td>0.114111</td>\n",
              "      <td>0.043742</td>\n",
              "      <td>0.366814</td>\n",
              "      <td>0.051379</td>\n",
              "      <td>0.084901</td>\n",
              "      <td>0.466509</td>\n",
              "      <td>0.041154</td>\n",
              "      <td>0.091456</td>\n",
              "      <td>0.247168</td>\n",
              "      <td>0.348231</td>\n",
              "      <td>0.038491</td>\n",
              "      <td>0.162550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.673695</td>\n",
              "      <td>-0.099631</td>\n",
              "      <td>0.486085</td>\n",
              "      <td>0.227604</td>\n",
              "      <td>-0.000800</td>\n",
              "      <td>0.411432</td>\n",
              "      <td>0.685160</td>\n",
              "      <td>-0.107264</td>\n",
              "      <td>0.403327</td>\n",
              "      <td>0.699680</td>\n",
              "      <td>-0.105119</td>\n",
              "      <td>0.365605</td>\n",
              "      <td>0.599879</td>\n",
              "      <td>-0.087972</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>0.566646</td>\n",
              "      <td>0.268614</td>\n",
              "      <td>0.517140</td>\n",
              "      <td>0.487549</td>\n",
              "      <td>-0.123952</td>\n",
              "      <td>0.407326</td>\n",
              "      <td>0.672411</td>\n",
              "      <td>-0.080824</td>\n",
              "      <td>0.400786</td>\n",
              "      <td>0.675086</td>\n",
              "      <td>-0.085923</td>\n",
              "      <td>0.467047</td>\n",
              "      <td>0.510292</td>\n",
              "      <td>-0.103923</td>\n",
              "      <td>0.406921</td>\n",
              "      <td>0.514333</td>\n",
              "      <td>-0.071427</td>\n",
              "      <td>0.460763</td>\n",
              "      <td>0.503372</td>\n",
              "      <td>-0.100683</td>\n",
              "      <td>0.483841</td>\n",
              "      <td>0.187230</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.350142</td>\n",
              "      <td>0.704083</td>\n",
              "      <td>-0.051000</td>\n",
              "      <td>0.258494</td>\n",
              "      <td>0.467294</td>\n",
              "      <td>0.006417</td>\n",
              "      <td>0.210762</td>\n",
              "      <td>0.484544</td>\n",
              "      <td>0.028842</td>\n",
              "      <td>0.256672</td>\n",
              "      <td>0.560486</td>\n",
              "      <td>-0.029805</td>\n",
              "      <td>0.163514</td>\n",
              "      <td>0.115912</td>\n",
              "      <td>0.038936</td>\n",
              "      <td>0.169306</td>\n",
              "      <td>0.169693</td>\n",
              "      <td>-0.000869</td>\n",
              "      <td>0.178214</td>\n",
              "      <td>0.229622</td>\n",
              "      <td>-0.030477</td>\n",
              "      <td>0.262613</td>\n",
              "      <td>0.771681</td>\n",
              "      <td>0.078652</td>\n",
              "      <td>0.294888</td>\n",
              "      <td>0.263658</td>\n",
              "      <td>-0.103125</td>\n",
              "      <td>0.291660</td>\n",
              "      <td>0.172433</td>\n",
              "      <td>-0.100819</td>\n",
              "      <td>0.290796</td>\n",
              "      <td>0.086691</td>\n",
              "      <td>-0.094484</td>\n",
              "      <td>0.382622</td>\n",
              "      <td>0.088537</td>\n",
              "      <td>-0.127045</td>\n",
              "      <td>0.190766</td>\n",
              "      <td>0.370796</td>\n",
              "      <td>0.059996</td>\n",
              "      <td>0.127881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.731658</td>\n",
              "      <td>0.042865</td>\n",
              "      <td>0.366543</td>\n",
              "      <td>0.318007</td>\n",
              "      <td>-0.107573</td>\n",
              "      <td>0.279443</td>\n",
              "      <td>0.738324</td>\n",
              "      <td>0.041889</td>\n",
              "      <td>0.274230</td>\n",
              "      <td>0.747920</td>\n",
              "      <td>0.050578</td>\n",
              "      <td>0.236568</td>\n",
              "      <td>0.660978</td>\n",
              "      <td>0.032212</td>\n",
              "      <td>0.120189</td>\n",
              "      <td>0.495042</td>\n",
              "      <td>0.403236</td>\n",
              "      <td>0.337132</td>\n",
              "      <td>0.624860</td>\n",
              "      <td>-0.094728</td>\n",
              "      <td>0.281208</td>\n",
              "      <td>0.722307</td>\n",
              "      <td>0.068394</td>\n",
              "      <td>0.274025</td>\n",
              "      <td>0.721999</td>\n",
              "      <td>0.067390</td>\n",
              "      <td>0.308012</td>\n",
              "      <td>0.624537</td>\n",
              "      <td>-0.054114</td>\n",
              "      <td>0.268657</td>\n",
              "      <td>0.601183</td>\n",
              "      <td>-0.016952</td>\n",
              "      <td>0.302620</td>\n",
              "      <td>0.616359</td>\n",
              "      <td>-0.057269</td>\n",
              "      <td>0.366551</td>\n",
              "      <td>0.274136</td>\n",
              "      <td>-0.123009</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.478097</td>\n",
              "      <td>0.829416</td>\n",
              "      <td>-0.035864</td>\n",
              "      <td>0.338219</td>\n",
              "      <td>0.586676</td>\n",
              "      <td>-0.044701</td>\n",
              "      <td>0.281508</td>\n",
              "      <td>0.608822</td>\n",
              "      <td>-0.030072</td>\n",
              "      <td>0.362336</td>\n",
              "      <td>0.690208</td>\n",
              "      <td>-0.055267</td>\n",
              "      <td>0.158889</td>\n",
              "      <td>0.208705</td>\n",
              "      <td>-0.127339</td>\n",
              "      <td>0.195300</td>\n",
              "      <td>0.286799</td>\n",
              "      <td>-0.150213</td>\n",
              "      <td>0.229269</td>\n",
              "      <td>0.368802</td>\n",
              "      <td>-0.164157</td>\n",
              "      <td>0.357517</td>\n",
              "      <td>0.888281</td>\n",
              "      <td>0.091781</td>\n",
              "      <td>0.391913</td>\n",
              "      <td>0.394345</td>\n",
              "      <td>-0.200909</td>\n",
              "      <td>0.374054</td>\n",
              "      <td>0.287385</td>\n",
              "      <td>-0.220581</td>\n",
              "      <td>0.355048</td>\n",
              "      <td>0.184166</td>\n",
              "      <td>-0.231421</td>\n",
              "      <td>0.476613</td>\n",
              "      <td>0.184120</td>\n",
              "      <td>-0.238496</td>\n",
              "      <td>0.228681</td>\n",
              "      <td>0.488865</td>\n",
              "      <td>-0.035819</td>\n",
              "      <td>0.134504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.857773</td>\n",
              "      <td>0.046278</td>\n",
              "      <td>0.478034</td>\n",
              "      <td>0.444558</td>\n",
              "      <td>-0.172615</td>\n",
              "      <td>0.386257</td>\n",
              "      <td>0.865752</td>\n",
              "      <td>0.048123</td>\n",
              "      <td>0.378545</td>\n",
              "      <td>0.874142</td>\n",
              "      <td>0.057897</td>\n",
              "      <td>0.340892</td>\n",
              "      <td>0.793487</td>\n",
              "      <td>0.010045</td>\n",
              "      <td>0.054897</td>\n",
              "      <td>0.550742</td>\n",
              "      <td>0.351424</td>\n",
              "      <td>0.478415</td>\n",
              "      <td>0.751673</td>\n",
              "      <td>-0.092016</td>\n",
              "      <td>0.385269</td>\n",
              "      <td>0.849144</td>\n",
              "      <td>0.059494</td>\n",
              "      <td>0.379350</td>\n",
              "      <td>0.852295</td>\n",
              "      <td>0.059011</td>\n",
              "      <td>0.434535</td>\n",
              "      <td>0.750683</td>\n",
              "      <td>-0.052339</td>\n",
              "      <td>0.380534</td>\n",
              "      <td>0.728616</td>\n",
              "      <td>-0.024325</td>\n",
              "      <td>0.428548</td>\n",
              "      <td>0.743129</td>\n",
              "      <td>-0.058275</td>\n",
              "      <td>0.478435</td>\n",
              "      <td>0.401297</td>\n",
              "      <td>-0.201465</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.440407</td>\n",
              "      <td>0.633268</td>\n",
              "      <td>-0.130409</td>\n",
              "      <td>0.313094</td>\n",
              "      <td>0.456206</td>\n",
              "      <td>0.001129</td>\n",
              "      <td>0.255751</td>\n",
              "      <td>0.477937</td>\n",
              "      <td>0.012412</td>\n",
              "      <td>0.327564</td>\n",
              "      <td>0.527424</td>\n",
              "      <td>-0.067629</td>\n",
              "      <td>0.187209</td>\n",
              "      <td>0.126621</td>\n",
              "      <td>0.163992</td>\n",
              "      <td>0.209704</td>\n",
              "      <td>0.178016</td>\n",
              "      <td>0.096623</td>\n",
              "      <td>0.230991</td>\n",
              "      <td>0.237480</td>\n",
              "      <td>0.042394</td>\n",
              "      <td>0.312069</td>\n",
              "      <td>0.801668</td>\n",
              "      <td>0.008883</td>\n",
              "      <td>0.372335</td>\n",
              "      <td>0.248090</td>\n",
              "      <td>-0.028703</td>\n",
              "      <td>0.360934</td>\n",
              "      <td>0.151648</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.349558</td>\n",
              "      <td>0.068262</td>\n",
              "      <td>0.044676</td>\n",
              "      <td>0.458189</td>\n",
              "      <td>0.061373</td>\n",
              "      <td>0.022868</td>\n",
              "      <td>0.221603</td>\n",
              "      <td>0.385956</td>\n",
              "      <td>0.085039</td>\n",
              "      <td>0.143525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.774976</td>\n",
              "      <td>-0.017430</td>\n",
              "      <td>0.452601</td>\n",
              "      <td>0.292870</td>\n",
              "      <td>-0.042988</td>\n",
              "      <td>0.331663</td>\n",
              "      <td>0.783315</td>\n",
              "      <td>-0.020361</td>\n",
              "      <td>0.322092</td>\n",
              "      <td>0.792106</td>\n",
              "      <td>-0.013262</td>\n",
              "      <td>0.298874</td>\n",
              "      <td>0.636628</td>\n",
              "      <td>-0.034253</td>\n",
              "      <td>0.112172</td>\n",
              "      <td>0.549183</td>\n",
              "      <td>0.409472</td>\n",
              "      <td>0.433293</td>\n",
              "      <td>0.558691</td>\n",
              "      <td>-0.145084</td>\n",
              "      <td>0.341583</td>\n",
              "      <td>0.752286</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.752438</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.395023</td>\n",
              "      <td>0.569501</td>\n",
              "      <td>-0.106214</td>\n",
              "      <td>0.345461</td>\n",
              "      <td>0.563145</td>\n",
              "      <td>-0.061551</td>\n",
              "      <td>0.390786</td>\n",
              "      <td>0.563328</td>\n",
              "      <td>-0.106485</td>\n",
              "      <td>0.452236</td>\n",
              "      <td>0.249019</td>\n",
              "      <td>-0.042549</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.508843</td>\n",
              "      <td>0.783509</td>\n",
              "      <td>-0.075715</td>\n",
              "      <td>0.342278</td>\n",
              "      <td>0.582771</td>\n",
              "      <td>-0.031950</td>\n",
              "      <td>0.289867</td>\n",
              "      <td>0.611026</td>\n",
              "      <td>-0.020946</td>\n",
              "      <td>0.375631</td>\n",
              "      <td>0.676300</td>\n",
              "      <td>-0.067914</td>\n",
              "      <td>0.148420</td>\n",
              "      <td>0.218699</td>\n",
              "      <td>-0.009825</td>\n",
              "      <td>0.183308</td>\n",
              "      <td>0.281258</td>\n",
              "      <td>-0.048717</td>\n",
              "      <td>0.216023</td>\n",
              "      <td>0.349559</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>0.396198</td>\n",
              "      <td>0.916336</td>\n",
              "      <td>0.045910</td>\n",
              "      <td>0.370082</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>-0.122842</td>\n",
              "      <td>0.345563</td>\n",
              "      <td>0.249492</td>\n",
              "      <td>-0.119133</td>\n",
              "      <td>0.317737</td>\n",
              "      <td>0.152755</td>\n",
              "      <td>-0.109723</td>\n",
              "      <td>0.428981</td>\n",
              "      <td>0.135849</td>\n",
              "      <td>-0.117880</td>\n",
              "      <td>0.226637</td>\n",
              "      <td>0.504523</td>\n",
              "      <td>0.004935</td>\n",
              "      <td>0.154398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.875826</td>\n",
              "      <td>0.011859</td>\n",
              "      <td>0.461890</td>\n",
              "      <td>0.394244</td>\n",
              "      <td>-0.109819</td>\n",
              "      <td>0.415073</td>\n",
              "      <td>0.887762</td>\n",
              "      <td>0.011303</td>\n",
              "      <td>0.407975</td>\n",
              "      <td>0.899817</td>\n",
              "      <td>0.019094</td>\n",
              "      <td>0.362383</td>\n",
              "      <td>0.781520</td>\n",
              "      <td>-0.017575</td>\n",
              "      <td>0.112559</td>\n",
              "      <td>0.630753</td>\n",
              "      <td>0.341884</td>\n",
              "      <td>0.502403</td>\n",
              "      <td>0.719198</td>\n",
              "      <td>-0.114545</td>\n",
              "      <td>0.415689</td>\n",
              "      <td>0.858130</td>\n",
              "      <td>0.028379</td>\n",
              "      <td>0.405933</td>\n",
              "      <td>0.861435</td>\n",
              "      <td>0.026839</td>\n",
              "      <td>0.456500</td>\n",
              "      <td>0.724646</td>\n",
              "      <td>-0.078316</td>\n",
              "      <td>0.398571</td>\n",
              "      <td>0.712659</td>\n",
              "      <td>-0.046950</td>\n",
              "      <td>0.450282</td>\n",
              "      <td>0.718590</td>\n",
              "      <td>-0.081530</td>\n",
              "      <td>0.455816</td>\n",
              "      <td>0.346828</td>\n",
              "      <td>-0.125290</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14727</th>\n",
              "      <td>0.560496</td>\n",
              "      <td>0.769144</td>\n",
              "      <td>-0.111749</td>\n",
              "      <td>0.395681</td>\n",
              "      <td>0.565257</td>\n",
              "      <td>-0.017007</td>\n",
              "      <td>0.342978</td>\n",
              "      <td>0.594650</td>\n",
              "      <td>-0.008701</td>\n",
              "      <td>0.431437</td>\n",
              "      <td>0.644474</td>\n",
              "      <td>-0.072280</td>\n",
              "      <td>0.212711</td>\n",
              "      <td>0.199529</td>\n",
              "      <td>0.095772</td>\n",
              "      <td>0.235907</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>0.043887</td>\n",
              "      <td>0.261247</td>\n",
              "      <td>0.286265</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>0.444982</td>\n",
              "      <td>0.899928</td>\n",
              "      <td>-0.032473</td>\n",
              "      <td>0.421535</td>\n",
              "      <td>0.284107</td>\n",
              "      <td>-0.043228</td>\n",
              "      <td>0.409811</td>\n",
              "      <td>0.212278</td>\n",
              "      <td>-0.018695</td>\n",
              "      <td>0.392661</td>\n",
              "      <td>0.132065</td>\n",
              "      <td>0.010029</td>\n",
              "      <td>0.507709</td>\n",
              "      <td>0.118564</td>\n",
              "      <td>0.007106</td>\n",
              "      <td>0.279881</td>\n",
              "      <td>0.494782</td>\n",
              "      <td>0.037404</td>\n",
              "      <td>0.207185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.865063</td>\n",
              "      <td>-0.052868</td>\n",
              "      <td>0.521913</td>\n",
              "      <td>0.335642</td>\n",
              "      <td>-0.041206</td>\n",
              "      <td>0.468909</td>\n",
              "      <td>0.876704</td>\n",
              "      <td>-0.059213</td>\n",
              "      <td>0.459812</td>\n",
              "      <td>0.887839</td>\n",
              "      <td>-0.054344</td>\n",
              "      <td>0.422744</td>\n",
              "      <td>0.770018</td>\n",
              "      <td>-0.057373</td>\n",
              "      <td>0.168215</td>\n",
              "      <td>0.636903</td>\n",
              "      <td>0.338159</td>\n",
              "      <td>0.546422</td>\n",
              "      <td>0.660709</td>\n",
              "      <td>-0.123501</td>\n",
              "      <td>0.473307</td>\n",
              "      <td>0.854591</td>\n",
              "      <td>-0.037045</td>\n",
              "      <td>0.464889</td>\n",
              "      <td>0.859350</td>\n",
              "      <td>-0.040605</td>\n",
              "      <td>0.506149</td>\n",
              "      <td>0.682234</td>\n",
              "      <td>-0.094430</td>\n",
              "      <td>0.455463</td>\n",
              "      <td>0.684035</td>\n",
              "      <td>-0.061109</td>\n",
              "      <td>0.500817</td>\n",
              "      <td>0.675388</td>\n",
              "      <td>-0.092881</td>\n",
              "      <td>0.517969</td>\n",
              "      <td>0.281741</td>\n",
              "      <td>-0.043033</td>\n",
              "      <td>1470</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14728</th>\n",
              "      <td>0.491013</td>\n",
              "      <td>0.712008</td>\n",
              "      <td>-0.122762</td>\n",
              "      <td>0.370838</td>\n",
              "      <td>0.521274</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.321248</td>\n",
              "      <td>0.549112</td>\n",
              "      <td>0.029226</td>\n",
              "      <td>0.397335</td>\n",
              "      <td>0.598340</td>\n",
              "      <td>-0.050178</td>\n",
              "      <td>0.231024</td>\n",
              "      <td>0.155950</td>\n",
              "      <td>0.198852</td>\n",
              "      <td>0.244556</td>\n",
              "      <td>0.193912</td>\n",
              "      <td>0.136056</td>\n",
              "      <td>0.262082</td>\n",
              "      <td>0.232480</td>\n",
              "      <td>0.084471</td>\n",
              "      <td>0.410388</td>\n",
              "      <td>0.861402</td>\n",
              "      <td>-0.035873</td>\n",
              "      <td>0.396322</td>\n",
              "      <td>0.229427</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.388710</td>\n",
              "      <td>0.161868</td>\n",
              "      <td>0.052182</td>\n",
              "      <td>0.378511</td>\n",
              "      <td>0.085723</td>\n",
              "      <td>0.091270</td>\n",
              "      <td>0.480310</td>\n",
              "      <td>0.072318</td>\n",
              "      <td>0.071249</td>\n",
              "      <td>0.277920</td>\n",
              "      <td>0.448965</td>\n",
              "      <td>0.099246</td>\n",
              "      <td>0.215149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820451</td>\n",
              "      <td>-0.054481</td>\n",
              "      <td>0.485971</td>\n",
              "      <td>0.284793</td>\n",
              "      <td>-0.002835</td>\n",
              "      <td>0.426605</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>-0.060726</td>\n",
              "      <td>0.418768</td>\n",
              "      <td>0.843263</td>\n",
              "      <td>-0.056251</td>\n",
              "      <td>0.383449</td>\n",
              "      <td>0.718743</td>\n",
              "      <td>-0.044822</td>\n",
              "      <td>0.213713</td>\n",
              "      <td>0.614015</td>\n",
              "      <td>0.380890</td>\n",
              "      <td>0.485615</td>\n",
              "      <td>0.622031</td>\n",
              "      <td>-0.120505</td>\n",
              "      <td>0.436568</td>\n",
              "      <td>0.807114</td>\n",
              "      <td>-0.036760</td>\n",
              "      <td>0.426990</td>\n",
              "      <td>0.810267</td>\n",
              "      <td>-0.040235</td>\n",
              "      <td>0.456969</td>\n",
              "      <td>0.639337</td>\n",
              "      <td>-0.088925</td>\n",
              "      <td>0.418537</td>\n",
              "      <td>0.638220</td>\n",
              "      <td>-0.049196</td>\n",
              "      <td>0.452996</td>\n",
              "      <td>0.632927</td>\n",
              "      <td>-0.086052</td>\n",
              "      <td>0.482375</td>\n",
              "      <td>0.228888</td>\n",
              "      <td>0.002328</td>\n",
              "      <td>1471</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14729</th>\n",
              "      <td>0.527434</td>\n",
              "      <td>0.738273</td>\n",
              "      <td>-0.079975</td>\n",
              "      <td>0.328769</td>\n",
              "      <td>0.457341</td>\n",
              "      <td>-0.067171</td>\n",
              "      <td>0.263649</td>\n",
              "      <td>0.484076</td>\n",
              "      <td>-0.066484</td>\n",
              "      <td>0.365410</td>\n",
              "      <td>0.582288</td>\n",
              "      <td>-0.097822</td>\n",
              "      <td>0.136217</td>\n",
              "      <td>0.021140</td>\n",
              "      <td>-0.074558</td>\n",
              "      <td>0.175402</td>\n",
              "      <td>0.085695</td>\n",
              "      <td>-0.108161</td>\n",
              "      <td>0.210071</td>\n",
              "      <td>0.146272</td>\n",
              "      <td>-0.132795</td>\n",
              "      <td>0.343199</td>\n",
              "      <td>0.830920</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>0.406371</td>\n",
              "      <td>0.175288</td>\n",
              "      <td>-0.141420</td>\n",
              "      <td>0.396257</td>\n",
              "      <td>0.092930</td>\n",
              "      <td>-0.141256</td>\n",
              "      <td>0.377956</td>\n",
              "      <td>-0.004383</td>\n",
              "      <td>-0.133272</td>\n",
              "      <td>0.510257</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>-0.116076</td>\n",
              "      <td>0.188901</td>\n",
              "      <td>0.338566</td>\n",
              "      <td>-0.055147</td>\n",
              "      <td>0.084274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798353</td>\n",
              "      <td>-0.025366</td>\n",
              "      <td>0.508404</td>\n",
              "      <td>0.246948</td>\n",
              "      <td>-0.110513</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>0.809397</td>\n",
              "      <td>-0.028493</td>\n",
              "      <td>0.373280</td>\n",
              "      <td>0.819410</td>\n",
              "      <td>-0.022557</td>\n",
              "      <td>0.337109</td>\n",
              "      <td>0.707447</td>\n",
              "      <td>-0.056202</td>\n",
              "      <td>-0.054723</td>\n",
              "      <td>0.439372</td>\n",
              "      <td>0.278810</td>\n",
              "      <td>0.532050</td>\n",
              "      <td>0.647020</td>\n",
              "      <td>-0.122507</td>\n",
              "      <td>0.381318</td>\n",
              "      <td>0.785802</td>\n",
              "      <td>-0.011244</td>\n",
              "      <td>0.373739</td>\n",
              "      <td>0.790772</td>\n",
              "      <td>-0.014106</td>\n",
              "      <td>0.463954</td>\n",
              "      <td>0.650501</td>\n",
              "      <td>-0.093005</td>\n",
              "      <td>0.385598</td>\n",
              "      <td>0.631218</td>\n",
              "      <td>-0.068217</td>\n",
              "      <td>0.455586</td>\n",
              "      <td>0.642425</td>\n",
              "      <td>-0.095598</td>\n",
              "      <td>0.511577</td>\n",
              "      <td>0.192825</td>\n",
              "      <td>-0.125415</td>\n",
              "      <td>1472</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14730</th>\n",
              "      <td>0.528845</td>\n",
              "      <td>0.735577</td>\n",
              "      <td>-0.041305</td>\n",
              "      <td>0.342966</td>\n",
              "      <td>0.537490</td>\n",
              "      <td>-0.021240</td>\n",
              "      <td>0.291597</td>\n",
              "      <td>0.566791</td>\n",
              "      <td>-0.001343</td>\n",
              "      <td>0.376911</td>\n",
              "      <td>0.633230</td>\n",
              "      <td>-0.048700</td>\n",
              "      <td>0.118212</td>\n",
              "      <td>0.205293</td>\n",
              "      <td>-0.040242</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.265181</td>\n",
              "      <td>-0.071281</td>\n",
              "      <td>0.188816</td>\n",
              "      <td>0.329133</td>\n",
              "      <td>-0.094329</td>\n",
              "      <td>0.402845</td>\n",
              "      <td>0.801184</td>\n",
              "      <td>0.070091</td>\n",
              "      <td>0.351299</td>\n",
              "      <td>0.319882</td>\n",
              "      <td>-0.148940</td>\n",
              "      <td>0.321788</td>\n",
              "      <td>0.228375</td>\n",
              "      <td>-0.157065</td>\n",
              "      <td>0.290454</td>\n",
              "      <td>0.137655</td>\n",
              "      <td>-0.158898</td>\n",
              "      <td>0.406340</td>\n",
              "      <td>0.110980</td>\n",
              "      <td>-0.176145</td>\n",
              "      <td>0.219970</td>\n",
              "      <td>0.473707</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>0.150090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.768059</td>\n",
              "      <td>0.035476</td>\n",
              "      <td>0.450491</td>\n",
              "      <td>0.353091</td>\n",
              "      <td>-0.136861</td>\n",
              "      <td>0.419335</td>\n",
              "      <td>0.775359</td>\n",
              "      <td>0.032669</td>\n",
              "      <td>0.414294</td>\n",
              "      <td>0.783746</td>\n",
              "      <td>0.040324</td>\n",
              "      <td>0.367322</td>\n",
              "      <td>0.733595</td>\n",
              "      <td>0.017130</td>\n",
              "      <td>0.119499</td>\n",
              "      <td>0.563274</td>\n",
              "      <td>0.379588</td>\n",
              "      <td>0.510960</td>\n",
              "      <td>0.668780</td>\n",
              "      <td>-0.098167</td>\n",
              "      <td>0.413863</td>\n",
              "      <td>0.767714</td>\n",
              "      <td>0.057815</td>\n",
              "      <td>0.407349</td>\n",
              "      <td>0.770133</td>\n",
              "      <td>0.055871</td>\n",
              "      <td>0.462658</td>\n",
              "      <td>0.677776</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.400108</td>\n",
              "      <td>0.668224</td>\n",
              "      <td>-0.026347</td>\n",
              "      <td>0.454312</td>\n",
              "      <td>0.672526</td>\n",
              "      <td>-0.062448</td>\n",
              "      <td>0.442451</td>\n",
              "      <td>0.308193</td>\n",
              "      <td>-0.157336</td>\n",
              "      <td>1473</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14731</th>\n",
              "      <td>0.452533</td>\n",
              "      <td>0.791973</td>\n",
              "      <td>-0.099548</td>\n",
              "      <td>0.335543</td>\n",
              "      <td>0.535332</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.284853</td>\n",
              "      <td>0.564165</td>\n",
              "      <td>0.029366</td>\n",
              "      <td>0.351197</td>\n",
              "      <td>0.637508</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>0.184646</td>\n",
              "      <td>0.126609</td>\n",
              "      <td>0.138878</td>\n",
              "      <td>0.201092</td>\n",
              "      <td>0.182896</td>\n",
              "      <td>0.081410</td>\n",
              "      <td>0.219815</td>\n",
              "      <td>0.240867</td>\n",
              "      <td>0.035379</td>\n",
              "      <td>0.358867</td>\n",
              "      <td>0.892018</td>\n",
              "      <td>-0.009873</td>\n",
              "      <td>0.362378</td>\n",
              "      <td>0.247935</td>\n",
              "      <td>-0.036624</td>\n",
              "      <td>0.353144</td>\n",
              "      <td>0.162810</td>\n",
              "      <td>-0.012710</td>\n",
              "      <td>0.343602</td>\n",
              "      <td>0.071618</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.451449</td>\n",
              "      <td>0.063975</td>\n",
              "      <td>-0.008453</td>\n",
              "      <td>0.239168</td>\n",
              "      <td>0.439385</td>\n",
              "      <td>0.086585</td>\n",
              "      <td>0.170206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.848754</td>\n",
              "      <td>-0.034963</td>\n",
              "      <td>0.452250</td>\n",
              "      <td>0.307410</td>\n",
              "      <td>-0.048951</td>\n",
              "      <td>0.375028</td>\n",
              "      <td>0.858712</td>\n",
              "      <td>-0.041104</td>\n",
              "      <td>0.368408</td>\n",
              "      <td>0.870020</td>\n",
              "      <td>-0.035212</td>\n",
              "      <td>0.335122</td>\n",
              "      <td>0.770662</td>\n",
              "      <td>-0.023500</td>\n",
              "      <td>0.156868</td>\n",
              "      <td>0.588514</td>\n",
              "      <td>0.422205</td>\n",
              "      <td>0.446972</td>\n",
              "      <td>0.681988</td>\n",
              "      <td>-0.115878</td>\n",
              "      <td>0.378910</td>\n",
              "      <td>0.843526</td>\n",
              "      <td>-0.012812</td>\n",
              "      <td>0.371462</td>\n",
              "      <td>0.847001</td>\n",
              "      <td>-0.016520</td>\n",
              "      <td>0.412317</td>\n",
              "      <td>0.695334</td>\n",
              "      <td>-0.079299</td>\n",
              "      <td>0.370427</td>\n",
              "      <td>0.685399</td>\n",
              "      <td>-0.037217</td>\n",
              "      <td>0.407154</td>\n",
              "      <td>0.687007</td>\n",
              "      <td>-0.078226</td>\n",
              "      <td>0.449482</td>\n",
              "      <td>0.251166</td>\n",
              "      <td>-0.053193</td>\n",
              "      <td>1474</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14732 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.516601  0.585682 -0.128171  ...  0.006673           0        0\n",
              "1      0.350142  0.704083 -0.051000  ... -0.123009           1        0\n",
              "2      0.478097  0.829416 -0.035864  ... -0.201465           2        0\n",
              "3      0.440407  0.633268 -0.130409  ... -0.042549           3        0\n",
              "4      0.508843  0.783509 -0.075715  ... -0.125290           4        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "14727  0.560496  0.769144 -0.111749  ... -0.043033        1470        6\n",
              "14728  0.491013  0.712008 -0.122762  ...  0.002328        1471        6\n",
              "14729  0.527434  0.738273 -0.079975  ... -0.125415        1472        6\n",
              "14730  0.528845  0.735577 -0.041305  ... -0.157336        1473        6\n",
              "14731  0.452533  0.791973 -0.099548  ... -0.053193        1474        6\n",
              "\n",
              "[14732 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "SGSiuoOG1Peq",
        "outputId": "27c14bb4-df29-4527-a348-e2fbcb80776d"
      },
      "source": [
        "df_valid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.646184</td>\n",
              "      <td>0.771045</td>\n",
              "      <td>-0.064509</td>\n",
              "      <td>0.479793</td>\n",
              "      <td>0.566351</td>\n",
              "      <td>-0.088064</td>\n",
              "      <td>0.426735</td>\n",
              "      <td>0.590875</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>0.520683</td>\n",
              "      <td>0.649389</td>\n",
              "      <td>-0.101141</td>\n",
              "      <td>0.297111</td>\n",
              "      <td>0.243108</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>0.351343</td>\n",
              "      <td>0.308081</td>\n",
              "      <td>-0.161240</td>\n",
              "      <td>0.396239</td>\n",
              "      <td>0.377762</td>\n",
              "      <td>-0.175256</td>\n",
              "      <td>0.463307</td>\n",
              "      <td>0.887630</td>\n",
              "      <td>-0.003552</td>\n",
              "      <td>0.560809</td>\n",
              "      <td>0.397581</td>\n",
              "      <td>-0.148304</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.302792</td>\n",
              "      <td>-0.148388</td>\n",
              "      <td>0.513932</td>\n",
              "      <td>0.218670</td>\n",
              "      <td>-0.145369</td>\n",
              "      <td>0.618115</td>\n",
              "      <td>0.221551</td>\n",
              "      <td>-0.109952</td>\n",
              "      <td>0.356944</td>\n",
              "      <td>0.489348</td>\n",
              "      <td>-0.107089</td>\n",
              "      <td>0.258764</td>\n",
              "      <td>...</td>\n",
              "      <td>0.861163</td>\n",
              "      <td>-0.021057</td>\n",
              "      <td>0.630754</td>\n",
              "      <td>0.437828</td>\n",
              "      <td>-0.104779</td>\n",
              "      <td>0.503365</td>\n",
              "      <td>0.870509</td>\n",
              "      <td>-0.021248</td>\n",
              "      <td>0.491106</td>\n",
              "      <td>0.878412</td>\n",
              "      <td>-0.018093</td>\n",
              "      <td>0.482451</td>\n",
              "      <td>0.760005</td>\n",
              "      <td>-0.072059</td>\n",
              "      <td>0.072792</td>\n",
              "      <td>0.583212</td>\n",
              "      <td>0.110986</td>\n",
              "      <td>0.669708</td>\n",
              "      <td>0.690351</td>\n",
              "      <td>-0.098678</td>\n",
              "      <td>0.497744</td>\n",
              "      <td>0.844740</td>\n",
              "      <td>-0.020899</td>\n",
              "      <td>0.490786</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>-0.021804</td>\n",
              "      <td>0.605350</td>\n",
              "      <td>0.697076</td>\n",
              "      <td>-0.082129</td>\n",
              "      <td>0.535840</td>\n",
              "      <td>0.685600</td>\n",
              "      <td>-0.071414</td>\n",
              "      <td>0.600393</td>\n",
              "      <td>0.690829</td>\n",
              "      <td>-0.085879</td>\n",
              "      <td>0.633086</td>\n",
              "      <td>0.403363</td>\n",
              "      <td>-0.117815</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.505669</td>\n",
              "      <td>0.783729</td>\n",
              "      <td>-0.018900</td>\n",
              "      <td>0.383348</td>\n",
              "      <td>0.551676</td>\n",
              "      <td>-0.021686</td>\n",
              "      <td>0.335575</td>\n",
              "      <td>0.573097</td>\n",
              "      <td>-0.002283</td>\n",
              "      <td>0.399952</td>\n",
              "      <td>0.650358</td>\n",
              "      <td>-0.031835</td>\n",
              "      <td>0.229131</td>\n",
              "      <td>0.181487</td>\n",
              "      <td>-0.092331</td>\n",
              "      <td>0.258003</td>\n",
              "      <td>0.257158</td>\n",
              "      <td>-0.111276</td>\n",
              "      <td>0.284237</td>\n",
              "      <td>0.338158</td>\n",
              "      <td>-0.123996</td>\n",
              "      <td>0.408235</td>\n",
              "      <td>0.862809</td>\n",
              "      <td>0.118722</td>\n",
              "      <td>0.414858</td>\n",
              "      <td>0.365437</td>\n",
              "      <td>-0.173962</td>\n",
              "      <td>0.397574</td>\n",
              "      <td>0.252688</td>\n",
              "      <td>-0.191834</td>\n",
              "      <td>0.380021</td>\n",
              "      <td>0.148873</td>\n",
              "      <td>-0.204391</td>\n",
              "      <td>0.480591</td>\n",
              "      <td>0.143603</td>\n",
              "      <td>-0.224993</td>\n",
              "      <td>0.294881</td>\n",
              "      <td>0.460291</td>\n",
              "      <td>-0.004822</td>\n",
              "      <td>0.217984</td>\n",
              "      <td>...</td>\n",
              "      <td>0.831755</td>\n",
              "      <td>0.075494</td>\n",
              "      <td>0.490290</td>\n",
              "      <td>0.410842</td>\n",
              "      <td>-0.158440</td>\n",
              "      <td>0.426757</td>\n",
              "      <td>0.840886</td>\n",
              "      <td>0.078719</td>\n",
              "      <td>0.419972</td>\n",
              "      <td>0.850027</td>\n",
              "      <td>0.088572</td>\n",
              "      <td>0.383190</td>\n",
              "      <td>0.750469</td>\n",
              "      <td>0.039978</td>\n",
              "      <td>0.169034</td>\n",
              "      <td>0.535456</td>\n",
              "      <td>0.333599</td>\n",
              "      <td>0.499905</td>\n",
              "      <td>0.714165</td>\n",
              "      <td>-0.075163</td>\n",
              "      <td>0.430074</td>\n",
              "      <td>0.816941</td>\n",
              "      <td>0.089510</td>\n",
              "      <td>0.422110</td>\n",
              "      <td>0.818853</td>\n",
              "      <td>0.090431</td>\n",
              "      <td>0.462675</td>\n",
              "      <td>0.711085</td>\n",
              "      <td>-0.036148</td>\n",
              "      <td>0.415931</td>\n",
              "      <td>0.687106</td>\n",
              "      <td>-0.008565</td>\n",
              "      <td>0.457287</td>\n",
              "      <td>0.704161</td>\n",
              "      <td>-0.042259</td>\n",
              "      <td>0.487358</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>-0.183734</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.570398</td>\n",
              "      <td>0.522947</td>\n",
              "      <td>-0.131259</td>\n",
              "      <td>0.385422</td>\n",
              "      <td>0.250885</td>\n",
              "      <td>-0.050247</td>\n",
              "      <td>0.313107</td>\n",
              "      <td>0.278246</td>\n",
              "      <td>-0.048979</td>\n",
              "      <td>0.409144</td>\n",
              "      <td>0.370157</td>\n",
              "      <td>-0.113957</td>\n",
              "      <td>0.206225</td>\n",
              "      <td>-0.195378</td>\n",
              "      <td>0.061084</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>-0.130779</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.276074</td>\n",
              "      <td>-0.063120</td>\n",
              "      <td>-0.046078</td>\n",
              "      <td>0.358516</td>\n",
              "      <td>0.730264</td>\n",
              "      <td>0.017391</td>\n",
              "      <td>0.468482</td>\n",
              "      <td>-0.048374</td>\n",
              "      <td>-0.084189</td>\n",
              "      <td>0.454622</td>\n",
              "      <td>-0.150500</td>\n",
              "      <td>-0.053712</td>\n",
              "      <td>0.437219</td>\n",
              "      <td>-0.252034</td>\n",
              "      <td>-0.025488</td>\n",
              "      <td>0.571085</td>\n",
              "      <td>-0.254265</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.253504</td>\n",
              "      <td>0.142910</td>\n",
              "      <td>0.007577</td>\n",
              "      <td>0.149256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.706014</td>\n",
              "      <td>-0.006454</td>\n",
              "      <td>0.569965</td>\n",
              "      <td>0.023101</td>\n",
              "      <td>-0.076176</td>\n",
              "      <td>0.390708</td>\n",
              "      <td>0.716519</td>\n",
              "      <td>-0.009776</td>\n",
              "      <td>0.376529</td>\n",
              "      <td>0.725704</td>\n",
              "      <td>-0.003093</td>\n",
              "      <td>0.352764</td>\n",
              "      <td>0.512117</td>\n",
              "      <td>-0.059380</td>\n",
              "      <td>0.041792</td>\n",
              "      <td>0.330219</td>\n",
              "      <td>0.345695</td>\n",
              "      <td>0.581562</td>\n",
              "      <td>0.438015</td>\n",
              "      <td>-0.170266</td>\n",
              "      <td>0.391589</td>\n",
              "      <td>0.667979</td>\n",
              "      <td>0.011425</td>\n",
              "      <td>0.375232</td>\n",
              "      <td>0.668794</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.507817</td>\n",
              "      <td>0.442177</td>\n",
              "      <td>-0.133319</td>\n",
              "      <td>0.426135</td>\n",
              "      <td>0.421102</td>\n",
              "      <td>-0.094905</td>\n",
              "      <td>0.501055</td>\n",
              "      <td>0.433366</td>\n",
              "      <td>-0.135095</td>\n",
              "      <td>0.571534</td>\n",
              "      <td>-0.036601</td>\n",
              "      <td>-0.077871</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.550640</td>\n",
              "      <td>0.693204</td>\n",
              "      <td>-0.039426</td>\n",
              "      <td>0.377895</td>\n",
              "      <td>0.420303</td>\n",
              "      <td>-0.049635</td>\n",
              "      <td>0.310557</td>\n",
              "      <td>0.444947</td>\n",
              "      <td>-0.030445</td>\n",
              "      <td>0.395424</td>\n",
              "      <td>0.535625</td>\n",
              "      <td>-0.071252</td>\n",
              "      <td>0.196327</td>\n",
              "      <td>0.037854</td>\n",
              "      <td>-0.122527</td>\n",
              "      <td>0.226640</td>\n",
              "      <td>0.102505</td>\n",
              "      <td>-0.149672</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.168133</td>\n",
              "      <td>-0.169691</td>\n",
              "      <td>0.385633</td>\n",
              "      <td>0.777333</td>\n",
              "      <td>0.130624</td>\n",
              "      <td>0.440921</td>\n",
              "      <td>0.182286</td>\n",
              "      <td>-0.220092</td>\n",
              "      <td>0.426143</td>\n",
              "      <td>0.093054</td>\n",
              "      <td>-0.238092</td>\n",
              "      <td>0.408764</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.251741</td>\n",
              "      <td>0.543859</td>\n",
              "      <td>-0.005355</td>\n",
              "      <td>-0.265706</td>\n",
              "      <td>0.259817</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>-0.029762</td>\n",
              "      <td>0.159518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.744009</td>\n",
              "      <td>0.082338</td>\n",
              "      <td>0.545075</td>\n",
              "      <td>0.237159</td>\n",
              "      <td>-0.195135</td>\n",
              "      <td>0.414130</td>\n",
              "      <td>0.752458</td>\n",
              "      <td>0.083467</td>\n",
              "      <td>0.404759</td>\n",
              "      <td>0.761788</td>\n",
              "      <td>0.094698</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>0.646520</td>\n",
              "      <td>0.029753</td>\n",
              "      <td>0.087381</td>\n",
              "      <td>0.425480</td>\n",
              "      <td>0.414335</td>\n",
              "      <td>0.550101</td>\n",
              "      <td>0.611782</td>\n",
              "      <td>-0.120470</td>\n",
              "      <td>0.411218</td>\n",
              "      <td>0.722901</td>\n",
              "      <td>0.103741</td>\n",
              "      <td>0.399978</td>\n",
              "      <td>0.723879</td>\n",
              "      <td>0.104076</td>\n",
              "      <td>0.488015</td>\n",
              "      <td>0.608906</td>\n",
              "      <td>-0.072477</td>\n",
              "      <td>0.414568</td>\n",
              "      <td>0.579445</td>\n",
              "      <td>-0.037483</td>\n",
              "      <td>0.479306</td>\n",
              "      <td>0.600204</td>\n",
              "      <td>-0.079331</td>\n",
              "      <td>0.544766</td>\n",
              "      <td>0.188114</td>\n",
              "      <td>-0.223748</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.524232</td>\n",
              "      <td>0.722622</td>\n",
              "      <td>-0.049771</td>\n",
              "      <td>0.378299</td>\n",
              "      <td>0.537701</td>\n",
              "      <td>-0.022929</td>\n",
              "      <td>0.332367</td>\n",
              "      <td>0.561998</td>\n",
              "      <td>-0.011429</td>\n",
              "      <td>0.401680</td>\n",
              "      <td>0.617495</td>\n",
              "      <td>-0.051351</td>\n",
              "      <td>0.205039</td>\n",
              "      <td>0.227481</td>\n",
              "      <td>-0.018862</td>\n",
              "      <td>0.235042</td>\n",
              "      <td>0.282814</td>\n",
              "      <td>-0.050678</td>\n",
              "      <td>0.265167</td>\n",
              "      <td>0.346141</td>\n",
              "      <td>-0.075391</td>\n",
              "      <td>0.425222</td>\n",
              "      <td>0.818088</td>\n",
              "      <td>0.049098</td>\n",
              "      <td>0.399753</td>\n",
              "      <td>0.352436</td>\n",
              "      <td>-0.117018</td>\n",
              "      <td>0.375707</td>\n",
              "      <td>0.255840</td>\n",
              "      <td>-0.116756</td>\n",
              "      <td>0.351857</td>\n",
              "      <td>0.170662</td>\n",
              "      <td>-0.111585</td>\n",
              "      <td>0.448775</td>\n",
              "      <td>0.154980</td>\n",
              "      <td>-0.122691</td>\n",
              "      <td>0.283837</td>\n",
              "      <td>0.472371</td>\n",
              "      <td>0.007503</td>\n",
              "      <td>0.215384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.779411</td>\n",
              "      <td>0.023573</td>\n",
              "      <td>0.478230</td>\n",
              "      <td>0.386732</td>\n",
              "      <td>-0.106638</td>\n",
              "      <td>0.441388</td>\n",
              "      <td>0.787471</td>\n",
              "      <td>0.022020</td>\n",
              "      <td>0.435998</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.027950</td>\n",
              "      <td>0.390656</td>\n",
              "      <td>0.715117</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.174290</td>\n",
              "      <td>0.598808</td>\n",
              "      <td>0.309484</td>\n",
              "      <td>0.511831</td>\n",
              "      <td>0.657377</td>\n",
              "      <td>-0.094378</td>\n",
              "      <td>0.437287</td>\n",
              "      <td>0.769950</td>\n",
              "      <td>0.040689</td>\n",
              "      <td>0.429929</td>\n",
              "      <td>0.771605</td>\n",
              "      <td>0.039537</td>\n",
              "      <td>0.471688</td>\n",
              "      <td>0.662292</td>\n",
              "      <td>-0.062175</td>\n",
              "      <td>0.420203</td>\n",
              "      <td>0.650105</td>\n",
              "      <td>-0.033778</td>\n",
              "      <td>0.465479</td>\n",
              "      <td>0.656399</td>\n",
              "      <td>-0.065202</td>\n",
              "      <td>0.473144</td>\n",
              "      <td>0.346925</td>\n",
              "      <td>-0.121656</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14631</th>\n",
              "      <td>0.529442</td>\n",
              "      <td>0.667210</td>\n",
              "      <td>-0.066573</td>\n",
              "      <td>0.386126</td>\n",
              "      <td>0.486287</td>\n",
              "      <td>-0.033634</td>\n",
              "      <td>0.341563</td>\n",
              "      <td>0.510485</td>\n",
              "      <td>-0.029726</td>\n",
              "      <td>0.418692</td>\n",
              "      <td>0.560215</td>\n",
              "      <td>-0.061615</td>\n",
              "      <td>0.219367</td>\n",
              "      <td>0.183714</td>\n",
              "      <td>-0.007092</td>\n",
              "      <td>0.248392</td>\n",
              "      <td>0.228448</td>\n",
              "      <td>-0.037222</td>\n",
              "      <td>0.276280</td>\n",
              "      <td>0.271904</td>\n",
              "      <td>-0.061118</td>\n",
              "      <td>0.423037</td>\n",
              "      <td>0.759102</td>\n",
              "      <td>-0.001626</td>\n",
              "      <td>0.413478</td>\n",
              "      <td>0.273705</td>\n",
              "      <td>-0.080519</td>\n",
              "      <td>0.398730</td>\n",
              "      <td>0.209388</td>\n",
              "      <td>-0.074626</td>\n",
              "      <td>0.379055</td>\n",
              "      <td>0.137959</td>\n",
              "      <td>-0.063146</td>\n",
              "      <td>0.475969</td>\n",
              "      <td>0.127867</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>0.284961</td>\n",
              "      <td>0.422919</td>\n",
              "      <td>-0.011928</td>\n",
              "      <td>0.216206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.732706</td>\n",
              "      <td>-0.021548</td>\n",
              "      <td>0.494808</td>\n",
              "      <td>0.313197</td>\n",
              "      <td>-0.065520</td>\n",
              "      <td>0.445838</td>\n",
              "      <td>0.741607</td>\n",
              "      <td>-0.024574</td>\n",
              "      <td>0.437814</td>\n",
              "      <td>0.749662</td>\n",
              "      <td>-0.020488</td>\n",
              "      <td>0.407561</td>\n",
              "      <td>0.660633</td>\n",
              "      <td>-0.038206</td>\n",
              "      <td>0.153465</td>\n",
              "      <td>0.517849</td>\n",
              "      <td>0.231584</td>\n",
              "      <td>0.524237</td>\n",
              "      <td>0.586032</td>\n",
              "      <td>-0.089157</td>\n",
              "      <td>0.447015</td>\n",
              "      <td>0.723673</td>\n",
              "      <td>-0.012398</td>\n",
              "      <td>0.440423</td>\n",
              "      <td>0.727365</td>\n",
              "      <td>-0.014400</td>\n",
              "      <td>0.484241</td>\n",
              "      <td>0.597897</td>\n",
              "      <td>-0.066358</td>\n",
              "      <td>0.436844</td>\n",
              "      <td>0.592494</td>\n",
              "      <td>-0.044895</td>\n",
              "      <td>0.479310</td>\n",
              "      <td>0.592026</td>\n",
              "      <td>-0.067038</td>\n",
              "      <td>0.491907</td>\n",
              "      <td>0.271605</td>\n",
              "      <td>-0.073554</td>\n",
              "      <td>1469</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14632</th>\n",
              "      <td>0.500539</td>\n",
              "      <td>0.734860</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>0.327387</td>\n",
              "      <td>0.516931</td>\n",
              "      <td>-0.010588</td>\n",
              "      <td>0.265571</td>\n",
              "      <td>0.545812</td>\n",
              "      <td>-0.007016</td>\n",
              "      <td>0.361814</td>\n",
              "      <td>0.604152</td>\n",
              "      <td>-0.083395</td>\n",
              "      <td>0.160265</td>\n",
              "      <td>0.151897</td>\n",
              "      <td>0.173618</td>\n",
              "      <td>0.184835</td>\n",
              "      <td>0.188428</td>\n",
              "      <td>0.104391</td>\n",
              "      <td>0.210830</td>\n",
              "      <td>0.227435</td>\n",
              "      <td>0.047180</td>\n",
              "      <td>0.350158</td>\n",
              "      <td>0.870057</td>\n",
              "      <td>-0.090012</td>\n",
              "      <td>0.386060</td>\n",
              "      <td>0.230485</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.377281</td>\n",
              "      <td>0.166759</td>\n",
              "      <td>0.043994</td>\n",
              "      <td>0.364579</td>\n",
              "      <td>0.095391</td>\n",
              "      <td>0.089954</td>\n",
              "      <td>0.487561</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.089601</td>\n",
              "      <td>0.203078</td>\n",
              "      <td>0.435127</td>\n",
              "      <td>0.063342</td>\n",
              "      <td>0.114195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820860</td>\n",
              "      <td>-0.104007</td>\n",
              "      <td>0.487804</td>\n",
              "      <td>0.291259</td>\n",
              "      <td>-0.005642</td>\n",
              "      <td>0.382895</td>\n",
              "      <td>0.832251</td>\n",
              "      <td>-0.113706</td>\n",
              "      <td>0.372782</td>\n",
              "      <td>0.845191</td>\n",
              "      <td>-0.110114</td>\n",
              "      <td>0.337598</td>\n",
              "      <td>0.735444</td>\n",
              "      <td>-0.089631</td>\n",
              "      <td>0.034739</td>\n",
              "      <td>0.615029</td>\n",
              "      <td>0.373997</td>\n",
              "      <td>0.498705</td>\n",
              "      <td>0.626003</td>\n",
              "      <td>-0.145978</td>\n",
              "      <td>0.384501</td>\n",
              "      <td>0.817192</td>\n",
              "      <td>-0.083325</td>\n",
              "      <td>0.376356</td>\n",
              "      <td>0.821481</td>\n",
              "      <td>-0.089008</td>\n",
              "      <td>0.447718</td>\n",
              "      <td>0.647504</td>\n",
              "      <td>-0.116903</td>\n",
              "      <td>0.385622</td>\n",
              "      <td>0.649125</td>\n",
              "      <td>-0.076227</td>\n",
              "      <td>0.441510</td>\n",
              "      <td>0.639893</td>\n",
              "      <td>-0.113286</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.238567</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>1470</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14633</th>\n",
              "      <td>0.494283</td>\n",
              "      <td>0.710024</td>\n",
              "      <td>-0.116081</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.497803</td>\n",
              "      <td>-0.009445</td>\n",
              "      <td>0.295208</td>\n",
              "      <td>0.520917</td>\n",
              "      <td>-0.001318</td>\n",
              "      <td>0.381529</td>\n",
              "      <td>0.580622</td>\n",
              "      <td>-0.068246</td>\n",
              "      <td>0.204759</td>\n",
              "      <td>0.123697</td>\n",
              "      <td>0.127479</td>\n",
              "      <td>0.225363</td>\n",
              "      <td>0.164128</td>\n",
              "      <td>0.072507</td>\n",
              "      <td>0.249508</td>\n",
              "      <td>0.206157</td>\n",
              "      <td>0.027697</td>\n",
              "      <td>0.370819</td>\n",
              "      <td>0.833998</td>\n",
              "      <td>-0.037612</td>\n",
              "      <td>0.400362</td>\n",
              "      <td>0.218823</td>\n",
              "      <td>-0.020845</td>\n",
              "      <td>0.388722</td>\n",
              "      <td>0.150536</td>\n",
              "      <td>0.007938</td>\n",
              "      <td>0.374952</td>\n",
              "      <td>0.073443</td>\n",
              "      <td>0.040432</td>\n",
              "      <td>0.483528</td>\n",
              "      <td>0.071126</td>\n",
              "      <td>0.034102</td>\n",
              "      <td>0.244181</td>\n",
              "      <td>0.414586</td>\n",
              "      <td>0.053564</td>\n",
              "      <td>0.170244</td>\n",
              "      <td>...</td>\n",
              "      <td>0.797283</td>\n",
              "      <td>-0.057481</td>\n",
              "      <td>0.491774</td>\n",
              "      <td>0.282192</td>\n",
              "      <td>-0.024813</td>\n",
              "      <td>0.390480</td>\n",
              "      <td>0.806401</td>\n",
              "      <td>-0.063569</td>\n",
              "      <td>0.381867</td>\n",
              "      <td>0.816910</td>\n",
              "      <td>-0.058509</td>\n",
              "      <td>0.356374</td>\n",
              "      <td>0.697895</td>\n",
              "      <td>-0.055486</td>\n",
              "      <td>0.130981</td>\n",
              "      <td>0.558962</td>\n",
              "      <td>0.341036</td>\n",
              "      <td>0.493862</td>\n",
              "      <td>0.611363</td>\n",
              "      <td>-0.122746</td>\n",
              "      <td>0.395654</td>\n",
              "      <td>0.785397</td>\n",
              "      <td>-0.038678</td>\n",
              "      <td>0.386269</td>\n",
              "      <td>0.787321</td>\n",
              "      <td>-0.042230</td>\n",
              "      <td>0.452422</td>\n",
              "      <td>0.626979</td>\n",
              "      <td>-0.094577</td>\n",
              "      <td>0.401670</td>\n",
              "      <td>0.622043</td>\n",
              "      <td>-0.060001</td>\n",
              "      <td>0.447939</td>\n",
              "      <td>0.619601</td>\n",
              "      <td>-0.092344</td>\n",
              "      <td>0.490952</td>\n",
              "      <td>0.228598</td>\n",
              "      <td>-0.023898</td>\n",
              "      <td>1471</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14634</th>\n",
              "      <td>0.480344</td>\n",
              "      <td>0.614472</td>\n",
              "      <td>-0.073619</td>\n",
              "      <td>0.351348</td>\n",
              "      <td>0.359926</td>\n",
              "      <td>-0.029388</td>\n",
              "      <td>0.296622</td>\n",
              "      <td>0.382071</td>\n",
              "      <td>-0.013036</td>\n",
              "      <td>0.371280</td>\n",
              "      <td>0.466668</td>\n",
              "      <td>-0.062456</td>\n",
              "      <td>0.200838</td>\n",
              "      <td>-0.066494</td>\n",
              "      <td>-0.011770</td>\n",
              "      <td>0.220040</td>\n",
              "      <td>-0.001829</td>\n",
              "      <td>-0.048701</td>\n",
              "      <td>0.240770</td>\n",
              "      <td>0.061476</td>\n",
              "      <td>-0.075901</td>\n",
              "      <td>0.363895</td>\n",
              "      <td>0.734271</td>\n",
              "      <td>0.071702</td>\n",
              "      <td>0.399486</td>\n",
              "      <td>0.096403</td>\n",
              "      <td>-0.128011</td>\n",
              "      <td>0.393551</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.130443</td>\n",
              "      <td>0.384674</td>\n",
              "      <td>-0.095847</td>\n",
              "      <td>-0.123506</td>\n",
              "      <td>0.504907</td>\n",
              "      <td>-0.092631</td>\n",
              "      <td>-0.136478</td>\n",
              "      <td>0.245395</td>\n",
              "      <td>0.251886</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.169527</td>\n",
              "      <td>...</td>\n",
              "      <td>0.721825</td>\n",
              "      <td>0.031216</td>\n",
              "      <td>0.494581</td>\n",
              "      <td>0.161664</td>\n",
              "      <td>-0.117363</td>\n",
              "      <td>0.387729</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.031917</td>\n",
              "      <td>0.376631</td>\n",
              "      <td>0.738281</td>\n",
              "      <td>0.040350</td>\n",
              "      <td>0.353397</td>\n",
              "      <td>0.587014</td>\n",
              "      <td>-0.009165</td>\n",
              "      <td>0.138784</td>\n",
              "      <td>0.330477</td>\n",
              "      <td>0.364592</td>\n",
              "      <td>0.478526</td>\n",
              "      <td>0.523328</td>\n",
              "      <td>-0.111924</td>\n",
              "      <td>0.401928</td>\n",
              "      <td>0.697973</td>\n",
              "      <td>0.040930</td>\n",
              "      <td>0.390769</td>\n",
              "      <td>0.701493</td>\n",
              "      <td>0.040458</td>\n",
              "      <td>0.437818</td>\n",
              "      <td>0.528118</td>\n",
              "      <td>-0.072447</td>\n",
              "      <td>0.389704</td>\n",
              "      <td>0.510692</td>\n",
              "      <td>-0.040840</td>\n",
              "      <td>0.432847</td>\n",
              "      <td>0.520244</td>\n",
              "      <td>-0.076076</td>\n",
              "      <td>0.496153</td>\n",
              "      <td>0.107584</td>\n",
              "      <td>-0.134429</td>\n",
              "      <td>1472</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>0.451848</td>\n",
              "      <td>0.622114</td>\n",
              "      <td>-0.181673</td>\n",
              "      <td>0.305574</td>\n",
              "      <td>0.401656</td>\n",
              "      <td>-0.015953</td>\n",
              "      <td>0.233015</td>\n",
              "      <td>0.421863</td>\n",
              "      <td>-0.015226</td>\n",
              "      <td>0.335668</td>\n",
              "      <td>0.477531</td>\n",
              "      <td>-0.099770</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>0.018560</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.221613</td>\n",
              "      <td>0.058618</td>\n",
              "      <td>0.131506</td>\n",
              "      <td>0.242395</td>\n",
              "      <td>0.106519</td>\n",
              "      <td>0.064972</td>\n",
              "      <td>0.257232</td>\n",
              "      <td>0.817205</td>\n",
              "      <td>-0.085517</td>\n",
              "      <td>0.429682</td>\n",
              "      <td>0.147278</td>\n",
              "      <td>0.012527</td>\n",
              "      <td>0.433578</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.067867</td>\n",
              "      <td>0.434073</td>\n",
              "      <td>-0.013070</td>\n",
              "      <td>0.124950</td>\n",
              "      <td>0.568935</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.123650</td>\n",
              "      <td>0.182558</td>\n",
              "      <td>0.309983</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>0.081712</td>\n",
              "      <td>...</td>\n",
              "      <td>0.787673</td>\n",
              "      <td>-0.104376</td>\n",
              "      <td>0.523025</td>\n",
              "      <td>0.221324</td>\n",
              "      <td>0.002962</td>\n",
              "      <td>0.306585</td>\n",
              "      <td>0.796358</td>\n",
              "      <td>-0.111493</td>\n",
              "      <td>0.289647</td>\n",
              "      <td>0.805510</td>\n",
              "      <td>-0.107110</td>\n",
              "      <td>0.283806</td>\n",
              "      <td>0.617631</td>\n",
              "      <td>-0.111347</td>\n",
              "      <td>-0.033763</td>\n",
              "      <td>0.493091</td>\n",
              "      <td>0.385915</td>\n",
              "      <td>0.465378</td>\n",
              "      <td>0.510059</td>\n",
              "      <td>-0.169199</td>\n",
              "      <td>0.322039</td>\n",
              "      <td>0.765549</td>\n",
              "      <td>-0.091213</td>\n",
              "      <td>0.309973</td>\n",
              "      <td>0.766603</td>\n",
              "      <td>-0.096392</td>\n",
              "      <td>0.413244</td>\n",
              "      <td>0.528470</td>\n",
              "      <td>-0.136841</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>0.525035</td>\n",
              "      <td>-0.093184</td>\n",
              "      <td>0.409769</td>\n",
              "      <td>0.520118</td>\n",
              "      <td>-0.133804</td>\n",
              "      <td>0.533610</td>\n",
              "      <td>0.171482</td>\n",
              "      <td>0.013007</td>\n",
              "      <td>1473</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14636 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.646184  0.771045 -0.064509  ... -0.117815           0        0\n",
              "1      0.505669  0.783729 -0.018900  ... -0.183734           1        0\n",
              "2      0.570398  0.522947 -0.131259  ... -0.077871           3        0\n",
              "3      0.550640  0.693204 -0.039426  ... -0.223748           4        0\n",
              "4      0.524232  0.722622 -0.049771  ... -0.121656           5        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "14631  0.529442  0.667210 -0.066573  ... -0.073554        1469        6\n",
              "14632  0.500539  0.734860 -0.148886  ...  0.000434        1470        6\n",
              "14633  0.494283  0.710024 -0.116081  ... -0.023898        1471        6\n",
              "14634  0.480344  0.614472 -0.073619  ... -0.134429        1472        6\n",
              "14635  0.451848  0.622114 -0.181673  ...  0.013007        1473        6\n",
              "\n",
              "[14636 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT4rg5Ugl6ll"
      },
      "source": [
        "Unnameはいらないので削除"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ZvzA8Gy1EUhN",
        "outputId": "b3a4b351-b97d-436e-f353-24b5e1a2a1fe"
      },
      "source": [
        "df_train.drop(columns=df_train.columns[[-2]],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405580</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.362563</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.629462</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.406199</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.077165</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.366158</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.392929</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.429142</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.469884</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405460</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1405 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_y       9_z  correct\n",
              "0      0.470799  0.682127 -0.080930  ...  0.269363 -0.045183        0\n",
              "1      0.469567  0.799870 -0.017644  ...  0.319253 -0.206077        0\n",
              "2      0.711111  0.717369 -0.049849  ...  0.328598 -0.146556        0\n",
              "3      0.483527  0.809326 -0.011760  ...  0.411362 -0.195280        0\n",
              "4      0.061688  0.692855 -0.050553  ...  0.306380 -0.058228        0\n",
              "...         ...       ...       ...  ...       ...       ...      ...\n",
              "82190  0.436367  0.787778 -0.124726  ...  0.236569 -0.051585        6\n",
              "82191  0.456891  0.704518 -0.093469  ...  0.282853 -0.061855        6\n",
              "82192  0.502264  0.729350 -0.124422  ...  0.268140 -0.034920        6\n",
              "82193  0.539922  0.733853 -0.116346  ...  0.276309 -0.023272        6\n",
              "82194  0.508446  0.805487 -0.070251  ...  0.245632 -0.153780        6\n",
              "\n",
              "[82195 rows x 1405 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OqO9I_q6S1z"
      },
      "source": [
        "967,5,17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "hrF0BgW-1UTy",
        "outputId": "2bad643c-b417-43a9-b3a5-23e89353ca17"
      },
      "source": [
        "df_test.drop(columns=df_test.columns[[-2]],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.516601</td>\n",
              "      <td>0.585682</td>\n",
              "      <td>-0.128171</td>\n",
              "      <td>0.354741</td>\n",
              "      <td>0.407085</td>\n",
              "      <td>-0.017539</td>\n",
              "      <td>0.303413</td>\n",
              "      <td>0.437223</td>\n",
              "      <td>-0.022323</td>\n",
              "      <td>0.385631</td>\n",
              "      <td>0.474988</td>\n",
              "      <td>-0.077427</td>\n",
              "      <td>0.192280</td>\n",
              "      <td>0.117430</td>\n",
              "      <td>0.133606</td>\n",
              "      <td>0.227670</td>\n",
              "      <td>0.153081</td>\n",
              "      <td>0.075319</td>\n",
              "      <td>0.261869</td>\n",
              "      <td>0.196746</td>\n",
              "      <td>0.025099</td>\n",
              "      <td>0.384131</td>\n",
              "      <td>0.734853</td>\n",
              "      <td>-0.092528</td>\n",
              "      <td>0.409821</td>\n",
              "      <td>0.185824</td>\n",
              "      <td>-0.000665</td>\n",
              "      <td>0.388524</td>\n",
              "      <td>0.114111</td>\n",
              "      <td>0.043742</td>\n",
              "      <td>0.366814</td>\n",
              "      <td>0.051379</td>\n",
              "      <td>0.084901</td>\n",
              "      <td>0.466509</td>\n",
              "      <td>0.041154</td>\n",
              "      <td>0.091456</td>\n",
              "      <td>0.247168</td>\n",
              "      <td>0.348231</td>\n",
              "      <td>0.038491</td>\n",
              "      <td>0.162550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.419576</td>\n",
              "      <td>0.673695</td>\n",
              "      <td>-0.099631</td>\n",
              "      <td>0.486085</td>\n",
              "      <td>0.227604</td>\n",
              "      <td>-0.000800</td>\n",
              "      <td>0.411432</td>\n",
              "      <td>0.685160</td>\n",
              "      <td>-0.107264</td>\n",
              "      <td>0.403327</td>\n",
              "      <td>0.699680</td>\n",
              "      <td>-0.105119</td>\n",
              "      <td>0.365605</td>\n",
              "      <td>0.599879</td>\n",
              "      <td>-0.087972</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>0.566646</td>\n",
              "      <td>0.268614</td>\n",
              "      <td>0.517140</td>\n",
              "      <td>0.487549</td>\n",
              "      <td>-0.123952</td>\n",
              "      <td>0.407326</td>\n",
              "      <td>0.672411</td>\n",
              "      <td>-0.080824</td>\n",
              "      <td>0.400786</td>\n",
              "      <td>0.675086</td>\n",
              "      <td>-0.085923</td>\n",
              "      <td>0.467047</td>\n",
              "      <td>0.510292</td>\n",
              "      <td>-0.103923</td>\n",
              "      <td>0.406921</td>\n",
              "      <td>0.514333</td>\n",
              "      <td>-0.071427</td>\n",
              "      <td>0.460763</td>\n",
              "      <td>0.503372</td>\n",
              "      <td>-0.100683</td>\n",
              "      <td>0.483841</td>\n",
              "      <td>0.187230</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.350142</td>\n",
              "      <td>0.704083</td>\n",
              "      <td>-0.051000</td>\n",
              "      <td>0.258494</td>\n",
              "      <td>0.467294</td>\n",
              "      <td>0.006417</td>\n",
              "      <td>0.210762</td>\n",
              "      <td>0.484544</td>\n",
              "      <td>0.028842</td>\n",
              "      <td>0.256672</td>\n",
              "      <td>0.560486</td>\n",
              "      <td>-0.029805</td>\n",
              "      <td>0.163514</td>\n",
              "      <td>0.115912</td>\n",
              "      <td>0.038936</td>\n",
              "      <td>0.169306</td>\n",
              "      <td>0.169693</td>\n",
              "      <td>-0.000869</td>\n",
              "      <td>0.178214</td>\n",
              "      <td>0.229622</td>\n",
              "      <td>-0.030477</td>\n",
              "      <td>0.262613</td>\n",
              "      <td>0.771681</td>\n",
              "      <td>0.078652</td>\n",
              "      <td>0.294888</td>\n",
              "      <td>0.263658</td>\n",
              "      <td>-0.103125</td>\n",
              "      <td>0.291660</td>\n",
              "      <td>0.172433</td>\n",
              "      <td>-0.100819</td>\n",
              "      <td>0.290796</td>\n",
              "      <td>0.086691</td>\n",
              "      <td>-0.094484</td>\n",
              "      <td>0.382622</td>\n",
              "      <td>0.088537</td>\n",
              "      <td>-0.127045</td>\n",
              "      <td>0.190766</td>\n",
              "      <td>0.370796</td>\n",
              "      <td>0.059996</td>\n",
              "      <td>0.127881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.286113</td>\n",
              "      <td>0.731658</td>\n",
              "      <td>0.042865</td>\n",
              "      <td>0.366543</td>\n",
              "      <td>0.318007</td>\n",
              "      <td>-0.107573</td>\n",
              "      <td>0.279443</td>\n",
              "      <td>0.738324</td>\n",
              "      <td>0.041889</td>\n",
              "      <td>0.274230</td>\n",
              "      <td>0.747920</td>\n",
              "      <td>0.050578</td>\n",
              "      <td>0.236568</td>\n",
              "      <td>0.660978</td>\n",
              "      <td>0.032212</td>\n",
              "      <td>0.120189</td>\n",
              "      <td>0.495042</td>\n",
              "      <td>0.403236</td>\n",
              "      <td>0.337132</td>\n",
              "      <td>0.624860</td>\n",
              "      <td>-0.094728</td>\n",
              "      <td>0.281208</td>\n",
              "      <td>0.722307</td>\n",
              "      <td>0.068394</td>\n",
              "      <td>0.274025</td>\n",
              "      <td>0.721999</td>\n",
              "      <td>0.067390</td>\n",
              "      <td>0.308012</td>\n",
              "      <td>0.624537</td>\n",
              "      <td>-0.054114</td>\n",
              "      <td>0.268657</td>\n",
              "      <td>0.601183</td>\n",
              "      <td>-0.016952</td>\n",
              "      <td>0.302620</td>\n",
              "      <td>0.616359</td>\n",
              "      <td>-0.057269</td>\n",
              "      <td>0.366551</td>\n",
              "      <td>0.274136</td>\n",
              "      <td>-0.123009</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.478097</td>\n",
              "      <td>0.829416</td>\n",
              "      <td>-0.035864</td>\n",
              "      <td>0.338219</td>\n",
              "      <td>0.586676</td>\n",
              "      <td>-0.044701</td>\n",
              "      <td>0.281508</td>\n",
              "      <td>0.608822</td>\n",
              "      <td>-0.030072</td>\n",
              "      <td>0.362336</td>\n",
              "      <td>0.690208</td>\n",
              "      <td>-0.055267</td>\n",
              "      <td>0.158889</td>\n",
              "      <td>0.208705</td>\n",
              "      <td>-0.127339</td>\n",
              "      <td>0.195300</td>\n",
              "      <td>0.286799</td>\n",
              "      <td>-0.150213</td>\n",
              "      <td>0.229269</td>\n",
              "      <td>0.368802</td>\n",
              "      <td>-0.164157</td>\n",
              "      <td>0.357517</td>\n",
              "      <td>0.888281</td>\n",
              "      <td>0.091781</td>\n",
              "      <td>0.391913</td>\n",
              "      <td>0.394345</td>\n",
              "      <td>-0.200909</td>\n",
              "      <td>0.374054</td>\n",
              "      <td>0.287385</td>\n",
              "      <td>-0.220581</td>\n",
              "      <td>0.355048</td>\n",
              "      <td>0.184166</td>\n",
              "      <td>-0.231421</td>\n",
              "      <td>0.476613</td>\n",
              "      <td>0.184120</td>\n",
              "      <td>-0.238496</td>\n",
              "      <td>0.228681</td>\n",
              "      <td>0.488865</td>\n",
              "      <td>-0.035819</td>\n",
              "      <td>0.134504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.394229</td>\n",
              "      <td>0.857773</td>\n",
              "      <td>0.046278</td>\n",
              "      <td>0.478034</td>\n",
              "      <td>0.444558</td>\n",
              "      <td>-0.172615</td>\n",
              "      <td>0.386257</td>\n",
              "      <td>0.865752</td>\n",
              "      <td>0.048123</td>\n",
              "      <td>0.378545</td>\n",
              "      <td>0.874142</td>\n",
              "      <td>0.057897</td>\n",
              "      <td>0.340892</td>\n",
              "      <td>0.793487</td>\n",
              "      <td>0.010045</td>\n",
              "      <td>0.054897</td>\n",
              "      <td>0.550742</td>\n",
              "      <td>0.351424</td>\n",
              "      <td>0.478415</td>\n",
              "      <td>0.751673</td>\n",
              "      <td>-0.092016</td>\n",
              "      <td>0.385269</td>\n",
              "      <td>0.849144</td>\n",
              "      <td>0.059494</td>\n",
              "      <td>0.379350</td>\n",
              "      <td>0.852295</td>\n",
              "      <td>0.059011</td>\n",
              "      <td>0.434535</td>\n",
              "      <td>0.750683</td>\n",
              "      <td>-0.052339</td>\n",
              "      <td>0.380534</td>\n",
              "      <td>0.728616</td>\n",
              "      <td>-0.024325</td>\n",
              "      <td>0.428548</td>\n",
              "      <td>0.743129</td>\n",
              "      <td>-0.058275</td>\n",
              "      <td>0.478435</td>\n",
              "      <td>0.401297</td>\n",
              "      <td>-0.201465</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.440407</td>\n",
              "      <td>0.633268</td>\n",
              "      <td>-0.130409</td>\n",
              "      <td>0.313094</td>\n",
              "      <td>0.456206</td>\n",
              "      <td>0.001129</td>\n",
              "      <td>0.255751</td>\n",
              "      <td>0.477937</td>\n",
              "      <td>0.012412</td>\n",
              "      <td>0.327564</td>\n",
              "      <td>0.527424</td>\n",
              "      <td>-0.067629</td>\n",
              "      <td>0.187209</td>\n",
              "      <td>0.126621</td>\n",
              "      <td>0.163992</td>\n",
              "      <td>0.209704</td>\n",
              "      <td>0.178016</td>\n",
              "      <td>0.096623</td>\n",
              "      <td>0.230991</td>\n",
              "      <td>0.237480</td>\n",
              "      <td>0.042394</td>\n",
              "      <td>0.312069</td>\n",
              "      <td>0.801668</td>\n",
              "      <td>0.008883</td>\n",
              "      <td>0.372335</td>\n",
              "      <td>0.248090</td>\n",
              "      <td>-0.028703</td>\n",
              "      <td>0.360934</td>\n",
              "      <td>0.151648</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.349558</td>\n",
              "      <td>0.068262</td>\n",
              "      <td>0.044676</td>\n",
              "      <td>0.458189</td>\n",
              "      <td>0.061373</td>\n",
              "      <td>0.022868</td>\n",
              "      <td>0.221603</td>\n",
              "      <td>0.385956</td>\n",
              "      <td>0.085039</td>\n",
              "      <td>0.143525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.344682</td>\n",
              "      <td>0.774976</td>\n",
              "      <td>-0.017430</td>\n",
              "      <td>0.452601</td>\n",
              "      <td>0.292870</td>\n",
              "      <td>-0.042988</td>\n",
              "      <td>0.331663</td>\n",
              "      <td>0.783315</td>\n",
              "      <td>-0.020361</td>\n",
              "      <td>0.322092</td>\n",
              "      <td>0.792106</td>\n",
              "      <td>-0.013262</td>\n",
              "      <td>0.298874</td>\n",
              "      <td>0.636628</td>\n",
              "      <td>-0.034253</td>\n",
              "      <td>0.112172</td>\n",
              "      <td>0.549183</td>\n",
              "      <td>0.409472</td>\n",
              "      <td>0.433293</td>\n",
              "      <td>0.558691</td>\n",
              "      <td>-0.145084</td>\n",
              "      <td>0.341583</td>\n",
              "      <td>0.752286</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.752438</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.395023</td>\n",
              "      <td>0.569501</td>\n",
              "      <td>-0.106214</td>\n",
              "      <td>0.345461</td>\n",
              "      <td>0.563145</td>\n",
              "      <td>-0.061551</td>\n",
              "      <td>0.390786</td>\n",
              "      <td>0.563328</td>\n",
              "      <td>-0.106485</td>\n",
              "      <td>0.452236</td>\n",
              "      <td>0.249019</td>\n",
              "      <td>-0.042549</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.508843</td>\n",
              "      <td>0.783509</td>\n",
              "      <td>-0.075715</td>\n",
              "      <td>0.342278</td>\n",
              "      <td>0.582771</td>\n",
              "      <td>-0.031950</td>\n",
              "      <td>0.289867</td>\n",
              "      <td>0.611026</td>\n",
              "      <td>-0.020946</td>\n",
              "      <td>0.375631</td>\n",
              "      <td>0.676300</td>\n",
              "      <td>-0.067914</td>\n",
              "      <td>0.148420</td>\n",
              "      <td>0.218699</td>\n",
              "      <td>-0.009825</td>\n",
              "      <td>0.183308</td>\n",
              "      <td>0.281258</td>\n",
              "      <td>-0.048717</td>\n",
              "      <td>0.216023</td>\n",
              "      <td>0.349559</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>0.396198</td>\n",
              "      <td>0.916336</td>\n",
              "      <td>0.045910</td>\n",
              "      <td>0.370082</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>-0.122842</td>\n",
              "      <td>0.345563</td>\n",
              "      <td>0.249492</td>\n",
              "      <td>-0.119133</td>\n",
              "      <td>0.317737</td>\n",
              "      <td>0.152755</td>\n",
              "      <td>-0.109723</td>\n",
              "      <td>0.428981</td>\n",
              "      <td>0.135849</td>\n",
              "      <td>-0.117880</td>\n",
              "      <td>0.226637</td>\n",
              "      <td>0.504523</td>\n",
              "      <td>0.004935</td>\n",
              "      <td>0.154398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.424438</td>\n",
              "      <td>0.875826</td>\n",
              "      <td>0.011859</td>\n",
              "      <td>0.461890</td>\n",
              "      <td>0.394244</td>\n",
              "      <td>-0.109819</td>\n",
              "      <td>0.415073</td>\n",
              "      <td>0.887762</td>\n",
              "      <td>0.011303</td>\n",
              "      <td>0.407975</td>\n",
              "      <td>0.899817</td>\n",
              "      <td>0.019094</td>\n",
              "      <td>0.362383</td>\n",
              "      <td>0.781520</td>\n",
              "      <td>-0.017575</td>\n",
              "      <td>0.112559</td>\n",
              "      <td>0.630753</td>\n",
              "      <td>0.341884</td>\n",
              "      <td>0.502403</td>\n",
              "      <td>0.719198</td>\n",
              "      <td>-0.114545</td>\n",
              "      <td>0.415689</td>\n",
              "      <td>0.858130</td>\n",
              "      <td>0.028379</td>\n",
              "      <td>0.405933</td>\n",
              "      <td>0.861435</td>\n",
              "      <td>0.026839</td>\n",
              "      <td>0.456500</td>\n",
              "      <td>0.724646</td>\n",
              "      <td>-0.078316</td>\n",
              "      <td>0.398571</td>\n",
              "      <td>0.712659</td>\n",
              "      <td>-0.046950</td>\n",
              "      <td>0.450282</td>\n",
              "      <td>0.718590</td>\n",
              "      <td>-0.081530</td>\n",
              "      <td>0.455816</td>\n",
              "      <td>0.346828</td>\n",
              "      <td>-0.125290</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14727</th>\n",
              "      <td>0.560496</td>\n",
              "      <td>0.769144</td>\n",
              "      <td>-0.111749</td>\n",
              "      <td>0.395681</td>\n",
              "      <td>0.565257</td>\n",
              "      <td>-0.017007</td>\n",
              "      <td>0.342978</td>\n",
              "      <td>0.594650</td>\n",
              "      <td>-0.008701</td>\n",
              "      <td>0.431437</td>\n",
              "      <td>0.644474</td>\n",
              "      <td>-0.072280</td>\n",
              "      <td>0.212711</td>\n",
              "      <td>0.199529</td>\n",
              "      <td>0.095772</td>\n",
              "      <td>0.235907</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>0.043887</td>\n",
              "      <td>0.261247</td>\n",
              "      <td>0.286265</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>0.444982</td>\n",
              "      <td>0.899928</td>\n",
              "      <td>-0.032473</td>\n",
              "      <td>0.421535</td>\n",
              "      <td>0.284107</td>\n",
              "      <td>-0.043228</td>\n",
              "      <td>0.409811</td>\n",
              "      <td>0.212278</td>\n",
              "      <td>-0.018695</td>\n",
              "      <td>0.392661</td>\n",
              "      <td>0.132065</td>\n",
              "      <td>0.010029</td>\n",
              "      <td>0.507709</td>\n",
              "      <td>0.118564</td>\n",
              "      <td>0.007106</td>\n",
              "      <td>0.279881</td>\n",
              "      <td>0.494782</td>\n",
              "      <td>0.037404</td>\n",
              "      <td>0.207185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.479727</td>\n",
              "      <td>0.865063</td>\n",
              "      <td>-0.052868</td>\n",
              "      <td>0.521913</td>\n",
              "      <td>0.335642</td>\n",
              "      <td>-0.041206</td>\n",
              "      <td>0.468909</td>\n",
              "      <td>0.876704</td>\n",
              "      <td>-0.059213</td>\n",
              "      <td>0.459812</td>\n",
              "      <td>0.887839</td>\n",
              "      <td>-0.054344</td>\n",
              "      <td>0.422744</td>\n",
              "      <td>0.770018</td>\n",
              "      <td>-0.057373</td>\n",
              "      <td>0.168215</td>\n",
              "      <td>0.636903</td>\n",
              "      <td>0.338159</td>\n",
              "      <td>0.546422</td>\n",
              "      <td>0.660709</td>\n",
              "      <td>-0.123501</td>\n",
              "      <td>0.473307</td>\n",
              "      <td>0.854591</td>\n",
              "      <td>-0.037045</td>\n",
              "      <td>0.464889</td>\n",
              "      <td>0.859350</td>\n",
              "      <td>-0.040605</td>\n",
              "      <td>0.506149</td>\n",
              "      <td>0.682234</td>\n",
              "      <td>-0.094430</td>\n",
              "      <td>0.455463</td>\n",
              "      <td>0.684035</td>\n",
              "      <td>-0.061109</td>\n",
              "      <td>0.500817</td>\n",
              "      <td>0.675388</td>\n",
              "      <td>-0.092881</td>\n",
              "      <td>0.517969</td>\n",
              "      <td>0.281741</td>\n",
              "      <td>-0.043033</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14728</th>\n",
              "      <td>0.491013</td>\n",
              "      <td>0.712008</td>\n",
              "      <td>-0.122762</td>\n",
              "      <td>0.370838</td>\n",
              "      <td>0.521274</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.321248</td>\n",
              "      <td>0.549112</td>\n",
              "      <td>0.029226</td>\n",
              "      <td>0.397335</td>\n",
              "      <td>0.598340</td>\n",
              "      <td>-0.050178</td>\n",
              "      <td>0.231024</td>\n",
              "      <td>0.155950</td>\n",
              "      <td>0.198852</td>\n",
              "      <td>0.244556</td>\n",
              "      <td>0.193912</td>\n",
              "      <td>0.136056</td>\n",
              "      <td>0.262082</td>\n",
              "      <td>0.232480</td>\n",
              "      <td>0.084471</td>\n",
              "      <td>0.410388</td>\n",
              "      <td>0.861402</td>\n",
              "      <td>-0.035873</td>\n",
              "      <td>0.396322</td>\n",
              "      <td>0.229427</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.388710</td>\n",
              "      <td>0.161868</td>\n",
              "      <td>0.052182</td>\n",
              "      <td>0.378511</td>\n",
              "      <td>0.085723</td>\n",
              "      <td>0.091270</td>\n",
              "      <td>0.480310</td>\n",
              "      <td>0.072318</td>\n",
              "      <td>0.071249</td>\n",
              "      <td>0.277920</td>\n",
              "      <td>0.448965</td>\n",
              "      <td>0.099246</td>\n",
              "      <td>0.215149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.437324</td>\n",
              "      <td>0.820451</td>\n",
              "      <td>-0.054481</td>\n",
              "      <td>0.485971</td>\n",
              "      <td>0.284793</td>\n",
              "      <td>-0.002835</td>\n",
              "      <td>0.426605</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>-0.060726</td>\n",
              "      <td>0.418768</td>\n",
              "      <td>0.843263</td>\n",
              "      <td>-0.056251</td>\n",
              "      <td>0.383449</td>\n",
              "      <td>0.718743</td>\n",
              "      <td>-0.044822</td>\n",
              "      <td>0.213713</td>\n",
              "      <td>0.614015</td>\n",
              "      <td>0.380890</td>\n",
              "      <td>0.485615</td>\n",
              "      <td>0.622031</td>\n",
              "      <td>-0.120505</td>\n",
              "      <td>0.436568</td>\n",
              "      <td>0.807114</td>\n",
              "      <td>-0.036760</td>\n",
              "      <td>0.426990</td>\n",
              "      <td>0.810267</td>\n",
              "      <td>-0.040235</td>\n",
              "      <td>0.456969</td>\n",
              "      <td>0.639337</td>\n",
              "      <td>-0.088925</td>\n",
              "      <td>0.418537</td>\n",
              "      <td>0.638220</td>\n",
              "      <td>-0.049196</td>\n",
              "      <td>0.452996</td>\n",
              "      <td>0.632927</td>\n",
              "      <td>-0.086052</td>\n",
              "      <td>0.482375</td>\n",
              "      <td>0.228888</td>\n",
              "      <td>0.002328</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14729</th>\n",
              "      <td>0.527434</td>\n",
              "      <td>0.738273</td>\n",
              "      <td>-0.079975</td>\n",
              "      <td>0.328769</td>\n",
              "      <td>0.457341</td>\n",
              "      <td>-0.067171</td>\n",
              "      <td>0.263649</td>\n",
              "      <td>0.484076</td>\n",
              "      <td>-0.066484</td>\n",
              "      <td>0.365410</td>\n",
              "      <td>0.582288</td>\n",
              "      <td>-0.097822</td>\n",
              "      <td>0.136217</td>\n",
              "      <td>0.021140</td>\n",
              "      <td>-0.074558</td>\n",
              "      <td>0.175402</td>\n",
              "      <td>0.085695</td>\n",
              "      <td>-0.108161</td>\n",
              "      <td>0.210071</td>\n",
              "      <td>0.146272</td>\n",
              "      <td>-0.132795</td>\n",
              "      <td>0.343199</td>\n",
              "      <td>0.830920</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>0.406371</td>\n",
              "      <td>0.175288</td>\n",
              "      <td>-0.141420</td>\n",
              "      <td>0.396257</td>\n",
              "      <td>0.092930</td>\n",
              "      <td>-0.141256</td>\n",
              "      <td>0.377956</td>\n",
              "      <td>-0.004383</td>\n",
              "      <td>-0.133272</td>\n",
              "      <td>0.510257</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>-0.116076</td>\n",
              "      <td>0.188901</td>\n",
              "      <td>0.338566</td>\n",
              "      <td>-0.055147</td>\n",
              "      <td>0.084274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.396311</td>\n",
              "      <td>0.798353</td>\n",
              "      <td>-0.025366</td>\n",
              "      <td>0.508404</td>\n",
              "      <td>0.246948</td>\n",
              "      <td>-0.110513</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>0.809397</td>\n",
              "      <td>-0.028493</td>\n",
              "      <td>0.373280</td>\n",
              "      <td>0.819410</td>\n",
              "      <td>-0.022557</td>\n",
              "      <td>0.337109</td>\n",
              "      <td>0.707447</td>\n",
              "      <td>-0.056202</td>\n",
              "      <td>-0.054723</td>\n",
              "      <td>0.439372</td>\n",
              "      <td>0.278810</td>\n",
              "      <td>0.532050</td>\n",
              "      <td>0.647020</td>\n",
              "      <td>-0.122507</td>\n",
              "      <td>0.381318</td>\n",
              "      <td>0.785802</td>\n",
              "      <td>-0.011244</td>\n",
              "      <td>0.373739</td>\n",
              "      <td>0.790772</td>\n",
              "      <td>-0.014106</td>\n",
              "      <td>0.463954</td>\n",
              "      <td>0.650501</td>\n",
              "      <td>-0.093005</td>\n",
              "      <td>0.385598</td>\n",
              "      <td>0.631218</td>\n",
              "      <td>-0.068217</td>\n",
              "      <td>0.455586</td>\n",
              "      <td>0.642425</td>\n",
              "      <td>-0.095598</td>\n",
              "      <td>0.511577</td>\n",
              "      <td>0.192825</td>\n",
              "      <td>-0.125415</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14730</th>\n",
              "      <td>0.528845</td>\n",
              "      <td>0.735577</td>\n",
              "      <td>-0.041305</td>\n",
              "      <td>0.342966</td>\n",
              "      <td>0.537490</td>\n",
              "      <td>-0.021240</td>\n",
              "      <td>0.291597</td>\n",
              "      <td>0.566791</td>\n",
              "      <td>-0.001343</td>\n",
              "      <td>0.376911</td>\n",
              "      <td>0.633230</td>\n",
              "      <td>-0.048700</td>\n",
              "      <td>0.118212</td>\n",
              "      <td>0.205293</td>\n",
              "      <td>-0.040242</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.265181</td>\n",
              "      <td>-0.071281</td>\n",
              "      <td>0.188816</td>\n",
              "      <td>0.329133</td>\n",
              "      <td>-0.094329</td>\n",
              "      <td>0.402845</td>\n",
              "      <td>0.801184</td>\n",
              "      <td>0.070091</td>\n",
              "      <td>0.351299</td>\n",
              "      <td>0.319882</td>\n",
              "      <td>-0.148940</td>\n",
              "      <td>0.321788</td>\n",
              "      <td>0.228375</td>\n",
              "      <td>-0.157065</td>\n",
              "      <td>0.290454</td>\n",
              "      <td>0.137655</td>\n",
              "      <td>-0.158898</td>\n",
              "      <td>0.406340</td>\n",
              "      <td>0.110980</td>\n",
              "      <td>-0.176145</td>\n",
              "      <td>0.219970</td>\n",
              "      <td>0.473707</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>0.150090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.425510</td>\n",
              "      <td>0.768059</td>\n",
              "      <td>0.035476</td>\n",
              "      <td>0.450491</td>\n",
              "      <td>0.353091</td>\n",
              "      <td>-0.136861</td>\n",
              "      <td>0.419335</td>\n",
              "      <td>0.775359</td>\n",
              "      <td>0.032669</td>\n",
              "      <td>0.414294</td>\n",
              "      <td>0.783746</td>\n",
              "      <td>0.040324</td>\n",
              "      <td>0.367322</td>\n",
              "      <td>0.733595</td>\n",
              "      <td>0.017130</td>\n",
              "      <td>0.119499</td>\n",
              "      <td>0.563274</td>\n",
              "      <td>0.379588</td>\n",
              "      <td>0.510960</td>\n",
              "      <td>0.668780</td>\n",
              "      <td>-0.098167</td>\n",
              "      <td>0.413863</td>\n",
              "      <td>0.767714</td>\n",
              "      <td>0.057815</td>\n",
              "      <td>0.407349</td>\n",
              "      <td>0.770133</td>\n",
              "      <td>0.055871</td>\n",
              "      <td>0.462658</td>\n",
              "      <td>0.677776</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.400108</td>\n",
              "      <td>0.668224</td>\n",
              "      <td>-0.026347</td>\n",
              "      <td>0.454312</td>\n",
              "      <td>0.672526</td>\n",
              "      <td>-0.062448</td>\n",
              "      <td>0.442451</td>\n",
              "      <td>0.308193</td>\n",
              "      <td>-0.157336</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14731</th>\n",
              "      <td>0.452533</td>\n",
              "      <td>0.791973</td>\n",
              "      <td>-0.099548</td>\n",
              "      <td>0.335543</td>\n",
              "      <td>0.535332</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.284853</td>\n",
              "      <td>0.564165</td>\n",
              "      <td>0.029366</td>\n",
              "      <td>0.351197</td>\n",
              "      <td>0.637508</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>0.184646</td>\n",
              "      <td>0.126609</td>\n",
              "      <td>0.138878</td>\n",
              "      <td>0.201092</td>\n",
              "      <td>0.182896</td>\n",
              "      <td>0.081410</td>\n",
              "      <td>0.219815</td>\n",
              "      <td>0.240867</td>\n",
              "      <td>0.035379</td>\n",
              "      <td>0.358867</td>\n",
              "      <td>0.892018</td>\n",
              "      <td>-0.009873</td>\n",
              "      <td>0.362378</td>\n",
              "      <td>0.247935</td>\n",
              "      <td>-0.036624</td>\n",
              "      <td>0.353144</td>\n",
              "      <td>0.162810</td>\n",
              "      <td>-0.012710</td>\n",
              "      <td>0.343602</td>\n",
              "      <td>0.071618</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.451449</td>\n",
              "      <td>0.063975</td>\n",
              "      <td>-0.008453</td>\n",
              "      <td>0.239168</td>\n",
              "      <td>0.439385</td>\n",
              "      <td>0.086585</td>\n",
              "      <td>0.170206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.383385</td>\n",
              "      <td>0.848754</td>\n",
              "      <td>-0.034963</td>\n",
              "      <td>0.452250</td>\n",
              "      <td>0.307410</td>\n",
              "      <td>-0.048951</td>\n",
              "      <td>0.375028</td>\n",
              "      <td>0.858712</td>\n",
              "      <td>-0.041104</td>\n",
              "      <td>0.368408</td>\n",
              "      <td>0.870020</td>\n",
              "      <td>-0.035212</td>\n",
              "      <td>0.335122</td>\n",
              "      <td>0.770662</td>\n",
              "      <td>-0.023500</td>\n",
              "      <td>0.156868</td>\n",
              "      <td>0.588514</td>\n",
              "      <td>0.422205</td>\n",
              "      <td>0.446972</td>\n",
              "      <td>0.681988</td>\n",
              "      <td>-0.115878</td>\n",
              "      <td>0.378910</td>\n",
              "      <td>0.843526</td>\n",
              "      <td>-0.012812</td>\n",
              "      <td>0.371462</td>\n",
              "      <td>0.847001</td>\n",
              "      <td>-0.016520</td>\n",
              "      <td>0.412317</td>\n",
              "      <td>0.695334</td>\n",
              "      <td>-0.079299</td>\n",
              "      <td>0.370427</td>\n",
              "      <td>0.685399</td>\n",
              "      <td>-0.037217</td>\n",
              "      <td>0.407154</td>\n",
              "      <td>0.687007</td>\n",
              "      <td>-0.078226</td>\n",
              "      <td>0.449482</td>\n",
              "      <td>0.251166</td>\n",
              "      <td>-0.053193</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14732 rows × 1405 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_y       9_z  correct\n",
              "0      0.516601  0.585682 -0.128171  ...  0.187230  0.006673        0\n",
              "1      0.350142  0.704083 -0.051000  ...  0.274136 -0.123009        0\n",
              "2      0.478097  0.829416 -0.035864  ...  0.401297 -0.201465        0\n",
              "3      0.440407  0.633268 -0.130409  ...  0.249019 -0.042549        0\n",
              "4      0.508843  0.783509 -0.075715  ...  0.346828 -0.125290        0\n",
              "...         ...       ...       ...  ...       ...       ...      ...\n",
              "14727  0.560496  0.769144 -0.111749  ...  0.281741 -0.043033        6\n",
              "14728  0.491013  0.712008 -0.122762  ...  0.228888  0.002328        6\n",
              "14729  0.527434  0.738273 -0.079975  ...  0.192825 -0.125415        6\n",
              "14730  0.528845  0.735577 -0.041305  ...  0.308193 -0.157336        6\n",
              "14731  0.452533  0.791973 -0.099548  ...  0.251166 -0.053193        6\n",
              "\n",
              "[14732 rows x 1405 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "pDKn2CRL1V_6",
        "outputId": "ca3840f8-a4ba-4f2f-c301-4d18bcf89a39"
      },
      "source": [
        "df_valid.drop(columns=df_valid.columns[[-2]],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.646184</td>\n",
              "      <td>0.771045</td>\n",
              "      <td>-0.064509</td>\n",
              "      <td>0.479793</td>\n",
              "      <td>0.566351</td>\n",
              "      <td>-0.088064</td>\n",
              "      <td>0.426735</td>\n",
              "      <td>0.590875</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>0.520683</td>\n",
              "      <td>0.649389</td>\n",
              "      <td>-0.101141</td>\n",
              "      <td>0.297111</td>\n",
              "      <td>0.243108</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>0.351343</td>\n",
              "      <td>0.308081</td>\n",
              "      <td>-0.161240</td>\n",
              "      <td>0.396239</td>\n",
              "      <td>0.377762</td>\n",
              "      <td>-0.175256</td>\n",
              "      <td>0.463307</td>\n",
              "      <td>0.887630</td>\n",
              "      <td>-0.003552</td>\n",
              "      <td>0.560809</td>\n",
              "      <td>0.397581</td>\n",
              "      <td>-0.148304</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.302792</td>\n",
              "      <td>-0.148388</td>\n",
              "      <td>0.513932</td>\n",
              "      <td>0.218670</td>\n",
              "      <td>-0.145369</td>\n",
              "      <td>0.618115</td>\n",
              "      <td>0.221551</td>\n",
              "      <td>-0.109952</td>\n",
              "      <td>0.356944</td>\n",
              "      <td>0.489348</td>\n",
              "      <td>-0.107089</td>\n",
              "      <td>0.258764</td>\n",
              "      <td>...</td>\n",
              "      <td>0.514319</td>\n",
              "      <td>0.861163</td>\n",
              "      <td>-0.021057</td>\n",
              "      <td>0.630754</td>\n",
              "      <td>0.437828</td>\n",
              "      <td>-0.104779</td>\n",
              "      <td>0.503365</td>\n",
              "      <td>0.870509</td>\n",
              "      <td>-0.021248</td>\n",
              "      <td>0.491106</td>\n",
              "      <td>0.878412</td>\n",
              "      <td>-0.018093</td>\n",
              "      <td>0.482451</td>\n",
              "      <td>0.760005</td>\n",
              "      <td>-0.072059</td>\n",
              "      <td>0.072792</td>\n",
              "      <td>0.583212</td>\n",
              "      <td>0.110986</td>\n",
              "      <td>0.669708</td>\n",
              "      <td>0.690351</td>\n",
              "      <td>-0.098678</td>\n",
              "      <td>0.497744</td>\n",
              "      <td>0.844740</td>\n",
              "      <td>-0.020899</td>\n",
              "      <td>0.490786</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>-0.021804</td>\n",
              "      <td>0.605350</td>\n",
              "      <td>0.697076</td>\n",
              "      <td>-0.082129</td>\n",
              "      <td>0.535840</td>\n",
              "      <td>0.685600</td>\n",
              "      <td>-0.071414</td>\n",
              "      <td>0.600393</td>\n",
              "      <td>0.690829</td>\n",
              "      <td>-0.085879</td>\n",
              "      <td>0.633086</td>\n",
              "      <td>0.403363</td>\n",
              "      <td>-0.117815</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.505669</td>\n",
              "      <td>0.783729</td>\n",
              "      <td>-0.018900</td>\n",
              "      <td>0.383348</td>\n",
              "      <td>0.551676</td>\n",
              "      <td>-0.021686</td>\n",
              "      <td>0.335575</td>\n",
              "      <td>0.573097</td>\n",
              "      <td>-0.002283</td>\n",
              "      <td>0.399952</td>\n",
              "      <td>0.650358</td>\n",
              "      <td>-0.031835</td>\n",
              "      <td>0.229131</td>\n",
              "      <td>0.181487</td>\n",
              "      <td>-0.092331</td>\n",
              "      <td>0.258003</td>\n",
              "      <td>0.257158</td>\n",
              "      <td>-0.111276</td>\n",
              "      <td>0.284237</td>\n",
              "      <td>0.338158</td>\n",
              "      <td>-0.123996</td>\n",
              "      <td>0.408235</td>\n",
              "      <td>0.862809</td>\n",
              "      <td>0.118722</td>\n",
              "      <td>0.414858</td>\n",
              "      <td>0.365437</td>\n",
              "      <td>-0.173962</td>\n",
              "      <td>0.397574</td>\n",
              "      <td>0.252688</td>\n",
              "      <td>-0.191834</td>\n",
              "      <td>0.380021</td>\n",
              "      <td>0.148873</td>\n",
              "      <td>-0.204391</td>\n",
              "      <td>0.480591</td>\n",
              "      <td>0.143603</td>\n",
              "      <td>-0.224993</td>\n",
              "      <td>0.294881</td>\n",
              "      <td>0.460291</td>\n",
              "      <td>-0.004822</td>\n",
              "      <td>0.217984</td>\n",
              "      <td>...</td>\n",
              "      <td>0.435181</td>\n",
              "      <td>0.831755</td>\n",
              "      <td>0.075494</td>\n",
              "      <td>0.490290</td>\n",
              "      <td>0.410842</td>\n",
              "      <td>-0.158440</td>\n",
              "      <td>0.426757</td>\n",
              "      <td>0.840886</td>\n",
              "      <td>0.078719</td>\n",
              "      <td>0.419972</td>\n",
              "      <td>0.850027</td>\n",
              "      <td>0.088572</td>\n",
              "      <td>0.383190</td>\n",
              "      <td>0.750469</td>\n",
              "      <td>0.039978</td>\n",
              "      <td>0.169034</td>\n",
              "      <td>0.535456</td>\n",
              "      <td>0.333599</td>\n",
              "      <td>0.499905</td>\n",
              "      <td>0.714165</td>\n",
              "      <td>-0.075163</td>\n",
              "      <td>0.430074</td>\n",
              "      <td>0.816941</td>\n",
              "      <td>0.089510</td>\n",
              "      <td>0.422110</td>\n",
              "      <td>0.818853</td>\n",
              "      <td>0.090431</td>\n",
              "      <td>0.462675</td>\n",
              "      <td>0.711085</td>\n",
              "      <td>-0.036148</td>\n",
              "      <td>0.415931</td>\n",
              "      <td>0.687106</td>\n",
              "      <td>-0.008565</td>\n",
              "      <td>0.457287</td>\n",
              "      <td>0.704161</td>\n",
              "      <td>-0.042259</td>\n",
              "      <td>0.487358</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>-0.183734</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.570398</td>\n",
              "      <td>0.522947</td>\n",
              "      <td>-0.131259</td>\n",
              "      <td>0.385422</td>\n",
              "      <td>0.250885</td>\n",
              "      <td>-0.050247</td>\n",
              "      <td>0.313107</td>\n",
              "      <td>0.278246</td>\n",
              "      <td>-0.048979</td>\n",
              "      <td>0.409144</td>\n",
              "      <td>0.370157</td>\n",
              "      <td>-0.113957</td>\n",
              "      <td>0.206225</td>\n",
              "      <td>-0.195378</td>\n",
              "      <td>0.061084</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>-0.130779</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.276074</td>\n",
              "      <td>-0.063120</td>\n",
              "      <td>-0.046078</td>\n",
              "      <td>0.358516</td>\n",
              "      <td>0.730264</td>\n",
              "      <td>0.017391</td>\n",
              "      <td>0.468482</td>\n",
              "      <td>-0.048374</td>\n",
              "      <td>-0.084189</td>\n",
              "      <td>0.454622</td>\n",
              "      <td>-0.150500</td>\n",
              "      <td>-0.053712</td>\n",
              "      <td>0.437219</td>\n",
              "      <td>-0.252034</td>\n",
              "      <td>-0.025488</td>\n",
              "      <td>0.571085</td>\n",
              "      <td>-0.254265</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.253504</td>\n",
              "      <td>0.142910</td>\n",
              "      <td>0.007577</td>\n",
              "      <td>0.149256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.407505</td>\n",
              "      <td>0.706014</td>\n",
              "      <td>-0.006454</td>\n",
              "      <td>0.569965</td>\n",
              "      <td>0.023101</td>\n",
              "      <td>-0.076176</td>\n",
              "      <td>0.390708</td>\n",
              "      <td>0.716519</td>\n",
              "      <td>-0.009776</td>\n",
              "      <td>0.376529</td>\n",
              "      <td>0.725704</td>\n",
              "      <td>-0.003093</td>\n",
              "      <td>0.352764</td>\n",
              "      <td>0.512117</td>\n",
              "      <td>-0.059380</td>\n",
              "      <td>0.041792</td>\n",
              "      <td>0.330219</td>\n",
              "      <td>0.345695</td>\n",
              "      <td>0.581562</td>\n",
              "      <td>0.438015</td>\n",
              "      <td>-0.170266</td>\n",
              "      <td>0.391589</td>\n",
              "      <td>0.667979</td>\n",
              "      <td>0.011425</td>\n",
              "      <td>0.375232</td>\n",
              "      <td>0.668794</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.507817</td>\n",
              "      <td>0.442177</td>\n",
              "      <td>-0.133319</td>\n",
              "      <td>0.426135</td>\n",
              "      <td>0.421102</td>\n",
              "      <td>-0.094905</td>\n",
              "      <td>0.501055</td>\n",
              "      <td>0.433366</td>\n",
              "      <td>-0.135095</td>\n",
              "      <td>0.571534</td>\n",
              "      <td>-0.036601</td>\n",
              "      <td>-0.077871</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.550640</td>\n",
              "      <td>0.693204</td>\n",
              "      <td>-0.039426</td>\n",
              "      <td>0.377895</td>\n",
              "      <td>0.420303</td>\n",
              "      <td>-0.049635</td>\n",
              "      <td>0.310557</td>\n",
              "      <td>0.444947</td>\n",
              "      <td>-0.030445</td>\n",
              "      <td>0.395424</td>\n",
              "      <td>0.535625</td>\n",
              "      <td>-0.071252</td>\n",
              "      <td>0.196327</td>\n",
              "      <td>0.037854</td>\n",
              "      <td>-0.122527</td>\n",
              "      <td>0.226640</td>\n",
              "      <td>0.102505</td>\n",
              "      <td>-0.149672</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.168133</td>\n",
              "      <td>-0.169691</td>\n",
              "      <td>0.385633</td>\n",
              "      <td>0.777333</td>\n",
              "      <td>0.130624</td>\n",
              "      <td>0.440921</td>\n",
              "      <td>0.182286</td>\n",
              "      <td>-0.220092</td>\n",
              "      <td>0.426143</td>\n",
              "      <td>0.093054</td>\n",
              "      <td>-0.238092</td>\n",
              "      <td>0.408764</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.251741</td>\n",
              "      <td>0.543859</td>\n",
              "      <td>-0.005355</td>\n",
              "      <td>-0.265706</td>\n",
              "      <td>0.259817</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>-0.029762</td>\n",
              "      <td>0.159518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.425009</td>\n",
              "      <td>0.744009</td>\n",
              "      <td>0.082338</td>\n",
              "      <td>0.545075</td>\n",
              "      <td>0.237159</td>\n",
              "      <td>-0.195135</td>\n",
              "      <td>0.414130</td>\n",
              "      <td>0.752458</td>\n",
              "      <td>0.083467</td>\n",
              "      <td>0.404759</td>\n",
              "      <td>0.761788</td>\n",
              "      <td>0.094698</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>0.646520</td>\n",
              "      <td>0.029753</td>\n",
              "      <td>0.087381</td>\n",
              "      <td>0.425480</td>\n",
              "      <td>0.414335</td>\n",
              "      <td>0.550101</td>\n",
              "      <td>0.611782</td>\n",
              "      <td>-0.120470</td>\n",
              "      <td>0.411218</td>\n",
              "      <td>0.722901</td>\n",
              "      <td>0.103741</td>\n",
              "      <td>0.399978</td>\n",
              "      <td>0.723879</td>\n",
              "      <td>0.104076</td>\n",
              "      <td>0.488015</td>\n",
              "      <td>0.608906</td>\n",
              "      <td>-0.072477</td>\n",
              "      <td>0.414568</td>\n",
              "      <td>0.579445</td>\n",
              "      <td>-0.037483</td>\n",
              "      <td>0.479306</td>\n",
              "      <td>0.600204</td>\n",
              "      <td>-0.079331</td>\n",
              "      <td>0.544766</td>\n",
              "      <td>0.188114</td>\n",
              "      <td>-0.223748</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.524232</td>\n",
              "      <td>0.722622</td>\n",
              "      <td>-0.049771</td>\n",
              "      <td>0.378299</td>\n",
              "      <td>0.537701</td>\n",
              "      <td>-0.022929</td>\n",
              "      <td>0.332367</td>\n",
              "      <td>0.561998</td>\n",
              "      <td>-0.011429</td>\n",
              "      <td>0.401680</td>\n",
              "      <td>0.617495</td>\n",
              "      <td>-0.051351</td>\n",
              "      <td>0.205039</td>\n",
              "      <td>0.227481</td>\n",
              "      <td>-0.018862</td>\n",
              "      <td>0.235042</td>\n",
              "      <td>0.282814</td>\n",
              "      <td>-0.050678</td>\n",
              "      <td>0.265167</td>\n",
              "      <td>0.346141</td>\n",
              "      <td>-0.075391</td>\n",
              "      <td>0.425222</td>\n",
              "      <td>0.818088</td>\n",
              "      <td>0.049098</td>\n",
              "      <td>0.399753</td>\n",
              "      <td>0.352436</td>\n",
              "      <td>-0.117018</td>\n",
              "      <td>0.375707</td>\n",
              "      <td>0.255840</td>\n",
              "      <td>-0.116756</td>\n",
              "      <td>0.351857</td>\n",
              "      <td>0.170662</td>\n",
              "      <td>-0.111585</td>\n",
              "      <td>0.448775</td>\n",
              "      <td>0.154980</td>\n",
              "      <td>-0.122691</td>\n",
              "      <td>0.283837</td>\n",
              "      <td>0.472371</td>\n",
              "      <td>0.007503</td>\n",
              "      <td>0.215384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.447646</td>\n",
              "      <td>0.779411</td>\n",
              "      <td>0.023573</td>\n",
              "      <td>0.478230</td>\n",
              "      <td>0.386732</td>\n",
              "      <td>-0.106638</td>\n",
              "      <td>0.441388</td>\n",
              "      <td>0.787471</td>\n",
              "      <td>0.022020</td>\n",
              "      <td>0.435998</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.027950</td>\n",
              "      <td>0.390656</td>\n",
              "      <td>0.715117</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.174290</td>\n",
              "      <td>0.598808</td>\n",
              "      <td>0.309484</td>\n",
              "      <td>0.511831</td>\n",
              "      <td>0.657377</td>\n",
              "      <td>-0.094378</td>\n",
              "      <td>0.437287</td>\n",
              "      <td>0.769950</td>\n",
              "      <td>0.040689</td>\n",
              "      <td>0.429929</td>\n",
              "      <td>0.771605</td>\n",
              "      <td>0.039537</td>\n",
              "      <td>0.471688</td>\n",
              "      <td>0.662292</td>\n",
              "      <td>-0.062175</td>\n",
              "      <td>0.420203</td>\n",
              "      <td>0.650105</td>\n",
              "      <td>-0.033778</td>\n",
              "      <td>0.465479</td>\n",
              "      <td>0.656399</td>\n",
              "      <td>-0.065202</td>\n",
              "      <td>0.473144</td>\n",
              "      <td>0.346925</td>\n",
              "      <td>-0.121656</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14631</th>\n",
              "      <td>0.529442</td>\n",
              "      <td>0.667210</td>\n",
              "      <td>-0.066573</td>\n",
              "      <td>0.386126</td>\n",
              "      <td>0.486287</td>\n",
              "      <td>-0.033634</td>\n",
              "      <td>0.341563</td>\n",
              "      <td>0.510485</td>\n",
              "      <td>-0.029726</td>\n",
              "      <td>0.418692</td>\n",
              "      <td>0.560215</td>\n",
              "      <td>-0.061615</td>\n",
              "      <td>0.219367</td>\n",
              "      <td>0.183714</td>\n",
              "      <td>-0.007092</td>\n",
              "      <td>0.248392</td>\n",
              "      <td>0.228448</td>\n",
              "      <td>-0.037222</td>\n",
              "      <td>0.276280</td>\n",
              "      <td>0.271904</td>\n",
              "      <td>-0.061118</td>\n",
              "      <td>0.423037</td>\n",
              "      <td>0.759102</td>\n",
              "      <td>-0.001626</td>\n",
              "      <td>0.413478</td>\n",
              "      <td>0.273705</td>\n",
              "      <td>-0.080519</td>\n",
              "      <td>0.398730</td>\n",
              "      <td>0.209388</td>\n",
              "      <td>-0.074626</td>\n",
              "      <td>0.379055</td>\n",
              "      <td>0.137959</td>\n",
              "      <td>-0.063146</td>\n",
              "      <td>0.475969</td>\n",
              "      <td>0.127867</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>0.284961</td>\n",
              "      <td>0.422919</td>\n",
              "      <td>-0.011928</td>\n",
              "      <td>0.216206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454311</td>\n",
              "      <td>0.732706</td>\n",
              "      <td>-0.021548</td>\n",
              "      <td>0.494808</td>\n",
              "      <td>0.313197</td>\n",
              "      <td>-0.065520</td>\n",
              "      <td>0.445838</td>\n",
              "      <td>0.741607</td>\n",
              "      <td>-0.024574</td>\n",
              "      <td>0.437814</td>\n",
              "      <td>0.749662</td>\n",
              "      <td>-0.020488</td>\n",
              "      <td>0.407561</td>\n",
              "      <td>0.660633</td>\n",
              "      <td>-0.038206</td>\n",
              "      <td>0.153465</td>\n",
              "      <td>0.517849</td>\n",
              "      <td>0.231584</td>\n",
              "      <td>0.524237</td>\n",
              "      <td>0.586032</td>\n",
              "      <td>-0.089157</td>\n",
              "      <td>0.447015</td>\n",
              "      <td>0.723673</td>\n",
              "      <td>-0.012398</td>\n",
              "      <td>0.440423</td>\n",
              "      <td>0.727365</td>\n",
              "      <td>-0.014400</td>\n",
              "      <td>0.484241</td>\n",
              "      <td>0.597897</td>\n",
              "      <td>-0.066358</td>\n",
              "      <td>0.436844</td>\n",
              "      <td>0.592494</td>\n",
              "      <td>-0.044895</td>\n",
              "      <td>0.479310</td>\n",
              "      <td>0.592026</td>\n",
              "      <td>-0.067038</td>\n",
              "      <td>0.491907</td>\n",
              "      <td>0.271605</td>\n",
              "      <td>-0.073554</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14632</th>\n",
              "      <td>0.500539</td>\n",
              "      <td>0.734860</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>0.327387</td>\n",
              "      <td>0.516931</td>\n",
              "      <td>-0.010588</td>\n",
              "      <td>0.265571</td>\n",
              "      <td>0.545812</td>\n",
              "      <td>-0.007016</td>\n",
              "      <td>0.361814</td>\n",
              "      <td>0.604152</td>\n",
              "      <td>-0.083395</td>\n",
              "      <td>0.160265</td>\n",
              "      <td>0.151897</td>\n",
              "      <td>0.173618</td>\n",
              "      <td>0.184835</td>\n",
              "      <td>0.188428</td>\n",
              "      <td>0.104391</td>\n",
              "      <td>0.210830</td>\n",
              "      <td>0.227435</td>\n",
              "      <td>0.047180</td>\n",
              "      <td>0.350158</td>\n",
              "      <td>0.870057</td>\n",
              "      <td>-0.090012</td>\n",
              "      <td>0.386060</td>\n",
              "      <td>0.230485</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.377281</td>\n",
              "      <td>0.166759</td>\n",
              "      <td>0.043994</td>\n",
              "      <td>0.364579</td>\n",
              "      <td>0.095391</td>\n",
              "      <td>0.089954</td>\n",
              "      <td>0.487561</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.089601</td>\n",
              "      <td>0.203078</td>\n",
              "      <td>0.435127</td>\n",
              "      <td>0.063342</td>\n",
              "      <td>0.114195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.393957</td>\n",
              "      <td>0.820860</td>\n",
              "      <td>-0.104007</td>\n",
              "      <td>0.487804</td>\n",
              "      <td>0.291259</td>\n",
              "      <td>-0.005642</td>\n",
              "      <td>0.382895</td>\n",
              "      <td>0.832251</td>\n",
              "      <td>-0.113706</td>\n",
              "      <td>0.372782</td>\n",
              "      <td>0.845191</td>\n",
              "      <td>-0.110114</td>\n",
              "      <td>0.337598</td>\n",
              "      <td>0.735444</td>\n",
              "      <td>-0.089631</td>\n",
              "      <td>0.034739</td>\n",
              "      <td>0.615029</td>\n",
              "      <td>0.373997</td>\n",
              "      <td>0.498705</td>\n",
              "      <td>0.626003</td>\n",
              "      <td>-0.145978</td>\n",
              "      <td>0.384501</td>\n",
              "      <td>0.817192</td>\n",
              "      <td>-0.083325</td>\n",
              "      <td>0.376356</td>\n",
              "      <td>0.821481</td>\n",
              "      <td>-0.089008</td>\n",
              "      <td>0.447718</td>\n",
              "      <td>0.647504</td>\n",
              "      <td>-0.116903</td>\n",
              "      <td>0.385622</td>\n",
              "      <td>0.649125</td>\n",
              "      <td>-0.076227</td>\n",
              "      <td>0.441510</td>\n",
              "      <td>0.639893</td>\n",
              "      <td>-0.113286</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.238567</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14633</th>\n",
              "      <td>0.494283</td>\n",
              "      <td>0.710024</td>\n",
              "      <td>-0.116081</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.497803</td>\n",
              "      <td>-0.009445</td>\n",
              "      <td>0.295208</td>\n",
              "      <td>0.520917</td>\n",
              "      <td>-0.001318</td>\n",
              "      <td>0.381529</td>\n",
              "      <td>0.580622</td>\n",
              "      <td>-0.068246</td>\n",
              "      <td>0.204759</td>\n",
              "      <td>0.123697</td>\n",
              "      <td>0.127479</td>\n",
              "      <td>0.225363</td>\n",
              "      <td>0.164128</td>\n",
              "      <td>0.072507</td>\n",
              "      <td>0.249508</td>\n",
              "      <td>0.206157</td>\n",
              "      <td>0.027697</td>\n",
              "      <td>0.370819</td>\n",
              "      <td>0.833998</td>\n",
              "      <td>-0.037612</td>\n",
              "      <td>0.400362</td>\n",
              "      <td>0.218823</td>\n",
              "      <td>-0.020845</td>\n",
              "      <td>0.388722</td>\n",
              "      <td>0.150536</td>\n",
              "      <td>0.007938</td>\n",
              "      <td>0.374952</td>\n",
              "      <td>0.073443</td>\n",
              "      <td>0.040432</td>\n",
              "      <td>0.483528</td>\n",
              "      <td>0.071126</td>\n",
              "      <td>0.034102</td>\n",
              "      <td>0.244181</td>\n",
              "      <td>0.414586</td>\n",
              "      <td>0.053564</td>\n",
              "      <td>0.170244</td>\n",
              "      <td>...</td>\n",
              "      <td>0.400971</td>\n",
              "      <td>0.797283</td>\n",
              "      <td>-0.057481</td>\n",
              "      <td>0.491774</td>\n",
              "      <td>0.282192</td>\n",
              "      <td>-0.024813</td>\n",
              "      <td>0.390480</td>\n",
              "      <td>0.806401</td>\n",
              "      <td>-0.063569</td>\n",
              "      <td>0.381867</td>\n",
              "      <td>0.816910</td>\n",
              "      <td>-0.058509</td>\n",
              "      <td>0.356374</td>\n",
              "      <td>0.697895</td>\n",
              "      <td>-0.055486</td>\n",
              "      <td>0.130981</td>\n",
              "      <td>0.558962</td>\n",
              "      <td>0.341036</td>\n",
              "      <td>0.493862</td>\n",
              "      <td>0.611363</td>\n",
              "      <td>-0.122746</td>\n",
              "      <td>0.395654</td>\n",
              "      <td>0.785397</td>\n",
              "      <td>-0.038678</td>\n",
              "      <td>0.386269</td>\n",
              "      <td>0.787321</td>\n",
              "      <td>-0.042230</td>\n",
              "      <td>0.452422</td>\n",
              "      <td>0.626979</td>\n",
              "      <td>-0.094577</td>\n",
              "      <td>0.401670</td>\n",
              "      <td>0.622043</td>\n",
              "      <td>-0.060001</td>\n",
              "      <td>0.447939</td>\n",
              "      <td>0.619601</td>\n",
              "      <td>-0.092344</td>\n",
              "      <td>0.490952</td>\n",
              "      <td>0.228598</td>\n",
              "      <td>-0.023898</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14634</th>\n",
              "      <td>0.480344</td>\n",
              "      <td>0.614472</td>\n",
              "      <td>-0.073619</td>\n",
              "      <td>0.351348</td>\n",
              "      <td>0.359926</td>\n",
              "      <td>-0.029388</td>\n",
              "      <td>0.296622</td>\n",
              "      <td>0.382071</td>\n",
              "      <td>-0.013036</td>\n",
              "      <td>0.371280</td>\n",
              "      <td>0.466668</td>\n",
              "      <td>-0.062456</td>\n",
              "      <td>0.200838</td>\n",
              "      <td>-0.066494</td>\n",
              "      <td>-0.011770</td>\n",
              "      <td>0.220040</td>\n",
              "      <td>-0.001829</td>\n",
              "      <td>-0.048701</td>\n",
              "      <td>0.240770</td>\n",
              "      <td>0.061476</td>\n",
              "      <td>-0.075901</td>\n",
              "      <td>0.363895</td>\n",
              "      <td>0.734271</td>\n",
              "      <td>0.071702</td>\n",
              "      <td>0.399486</td>\n",
              "      <td>0.096403</td>\n",
              "      <td>-0.128011</td>\n",
              "      <td>0.393551</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.130443</td>\n",
              "      <td>0.384674</td>\n",
              "      <td>-0.095847</td>\n",
              "      <td>-0.123506</td>\n",
              "      <td>0.504907</td>\n",
              "      <td>-0.092631</td>\n",
              "      <td>-0.136478</td>\n",
              "      <td>0.245395</td>\n",
              "      <td>0.251886</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.169527</td>\n",
              "      <td>...</td>\n",
              "      <td>0.401906</td>\n",
              "      <td>0.721825</td>\n",
              "      <td>0.031216</td>\n",
              "      <td>0.494581</td>\n",
              "      <td>0.161664</td>\n",
              "      <td>-0.117363</td>\n",
              "      <td>0.387729</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.031917</td>\n",
              "      <td>0.376631</td>\n",
              "      <td>0.738281</td>\n",
              "      <td>0.040350</td>\n",
              "      <td>0.353397</td>\n",
              "      <td>0.587014</td>\n",
              "      <td>-0.009165</td>\n",
              "      <td>0.138784</td>\n",
              "      <td>0.330477</td>\n",
              "      <td>0.364592</td>\n",
              "      <td>0.478526</td>\n",
              "      <td>0.523328</td>\n",
              "      <td>-0.111924</td>\n",
              "      <td>0.401928</td>\n",
              "      <td>0.697973</td>\n",
              "      <td>0.040930</td>\n",
              "      <td>0.390769</td>\n",
              "      <td>0.701493</td>\n",
              "      <td>0.040458</td>\n",
              "      <td>0.437818</td>\n",
              "      <td>0.528118</td>\n",
              "      <td>-0.072447</td>\n",
              "      <td>0.389704</td>\n",
              "      <td>0.510692</td>\n",
              "      <td>-0.040840</td>\n",
              "      <td>0.432847</td>\n",
              "      <td>0.520244</td>\n",
              "      <td>-0.076076</td>\n",
              "      <td>0.496153</td>\n",
              "      <td>0.107584</td>\n",
              "      <td>-0.134429</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>0.451848</td>\n",
              "      <td>0.622114</td>\n",
              "      <td>-0.181673</td>\n",
              "      <td>0.305574</td>\n",
              "      <td>0.401656</td>\n",
              "      <td>-0.015953</td>\n",
              "      <td>0.233015</td>\n",
              "      <td>0.421863</td>\n",
              "      <td>-0.015226</td>\n",
              "      <td>0.335668</td>\n",
              "      <td>0.477531</td>\n",
              "      <td>-0.099770</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>0.018560</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.221613</td>\n",
              "      <td>0.058618</td>\n",
              "      <td>0.131506</td>\n",
              "      <td>0.242395</td>\n",
              "      <td>0.106519</td>\n",
              "      <td>0.064972</td>\n",
              "      <td>0.257232</td>\n",
              "      <td>0.817205</td>\n",
              "      <td>-0.085517</td>\n",
              "      <td>0.429682</td>\n",
              "      <td>0.147278</td>\n",
              "      <td>0.012527</td>\n",
              "      <td>0.433578</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.067867</td>\n",
              "      <td>0.434073</td>\n",
              "      <td>-0.013070</td>\n",
              "      <td>0.124950</td>\n",
              "      <td>0.568935</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.123650</td>\n",
              "      <td>0.182558</td>\n",
              "      <td>0.309983</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>0.081712</td>\n",
              "      <td>...</td>\n",
              "      <td>0.325224</td>\n",
              "      <td>0.787673</td>\n",
              "      <td>-0.104376</td>\n",
              "      <td>0.523025</td>\n",
              "      <td>0.221324</td>\n",
              "      <td>0.002962</td>\n",
              "      <td>0.306585</td>\n",
              "      <td>0.796358</td>\n",
              "      <td>-0.111493</td>\n",
              "      <td>0.289647</td>\n",
              "      <td>0.805510</td>\n",
              "      <td>-0.107110</td>\n",
              "      <td>0.283806</td>\n",
              "      <td>0.617631</td>\n",
              "      <td>-0.111347</td>\n",
              "      <td>-0.033763</td>\n",
              "      <td>0.493091</td>\n",
              "      <td>0.385915</td>\n",
              "      <td>0.465378</td>\n",
              "      <td>0.510059</td>\n",
              "      <td>-0.169199</td>\n",
              "      <td>0.322039</td>\n",
              "      <td>0.765549</td>\n",
              "      <td>-0.091213</td>\n",
              "      <td>0.309973</td>\n",
              "      <td>0.766603</td>\n",
              "      <td>-0.096392</td>\n",
              "      <td>0.413244</td>\n",
              "      <td>0.528470</td>\n",
              "      <td>-0.136841</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>0.525035</td>\n",
              "      <td>-0.093184</td>\n",
              "      <td>0.409769</td>\n",
              "      <td>0.520118</td>\n",
              "      <td>-0.133804</td>\n",
              "      <td>0.533610</td>\n",
              "      <td>0.171482</td>\n",
              "      <td>0.013007</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14636 rows × 1405 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_y       9_z  correct\n",
              "0      0.646184  0.771045 -0.064509  ...  0.403363 -0.117815        0\n",
              "1      0.505669  0.783729 -0.018900  ...  0.367928 -0.183734        0\n",
              "2      0.570398  0.522947 -0.131259  ... -0.036601 -0.077871        0\n",
              "3      0.550640  0.693204 -0.039426  ...  0.188114 -0.223748        0\n",
              "4      0.524232  0.722622 -0.049771  ...  0.346925 -0.121656        0\n",
              "...         ...       ...       ...  ...       ...       ...      ...\n",
              "14631  0.529442  0.667210 -0.066573  ...  0.271605 -0.073554        6\n",
              "14632  0.500539  0.734860 -0.148886  ...  0.238567  0.000434        6\n",
              "14633  0.494283  0.710024 -0.116081  ...  0.228598 -0.023898        6\n",
              "14634  0.480344  0.614472 -0.073619  ...  0.107584 -0.134429        6\n",
              "14635  0.451848  0.622114 -0.181673  ...  0.171482  0.013007        6\n",
              "\n",
              "[14636 rows x 1405 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4gkpPe6eh6W"
      },
      "source": [
        "3659,4  \n",
        "30,487"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBmlmqc1mNui"
      },
      "source": [
        "それぞれのデータを目的変数と説明変数に分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xf5l4ylWS_V"
      },
      "source": [
        "x_train=DataFrame(df_train.drop(\"correct\",axis=1))\n",
        "y_train=DataFrame(df_train[\"correct\"])\n",
        "\n",
        "x_test=DataFrame(df_test.drop(\"correct\",axis=1))\n",
        "y_test=DataFrame(df_test[\"correct\"])\n",
        "\n",
        "x_valid=DataFrame(df_valid.drop(\"correct\",axis=1))\n",
        "y_valid=DataFrame(df_valid[\"correct\"])\n",
        "\n",
        "#データの整形\n",
        "x_train = x_train.astype(np.float)\n",
        "x_test = x_test.astype(np.float)\n",
        "x_valid = x_valid.astype(np.float)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,7)\n",
        "y_test = np_utils.to_categorical(y_test,7)\n",
        "y_valid = np_utils.to_categorical(y_valid,7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZIebnHV0MAz"
      },
      "source": [
        "batch_size=967 #バッチサイズ\n",
        "n_rnn = 1  # 時系列の数\n",
        "n_sample = len(x_train)-n_rnn  # サンプル数\n",
        "n_in=len(df_train.columns)-1 #入力層のニューロン数\n",
        "n_mid=128 #中間層のニューロン数\n",
        "n_out=7 #出力層のニューロン数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUtFTKJkmdSl"
      },
      "source": [
        "モデル構築  \n",
        "SimpleRNN:全結合の中間層が再起的になる。  \n",
        "LSTM:RNNの発展版であるLSTMを活用できる。複雑な時系列データを扱えるが学習に時間がかかる  \n",
        "GRU: LSTMの簡易版。パラメータが少ないので学習に時間がかからない"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sL9ng-uKFK5"
      },
      "source": [
        "#ニューラルネットワークの実装①\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_in,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_in,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_in,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4rTm8KqW0jK",
        "outputId": "8fa2c79d-66fb-4107-bcdb-47bed4037a67"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 50)                70300     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 7)                 357       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75,757\n",
            "Trainable params: 75,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdrsd9iXABvZ"
      },
      "source": [
        "adam = Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZDO6Y1hfTbP"
      },
      "source": [
        "model.save('/content/drive/MyDrive/data分析/Mediapipe/Mediapipemodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGnhC5IwkpQu",
        "outputId": "a8c539e3-d588-4fdc-f0ee-bd9976ed9f80"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "modelCheckpoint = ModelCheckpoint(filepath ='/content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight.h5',\n",
        "                                  monitor='val_accuracy',\n",
        "                                  verbose=1,\n",
        "                                  save_best_only=True,\n",
        "                                  save_weights_only=False,\n",
        "                                  mode='max',\n",
        "                                  period=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tqXuJjsmhTy"
      },
      "source": [
        "##  学習  \n",
        "とんでもない精度ですね"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "704KOTdfW_7y",
        "outputId": "6e90a93e-aadc-42f6-885e-bf755638130b"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    steps_per_epoch=85,\n",
        "                    epochs=3000,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    validation_batch_size=487,\n",
        "                    validation_steps=30,\n",
        "                    callbacks=[modelCheckpoint]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "Epoch 1751/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9245 - accuracy: 0.6222\n",
            "Epoch 01751: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9243 - accuracy: 0.6222 - val_loss: 1.9549 - val_accuracy: 0.2595\n",
            "Epoch 1752/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9398 - accuracy: 0.6119\n",
            "Epoch 01752: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9398 - accuracy: 0.6119 - val_loss: 1.9332 - val_accuracy: 0.2825\n",
            "Epoch 1753/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9226 - accuracy: 0.6233\n",
            "Epoch 01753: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9226 - accuracy: 0.6233 - val_loss: 1.9556 - val_accuracy: 0.2552\n",
            "Epoch 1754/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9235 - accuracy: 0.6214\n",
            "Epoch 01754: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9235 - accuracy: 0.6214 - val_loss: 1.9799 - val_accuracy: 0.2524\n",
            "Epoch 1755/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9232 - accuracy: 0.6230\n",
            "Epoch 01755: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9232 - accuracy: 0.6230 - val_loss: 1.9487 - val_accuracy: 0.2533\n",
            "Epoch 1756/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9227 - accuracy: 0.6245\n",
            "Epoch 01756: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6245 - val_loss: 1.9630 - val_accuracy: 0.2598\n",
            "Epoch 1757/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.6219\n",
            "Epoch 01757: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9260 - accuracy: 0.6219 - val_loss: 1.9600 - val_accuracy: 0.2606\n",
            "Epoch 1758/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9269 - accuracy: 0.6202\n",
            "Epoch 01758: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9269 - accuracy: 0.6202 - val_loss: 1.9480 - val_accuracy: 0.2490\n",
            "Epoch 1759/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9251 - accuracy: 0.6207\n",
            "Epoch 01759: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9251 - accuracy: 0.6207 - val_loss: 1.9934 - val_accuracy: 0.2363\n",
            "Epoch 1760/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9166 - accuracy: 0.6264\n",
            "Epoch 01760: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9166 - accuracy: 0.6265 - val_loss: 1.9140 - val_accuracy: 0.2680\n",
            "Epoch 1761/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9437 - accuracy: 0.6118\n",
            "Epoch 01761: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9431 - accuracy: 0.6121 - val_loss: 1.9054 - val_accuracy: 0.2756\n",
            "Epoch 1762/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9481 - accuracy: 0.6092\n",
            "Epoch 01762: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9481 - accuracy: 0.6092 - val_loss: 2.0851 - val_accuracy: 0.1988\n",
            "Epoch 1763/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9497 - accuracy: 0.6096\n",
            "Epoch 01763: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9497 - accuracy: 0.6096 - val_loss: 2.0359 - val_accuracy: 0.2132\n",
            "Epoch 1764/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9320 - accuracy: 0.6191\n",
            "Epoch 01764: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9320 - accuracy: 0.6191 - val_loss: 1.9145 - val_accuracy: 0.2752\n",
            "Epoch 1765/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9143 - accuracy: 0.6281\n",
            "Epoch 01765: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9143 - accuracy: 0.6281 - val_loss: 2.0160 - val_accuracy: 0.2383\n",
            "Epoch 1766/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9169 - accuracy: 0.6259\n",
            "Epoch 01766: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9173 - accuracy: 0.6257 - val_loss: 1.9341 - val_accuracy: 0.2744\n",
            "Epoch 1767/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9273 - accuracy: 0.6221\n",
            "Epoch 01767: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9273 - accuracy: 0.6221 - val_loss: 1.9473 - val_accuracy: 0.2628\n",
            "Epoch 1768/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.6237\n",
            "Epoch 01768: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9214 - accuracy: 0.6237 - val_loss: 1.9171 - val_accuracy: 0.2802\n",
            "Epoch 1769/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9227 - accuracy: 0.6239\n",
            "Epoch 01769: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6239 - val_loss: 1.9104 - val_accuracy: 0.2720\n",
            "Epoch 1770/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9347 - accuracy: 0.6185\n",
            "Epoch 01770: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9347 - accuracy: 0.6185 - val_loss: 1.9050 - val_accuracy: 0.2654\n",
            "Epoch 1771/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9349 - accuracy: 0.6151\n",
            "Epoch 01771: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9349 - accuracy: 0.6151 - val_loss: 1.8667 - val_accuracy: 0.2948\n",
            "Epoch 1772/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9234 - accuracy: 0.6223\n",
            "Epoch 01772: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9235 - accuracy: 0.6223 - val_loss: 1.9314 - val_accuracy: 0.2737\n",
            "Epoch 1773/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9165 - accuracy: 0.6267\n",
            "Epoch 01773: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9163 - accuracy: 0.6266 - val_loss: 1.9676 - val_accuracy: 0.2569\n",
            "Epoch 1774/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9220 - accuracy: 0.6232\n",
            "Epoch 01774: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9217 - accuracy: 0.6232 - val_loss: 1.9759 - val_accuracy: 0.2658\n",
            "Epoch 1775/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.6239\n",
            "Epoch 01775: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9233 - accuracy: 0.6239 - val_loss: 1.9508 - val_accuracy: 0.2734\n",
            "Epoch 1776/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9227 - accuracy: 0.6238\n",
            "Epoch 01776: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9225 - accuracy: 0.6238 - val_loss: 2.0117 - val_accuracy: 0.2463\n",
            "Epoch 1777/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9241 - accuracy: 0.6217\n",
            "Epoch 01777: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9241 - accuracy: 0.6217 - val_loss: 1.9369 - val_accuracy: 0.2669\n",
            "Epoch 1778/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9194 - accuracy: 0.6249\n",
            "Epoch 01778: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9197 - accuracy: 0.6247 - val_loss: 1.9856 - val_accuracy: 0.2502\n",
            "Epoch 1779/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9270 - accuracy: 0.6226\n",
            "Epoch 01779: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9276 - accuracy: 0.6222 - val_loss: 2.0389 - val_accuracy: 0.2346\n",
            "Epoch 1780/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9501 - accuracy: 0.6078\n",
            "Epoch 01780: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9501 - accuracy: 0.6078 - val_loss: 1.9324 - val_accuracy: 0.2903\n",
            "Epoch 1781/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.6230\n",
            "Epoch 01781: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9233 - accuracy: 0.6230 - val_loss: 2.0674 - val_accuracy: 0.1973\n",
            "Epoch 1782/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9222 - accuracy: 0.6226\n",
            "Epoch 01782: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9222 - accuracy: 0.6226 - val_loss: 2.0141 - val_accuracy: 0.2402\n",
            "Epoch 1783/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9184 - accuracy: 0.6255\n",
            "Epoch 01783: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9184 - accuracy: 0.6255 - val_loss: 1.9992 - val_accuracy: 0.2535\n",
            "Epoch 1784/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9258 - accuracy: 0.6227\n",
            "Epoch 01784: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9253 - accuracy: 0.6229 - val_loss: 2.0801 - val_accuracy: 0.2170\n",
            "Epoch 1785/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9262 - accuracy: 0.6210\n",
            "Epoch 01785: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9262 - accuracy: 0.6210 - val_loss: 2.0084 - val_accuracy: 0.2141\n",
            "Epoch 1786/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9307 - accuracy: 0.6182\n",
            "Epoch 01786: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9307 - accuracy: 0.6182 - val_loss: 2.0340 - val_accuracy: 0.2367\n",
            "Epoch 1787/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9116 - accuracy: 0.6288\n",
            "Epoch 01787: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9116 - accuracy: 0.6288 - val_loss: 1.9788 - val_accuracy: 0.2503\n",
            "Epoch 1788/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9201 - accuracy: 0.6238\n",
            "Epoch 01788: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9201 - accuracy: 0.6238 - val_loss: 1.9667 - val_accuracy: 0.2522\n",
            "Epoch 1789/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9238 - accuracy: 0.6212\n",
            "Epoch 01789: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9233 - accuracy: 0.6214 - val_loss: 1.8867 - val_accuracy: 0.2802\n",
            "Epoch 1790/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9234 - accuracy: 0.6246\n",
            "Epoch 01790: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9234 - accuracy: 0.6246 - val_loss: 1.9421 - val_accuracy: 0.2602\n",
            "Epoch 1791/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9218 - accuracy: 0.6237\n",
            "Epoch 01791: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9218 - accuracy: 0.6237 - val_loss: 1.9300 - val_accuracy: 0.2698\n",
            "Epoch 1792/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9307 - accuracy: 0.6191\n",
            "Epoch 01792: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9307 - accuracy: 0.6191 - val_loss: 2.0643 - val_accuracy: 0.2012\n",
            "Epoch 1793/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9479 - accuracy: 0.6086\n",
            "Epoch 01793: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9484 - accuracy: 0.6085 - val_loss: 2.0367 - val_accuracy: 0.2363\n",
            "Epoch 1794/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9492 - accuracy: 0.6075\n",
            "Epoch 01794: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9492 - accuracy: 0.6075 - val_loss: 1.9365 - val_accuracy: 0.2702\n",
            "Epoch 1795/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9189 - accuracy: 0.6251\n",
            "Epoch 01795: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9189 - accuracy: 0.6251 - val_loss: 2.0834 - val_accuracy: 0.2025\n",
            "Epoch 1796/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9225 - accuracy: 0.6221\n",
            "Epoch 01796: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9225 - accuracy: 0.6221 - val_loss: 1.9814 - val_accuracy: 0.2533\n",
            "Epoch 1797/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9194 - accuracy: 0.6246\n",
            "Epoch 01797: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9194 - accuracy: 0.6246 - val_loss: 1.9514 - val_accuracy: 0.2415\n",
            "Epoch 1798/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9296 - accuracy: 0.6185\n",
            "Epoch 01798: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9296 - accuracy: 0.6185 - val_loss: 1.9154 - val_accuracy: 0.2690\n",
            "Epoch 1799/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9199 - accuracy: 0.6247\n",
            "Epoch 01799: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9194 - accuracy: 0.6251 - val_loss: 1.9648 - val_accuracy: 0.2561\n",
            "Epoch 1800/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9158 - accuracy: 0.6263\n",
            "Epoch 01800: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9162 - accuracy: 0.6261 - val_loss: 2.0232 - val_accuracy: 0.2509\n",
            "Epoch 1801/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9213 - accuracy: 0.6232\n",
            "Epoch 01801: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9213 - accuracy: 0.6232 - val_loss: 1.9844 - val_accuracy: 0.2457\n",
            "Epoch 1802/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9168 - accuracy: 0.6272\n",
            "Epoch 01802: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9162 - accuracy: 0.6274 - val_loss: 1.9749 - val_accuracy: 0.2559\n",
            "Epoch 1803/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9290 - accuracy: 0.6202\n",
            "Epoch 01803: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9295 - accuracy: 0.6200 - val_loss: 2.0636 - val_accuracy: 0.2181\n",
            "Epoch 1804/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9196 - accuracy: 0.6232\n",
            "Epoch 01804: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9196 - accuracy: 0.6232 - val_loss: 2.0028 - val_accuracy: 0.2357\n",
            "Epoch 1805/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9211 - accuracy: 0.6225\n",
            "Epoch 01805: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9211 - accuracy: 0.6225 - val_loss: 2.0021 - val_accuracy: 0.2379\n",
            "Epoch 1806/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9220 - accuracy: 0.6238\n",
            "Epoch 01806: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9220 - accuracy: 0.6238 - val_loss: 1.9125 - val_accuracy: 0.2760\n",
            "Epoch 1807/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9204 - accuracy: 0.6241\n",
            "Epoch 01807: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9204 - accuracy: 0.6241 - val_loss: 1.9262 - val_accuracy: 0.2658\n",
            "Epoch 1808/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9333 - accuracy: 0.6168\n",
            "Epoch 01808: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9335 - accuracy: 0.6169 - val_loss: 1.9241 - val_accuracy: 0.2941\n",
            "Epoch 1809/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9231 - accuracy: 0.6210\n",
            "Epoch 01809: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9231 - accuracy: 0.6210 - val_loss: 1.9813 - val_accuracy: 0.2494\n",
            "Epoch 1810/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9220 - accuracy: 0.6210\n",
            "Epoch 01810: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9221 - accuracy: 0.6211 - val_loss: 1.9417 - val_accuracy: 0.2606\n",
            "Epoch 1811/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9246 - accuracy: 0.6215\n",
            "Epoch 01811: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9246 - accuracy: 0.6214 - val_loss: 1.9986 - val_accuracy: 0.2463\n",
            "Epoch 1812/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.6189\n",
            "Epoch 01812: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9309 - accuracy: 0.6186 - val_loss: 1.8974 - val_accuracy: 0.2862\n",
            "Epoch 1813/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9349 - accuracy: 0.6182\n",
            "Epoch 01813: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9345 - accuracy: 0.6183 - val_loss: 2.0187 - val_accuracy: 0.2396\n",
            "Epoch 1814/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9344 - accuracy: 0.6157\n",
            "Epoch 01814: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9344 - accuracy: 0.6157 - val_loss: 1.8758 - val_accuracy: 0.2758\n",
            "Epoch 1815/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9302 - accuracy: 0.6188\n",
            "Epoch 01815: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9302 - accuracy: 0.6188 - val_loss: 2.0671 - val_accuracy: 0.2086\n",
            "Epoch 1816/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9269 - accuracy: 0.6227\n",
            "Epoch 01816: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9269 - accuracy: 0.6227 - val_loss: 1.9821 - val_accuracy: 0.2682\n",
            "Epoch 1817/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9241 - accuracy: 0.6206\n",
            "Epoch 01817: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9241 - accuracy: 0.6206 - val_loss: 1.9567 - val_accuracy: 0.2659\n",
            "Epoch 1818/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9340 - accuracy: 0.6160\n",
            "Epoch 01818: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9340 - accuracy: 0.6160 - val_loss: 2.0535 - val_accuracy: 0.2135\n",
            "Epoch 1819/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9347 - accuracy: 0.6166\n",
            "Epoch 01819: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9347 - accuracy: 0.6166 - val_loss: 1.9837 - val_accuracy: 0.2502\n",
            "Epoch 1820/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9414 - accuracy: 0.6125\n",
            "Epoch 01820: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9411 - accuracy: 0.6126 - val_loss: 2.0390 - val_accuracy: 0.2418\n",
            "Epoch 1821/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9298 - accuracy: 0.6205\n",
            "Epoch 01821: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9294 - accuracy: 0.6205 - val_loss: 1.9788 - val_accuracy: 0.2414\n",
            "Epoch 1822/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9469 - accuracy: 0.6093\n",
            "Epoch 01822: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9469 - accuracy: 0.6093 - val_loss: 1.9616 - val_accuracy: 0.2600\n",
            "Epoch 1823/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9262 - accuracy: 0.6204\n",
            "Epoch 01823: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9261 - accuracy: 0.6204 - val_loss: 1.9508 - val_accuracy: 0.2464\n",
            "Epoch 1824/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9329 - accuracy: 0.6165\n",
            "Epoch 01824: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9334 - accuracy: 0.6164 - val_loss: 2.0034 - val_accuracy: 0.2526\n",
            "Epoch 1825/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9514 - accuracy: 0.6066\n",
            "Epoch 01825: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9514 - accuracy: 0.6070 - val_loss: 2.0232 - val_accuracy: 0.2256\n",
            "Epoch 1826/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9391 - accuracy: 0.6152\n",
            "Epoch 01826: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9382 - accuracy: 0.6156 - val_loss: 1.9650 - val_accuracy: 0.2416\n",
            "Epoch 1827/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9202 - accuracy: 0.6255\n",
            "Epoch 01827: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9206 - accuracy: 0.6253 - val_loss: 1.9119 - val_accuracy: 0.2945\n",
            "Epoch 1828/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9228 - accuracy: 0.6239\n",
            "Epoch 01828: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6239 - val_loss: 1.9971 - val_accuracy: 0.2402\n",
            "Epoch 1829/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9180 - accuracy: 0.6243\n",
            "Epoch 01829: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9180 - accuracy: 0.6244 - val_loss: 2.0351 - val_accuracy: 0.2311\n",
            "Epoch 1830/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9454 - accuracy: 0.6107\n",
            "Epoch 01830: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9456 - accuracy: 0.6105 - val_loss: 2.0211 - val_accuracy: 0.2193\n",
            "Epoch 1831/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9310 - accuracy: 0.6211\n",
            "Epoch 01831: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9305 - accuracy: 0.6217 - val_loss: 2.0288 - val_accuracy: 0.2415\n",
            "Epoch 1832/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9301 - accuracy: 0.6189\n",
            "Epoch 01832: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9301 - accuracy: 0.6189 - val_loss: 1.8960 - val_accuracy: 0.3030\n",
            "Epoch 1833/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9274 - accuracy: 0.6190\n",
            "Epoch 01833: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9277 - accuracy: 0.6189 - val_loss: 2.0113 - val_accuracy: 0.2304\n",
            "Epoch 1834/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.6207\n",
            "Epoch 01834: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9295 - accuracy: 0.6207 - val_loss: 1.9765 - val_accuracy: 0.2450\n",
            "Epoch 1835/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9299 - accuracy: 0.6178\n",
            "Epoch 01835: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9300 - accuracy: 0.6176 - val_loss: 1.9509 - val_accuracy: 0.2741\n",
            "Epoch 1836/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9294 - accuracy: 0.6198\n",
            "Epoch 01836: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9293 - accuracy: 0.6199 - val_loss: 1.9353 - val_accuracy: 0.2573\n",
            "Epoch 1837/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.6227\n",
            "Epoch 01837: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9233 - accuracy: 0.6227 - val_loss: 1.9891 - val_accuracy: 0.2467\n",
            "Epoch 1838/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9252 - accuracy: 0.6214\n",
            "Epoch 01838: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9246 - accuracy: 0.6217 - val_loss: 2.0283 - val_accuracy: 0.2288\n",
            "Epoch 1839/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6272\n",
            "Epoch 01839: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9150 - accuracy: 0.6272 - val_loss: 1.9730 - val_accuracy: 0.2516\n",
            "Epoch 1840/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9169 - accuracy: 0.6263\n",
            "Epoch 01840: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9166 - accuracy: 0.6265 - val_loss: 2.0865 - val_accuracy: 0.2107\n",
            "Epoch 1841/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9260 - accuracy: 0.6220\n",
            "Epoch 01841: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9252 - accuracy: 0.6222 - val_loss: 1.9697 - val_accuracy: 0.2464\n",
            "Epoch 1842/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9166 - accuracy: 0.6250\n",
            "Epoch 01842: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9166 - accuracy: 0.6250 - val_loss: 1.9520 - val_accuracy: 0.2578\n",
            "Epoch 1843/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9255 - accuracy: 0.6217\n",
            "Epoch 01843: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9250 - accuracy: 0.6219 - val_loss: 1.9786 - val_accuracy: 0.2379\n",
            "Epoch 1844/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9404 - accuracy: 0.6141\n",
            "Epoch 01844: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9404 - accuracy: 0.6141 - val_loss: 1.9880 - val_accuracy: 0.2454\n",
            "Epoch 1845/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9117 - accuracy: 0.6277\n",
            "Epoch 01845: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9117 - accuracy: 0.6277 - val_loss: 2.0081 - val_accuracy: 0.2306\n",
            "Epoch 1846/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9265 - accuracy: 0.6199\n",
            "Epoch 01846: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9265 - accuracy: 0.6199 - val_loss: 2.0528 - val_accuracy: 0.2226\n",
            "Epoch 1847/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9270 - accuracy: 0.6206\n",
            "Epoch 01847: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9269 - accuracy: 0.6206 - val_loss: 2.0406 - val_accuracy: 0.2211\n",
            "Epoch 1848/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.6210\n",
            "Epoch 01848: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9254 - accuracy: 0.6210 - val_loss: 1.9832 - val_accuracy: 0.2511\n",
            "Epoch 1849/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9251 - accuracy: 0.6204\n",
            "Epoch 01849: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9251 - accuracy: 0.6204 - val_loss: 2.0120 - val_accuracy: 0.2378\n",
            "Epoch 1850/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.6184\n",
            "Epoch 01850: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9308 - accuracy: 0.6181 - val_loss: 2.0299 - val_accuracy: 0.2355\n",
            "Epoch 1851/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9270 - accuracy: 0.6199\n",
            "Epoch 01851: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9270 - accuracy: 0.6199 - val_loss: 2.0243 - val_accuracy: 0.2344\n",
            "Epoch 1852/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9193 - accuracy: 0.6243\n",
            "Epoch 01852: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9193 - accuracy: 0.6243 - val_loss: 1.9794 - val_accuracy: 0.2445\n",
            "Epoch 1853/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9210 - accuracy: 0.6236\n",
            "Epoch 01853: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9210 - accuracy: 0.6236 - val_loss: 1.9628 - val_accuracy: 0.2474\n",
            "Epoch 1854/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6276\n",
            "Epoch 01854: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9159 - accuracy: 0.6276 - val_loss: 2.0231 - val_accuracy: 0.2409\n",
            "Epoch 1855/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9272 - accuracy: 0.6201\n",
            "Epoch 01855: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9267 - accuracy: 0.6203 - val_loss: 1.9823 - val_accuracy: 0.2494\n",
            "Epoch 1856/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9193 - accuracy: 0.6239\n",
            "Epoch 01856: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9193 - accuracy: 0.6239 - val_loss: 1.9061 - val_accuracy: 0.2828\n",
            "Epoch 1857/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9179 - accuracy: 0.6240\n",
            "Epoch 01857: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9175 - accuracy: 0.6242 - val_loss: 1.9147 - val_accuracy: 0.2825\n",
            "Epoch 1858/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9197 - accuracy: 0.6275\n",
            "Epoch 01858: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9197 - accuracy: 0.6275 - val_loss: 1.9750 - val_accuracy: 0.2441\n",
            "Epoch 1859/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9275 - accuracy: 0.6192\n",
            "Epoch 01859: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9275 - accuracy: 0.6192 - val_loss: 1.8653 - val_accuracy: 0.2821\n",
            "Epoch 1860/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9212 - accuracy: 0.6248\n",
            "Epoch 01860: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9210 - accuracy: 0.6248 - val_loss: 1.9496 - val_accuracy: 0.2515\n",
            "Epoch 1861/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9176 - accuracy: 0.6256\n",
            "Epoch 01861: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9177 - accuracy: 0.6256 - val_loss: 2.0129 - val_accuracy: 0.2303\n",
            "Epoch 1862/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9208 - accuracy: 0.6234\n",
            "Epoch 01862: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9205 - accuracy: 0.6233 - val_loss: 1.9606 - val_accuracy: 0.2446\n",
            "Epoch 1863/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9272 - accuracy: 0.6196\n",
            "Epoch 01863: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9269 - accuracy: 0.6197 - val_loss: 1.9909 - val_accuracy: 0.2554\n",
            "Epoch 1864/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9187 - accuracy: 0.6251\n",
            "Epoch 01864: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9181 - accuracy: 0.6253 - val_loss: 1.9488 - val_accuracy: 0.2533\n",
            "Epoch 1865/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9166 - accuracy: 0.6266\n",
            "Epoch 01865: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9167 - accuracy: 0.6264 - val_loss: 1.9151 - val_accuracy: 0.2669\n",
            "Epoch 1866/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9333 - accuracy: 0.6178\n",
            "Epoch 01866: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9333 - accuracy: 0.6178 - val_loss: 1.9676 - val_accuracy: 0.2472\n",
            "Epoch 1867/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9216 - accuracy: 0.6229\n",
            "Epoch 01867: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9216 - accuracy: 0.6229 - val_loss: 1.8991 - val_accuracy: 0.2845\n",
            "Epoch 1868/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.6247\n",
            "Epoch 01868: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9200 - accuracy: 0.6247 - val_loss: 1.9220 - val_accuracy: 0.2662\n",
            "Epoch 1869/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9477 - accuracy: 0.6081\n",
            "Epoch 01869: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9481 - accuracy: 0.6080 - val_loss: 1.8665 - val_accuracy: 0.2882\n",
            "Epoch 1870/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9231 - accuracy: 0.6226\n",
            "Epoch 01870: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9233 - accuracy: 0.6227 - val_loss: 2.0920 - val_accuracy: 0.2092\n",
            "Epoch 1871/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9190 - accuracy: 0.6235\n",
            "Epoch 01871: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9190 - accuracy: 0.6235 - val_loss: 2.0501 - val_accuracy: 0.2188\n",
            "Epoch 1872/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9198 - accuracy: 0.6242\n",
            "Epoch 01872: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9198 - accuracy: 0.6242 - val_loss: 1.9763 - val_accuracy: 0.2454\n",
            "Epoch 1873/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9118 - accuracy: 0.6283\n",
            "Epoch 01873: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9123 - accuracy: 0.6280 - val_loss: 2.0505 - val_accuracy: 0.2222\n",
            "Epoch 1874/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9195 - accuracy: 0.6228\n",
            "Epoch 01874: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9194 - accuracy: 0.6230 - val_loss: 2.0579 - val_accuracy: 0.2296\n",
            "Epoch 1875/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9235 - accuracy: 0.6217\n",
            "Epoch 01875: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9235 - accuracy: 0.6217 - val_loss: 1.9440 - val_accuracy: 0.2760\n",
            "Epoch 1876/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9294 - accuracy: 0.6179\n",
            "Epoch 01876: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9294 - accuracy: 0.6179 - val_loss: 1.9577 - val_accuracy: 0.2557\n",
            "Epoch 1877/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9186 - accuracy: 0.6257\n",
            "Epoch 01877: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9181 - accuracy: 0.6260 - val_loss: 1.9759 - val_accuracy: 0.2456\n",
            "Epoch 1878/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9187 - accuracy: 0.6228\n",
            "Epoch 01878: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9187 - accuracy: 0.6228 - val_loss: 2.1422 - val_accuracy: 0.1876\n",
            "Epoch 1879/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.6245\n",
            "Epoch 01879: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9198 - accuracy: 0.6250 - val_loss: 1.9562 - val_accuracy: 0.2646\n",
            "Epoch 1880/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9204 - accuracy: 0.6230\n",
            "Epoch 01880: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9202 - accuracy: 0.6233 - val_loss: 2.0129 - val_accuracy: 0.2418\n",
            "Epoch 1881/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.6238\n",
            "Epoch 01881: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9214 - accuracy: 0.6238 - val_loss: 1.9596 - val_accuracy: 0.2474\n",
            "Epoch 1882/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9281 - accuracy: 0.6203\n",
            "Epoch 01882: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9288 - accuracy: 0.6200 - val_loss: 2.0075 - val_accuracy: 0.2485\n",
            "Epoch 1883/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9232 - accuracy: 0.6219\n",
            "Epoch 01883: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9236 - accuracy: 0.6219 - val_loss: 1.9353 - val_accuracy: 0.2550\n",
            "Epoch 1884/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9268 - accuracy: 0.6198\n",
            "Epoch 01884: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9268 - accuracy: 0.6199 - val_loss: 1.9728 - val_accuracy: 0.2438\n",
            "Epoch 1885/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9237 - accuracy: 0.6211\n",
            "Epoch 01885: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9237 - accuracy: 0.6211 - val_loss: 1.9307 - val_accuracy: 0.2570\n",
            "Epoch 1886/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9164 - accuracy: 0.6281\n",
            "Epoch 01886: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9173 - accuracy: 0.6277 - val_loss: 1.9413 - val_accuracy: 0.2663\n",
            "Epoch 1887/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9262 - accuracy: 0.6220\n",
            "Epoch 01887: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9262 - accuracy: 0.6220 - val_loss: 1.9364 - val_accuracy: 0.2789\n",
            "Epoch 1888/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9173 - accuracy: 0.6256\n",
            "Epoch 01888: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9174 - accuracy: 0.6256 - val_loss: 1.9906 - val_accuracy: 0.2452\n",
            "Epoch 1889/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9271 - accuracy: 0.6211\n",
            "Epoch 01889: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9271 - accuracy: 0.6211 - val_loss: 1.9196 - val_accuracy: 0.2708\n",
            "Epoch 1890/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6269\n",
            "Epoch 01890: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9140 - accuracy: 0.6270 - val_loss: 1.9869 - val_accuracy: 0.2444\n",
            "Epoch 1891/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9266 - accuracy: 0.6190\n",
            "Epoch 01891: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9265 - accuracy: 0.6190 - val_loss: 1.9829 - val_accuracy: 0.2383\n",
            "Epoch 1892/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9225 - accuracy: 0.6244\n",
            "Epoch 01892: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9224 - accuracy: 0.6246 - val_loss: 1.9887 - val_accuracy: 0.2613\n",
            "Epoch 1893/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9205 - accuracy: 0.6252\n",
            "Epoch 01893: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 43ms/step - loss: 0.9202 - accuracy: 0.6253 - val_loss: 1.9350 - val_accuracy: 0.2620\n",
            "Epoch 1894/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9449 - accuracy: 0.6123\n",
            "Epoch 01894: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9449 - accuracy: 0.6123 - val_loss: 2.0158 - val_accuracy: 0.2211\n",
            "Epoch 1895/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9178 - accuracy: 0.6255\n",
            "Epoch 01895: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9178 - accuracy: 0.6255 - val_loss: 1.9445 - val_accuracy: 0.2630\n",
            "Epoch 1896/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9209 - accuracy: 0.6234\n",
            "Epoch 01896: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9209 - accuracy: 0.6234 - val_loss: 2.0257 - val_accuracy: 0.2418\n",
            "Epoch 1897/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9275 - accuracy: 0.6206\n",
            "Epoch 01897: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9275 - accuracy: 0.6206 - val_loss: 2.0201 - val_accuracy: 0.2424\n",
            "Epoch 1898/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9205 - accuracy: 0.6251\n",
            "Epoch 01898: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9205 - accuracy: 0.6251 - val_loss: 1.9356 - val_accuracy: 0.2595\n",
            "Epoch 1899/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9230 - accuracy: 0.6222\n",
            "Epoch 01899: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9237 - accuracy: 0.6216 - val_loss: 1.9790 - val_accuracy: 0.2618\n",
            "Epoch 1900/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9272 - accuracy: 0.6192\n",
            "Epoch 01900: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9272 - accuracy: 0.6192 - val_loss: 2.0128 - val_accuracy: 0.2469\n",
            "Epoch 1901/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.6241\n",
            "Epoch 01901: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9209 - accuracy: 0.6242 - val_loss: 2.0023 - val_accuracy: 0.2396\n",
            "Epoch 1902/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9226 - accuracy: 0.6243\n",
            "Epoch 01902: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9226 - accuracy: 0.6243 - val_loss: 2.0678 - val_accuracy: 0.2167\n",
            "Epoch 1903/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9192 - accuracy: 0.6229\n",
            "Epoch 01903: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9194 - accuracy: 0.6230 - val_loss: 1.9209 - val_accuracy: 0.2721\n",
            "Epoch 1904/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9236 - accuracy: 0.6214\n",
            "Epoch 01904: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9237 - accuracy: 0.6213 - val_loss: 2.0563 - val_accuracy: 0.2250\n",
            "Epoch 1905/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9232 - accuracy: 0.6228\n",
            "Epoch 01905: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9230 - accuracy: 0.6229 - val_loss: 2.0355 - val_accuracy: 0.2463\n",
            "Epoch 1906/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9183 - accuracy: 0.6218\n",
            "Epoch 01906: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9183 - accuracy: 0.6218 - val_loss: 1.9385 - val_accuracy: 0.2697\n",
            "Epoch 1907/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.6266\n",
            "Epoch 01907: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9149 - accuracy: 0.6266 - val_loss: 1.9254 - val_accuracy: 0.2746\n",
            "Epoch 1908/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9215 - accuracy: 0.6232\n",
            "Epoch 01908: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9210 - accuracy: 0.6235 - val_loss: 1.9956 - val_accuracy: 0.2418\n",
            "Epoch 1909/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9321 - accuracy: 0.6194\n",
            "Epoch 01909: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9321 - accuracy: 0.6194 - val_loss: 1.9325 - val_accuracy: 0.2810\n",
            "Epoch 1910/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9242 - accuracy: 0.6218\n",
            "Epoch 01910: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9248 - accuracy: 0.6217 - val_loss: 1.9953 - val_accuracy: 0.2360\n",
            "Epoch 1911/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9234 - accuracy: 0.6235\n",
            "Epoch 01911: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9234 - accuracy: 0.6235 - val_loss: 1.8806 - val_accuracy: 0.2871\n",
            "Epoch 1912/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9215 - accuracy: 0.6252\n",
            "Epoch 01912: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9203 - accuracy: 0.6258 - val_loss: 1.9586 - val_accuracy: 0.2595\n",
            "Epoch 1913/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9201 - accuracy: 0.6231\n",
            "Epoch 01913: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9201 - accuracy: 0.6231 - val_loss: 1.9725 - val_accuracy: 0.2575\n",
            "Epoch 1914/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9322 - accuracy: 0.6188\n",
            "Epoch 01914: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9322 - accuracy: 0.6188 - val_loss: 1.9742 - val_accuracy: 0.2542\n",
            "Epoch 1915/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9179 - accuracy: 0.6248\n",
            "Epoch 01915: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9176 - accuracy: 0.6251 - val_loss: 1.9327 - val_accuracy: 0.2700\n",
            "Epoch 1916/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9258 - accuracy: 0.6220\n",
            "Epoch 01916: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9252 - accuracy: 0.6221 - val_loss: 2.0392 - val_accuracy: 0.2441\n",
            "Epoch 1917/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9190 - accuracy: 0.6246\n",
            "Epoch 01917: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9190 - accuracy: 0.6246 - val_loss: 1.9874 - val_accuracy: 0.2363\n",
            "Epoch 1918/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9257 - accuracy: 0.6211\n",
            "Epoch 01918: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9269 - accuracy: 0.6209 - val_loss: 2.0361 - val_accuracy: 0.2253\n",
            "Epoch 1919/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9396 - accuracy: 0.6159\n",
            "Epoch 01919: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9396 - accuracy: 0.6159 - val_loss: 1.9593 - val_accuracy: 0.2543\n",
            "Epoch 1920/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9160 - accuracy: 0.6264\n",
            "Epoch 01920: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9160 - accuracy: 0.6264 - val_loss: 1.9739 - val_accuracy: 0.2595\n",
            "Epoch 1921/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9178 - accuracy: 0.6261\n",
            "Epoch 01921: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9168 - accuracy: 0.6266 - val_loss: 2.0384 - val_accuracy: 0.2261\n",
            "Epoch 1922/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9252 - accuracy: 0.6221\n",
            "Epoch 01922: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9252 - accuracy: 0.6221 - val_loss: 1.9360 - val_accuracy: 0.2677\n",
            "Epoch 1923/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9213 - accuracy: 0.6238\n",
            "Epoch 01923: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9213 - accuracy: 0.6238 - val_loss: 1.9286 - val_accuracy: 0.2698\n",
            "Epoch 1924/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9196 - accuracy: 0.6243\n",
            "Epoch 01924: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9188 - accuracy: 0.6246 - val_loss: 2.0369 - val_accuracy: 0.2280\n",
            "Epoch 1925/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9150 - accuracy: 0.6271\n",
            "Epoch 01925: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9154 - accuracy: 0.6269 - val_loss: 1.9408 - val_accuracy: 0.2710\n",
            "Epoch 1926/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9182 - accuracy: 0.6249\n",
            "Epoch 01926: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9185 - accuracy: 0.6248 - val_loss: 2.0669 - val_accuracy: 0.2288\n",
            "Epoch 1927/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9286 - accuracy: 0.6197\n",
            "Epoch 01927: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9286 - accuracy: 0.6197 - val_loss: 1.8760 - val_accuracy: 0.3022\n",
            "Epoch 1928/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9234 - accuracy: 0.6218\n",
            "Epoch 01928: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9234 - accuracy: 0.6218 - val_loss: 1.9721 - val_accuracy: 0.2447\n",
            "Epoch 1929/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9255 - accuracy: 0.6230\n",
            "Epoch 01929: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9247 - accuracy: 0.6234 - val_loss: 1.9393 - val_accuracy: 0.2602\n",
            "Epoch 1930/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9227 - accuracy: 0.6237\n",
            "Epoch 01930: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9227 - accuracy: 0.6237 - val_loss: 2.0314 - val_accuracy: 0.2316\n",
            "Epoch 1931/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9216 - accuracy: 0.6234\n",
            "Epoch 01931: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9214 - accuracy: 0.6234 - val_loss: 1.9570 - val_accuracy: 0.2466\n",
            "Epoch 1932/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9171 - accuracy: 0.6254\n",
            "Epoch 01932: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9169 - accuracy: 0.6258 - val_loss: 1.9558 - val_accuracy: 0.2611\n",
            "Epoch 1933/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9204 - accuracy: 0.6237\n",
            "Epoch 01933: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9204 - accuracy: 0.6237 - val_loss: 2.0141 - val_accuracy: 0.2339\n",
            "Epoch 1934/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9252 - accuracy: 0.6223\n",
            "Epoch 01934: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9257 - accuracy: 0.6220 - val_loss: 2.0252 - val_accuracy: 0.2238\n",
            "Epoch 1935/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9342 - accuracy: 0.6174\n",
            "Epoch 01935: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9340 - accuracy: 0.6175 - val_loss: 1.9258 - val_accuracy: 0.2683\n",
            "Epoch 1936/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6283\n",
            "Epoch 01936: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9127 - accuracy: 0.6284 - val_loss: 1.9611 - val_accuracy: 0.2501\n",
            "Epoch 1937/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9141 - accuracy: 0.6279\n",
            "Epoch 01937: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9141 - accuracy: 0.6279 - val_loss: 1.9986 - val_accuracy: 0.2383\n",
            "Epoch 1938/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9371 - accuracy: 0.6138\n",
            "Epoch 01938: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9371 - accuracy: 0.6138 - val_loss: 1.9562 - val_accuracy: 0.2474\n",
            "Epoch 1939/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.6218\n",
            "Epoch 01939: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9223 - accuracy: 0.6218 - val_loss: 1.9022 - val_accuracy: 0.2704\n",
            "Epoch 1940/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9178 - accuracy: 0.6251\n",
            "Epoch 01940: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9178 - accuracy: 0.6251 - val_loss: 1.9955 - val_accuracy: 0.2528\n",
            "Epoch 1941/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9231 - accuracy: 0.6206\n",
            "Epoch 01941: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9231 - accuracy: 0.6206 - val_loss: 1.9621 - val_accuracy: 0.2569\n",
            "Epoch 1942/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9220 - accuracy: 0.6226\n",
            "Epoch 01942: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9221 - accuracy: 0.6226 - val_loss: 2.0719 - val_accuracy: 0.2088\n",
            "Epoch 1943/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9176 - accuracy: 0.6255\n",
            "Epoch 01943: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9176 - accuracy: 0.6255 - val_loss: 1.9812 - val_accuracy: 0.2404\n",
            "Epoch 1944/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9220 - accuracy: 0.6237\n",
            "Epoch 01944: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9220 - accuracy: 0.6237 - val_loss: 1.9871 - val_accuracy: 0.2407\n",
            "Epoch 1945/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9299 - accuracy: 0.6188\n",
            "Epoch 01945: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9299 - accuracy: 0.6188 - val_loss: 1.9162 - val_accuracy: 0.2712\n",
            "Epoch 1946/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9118 - accuracy: 0.6275\n",
            "Epoch 01946: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9118 - accuracy: 0.6275 - val_loss: 2.0145 - val_accuracy: 0.2438\n",
            "Epoch 1947/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9155 - accuracy: 0.6276\n",
            "Epoch 01947: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9159 - accuracy: 0.6273 - val_loss: 2.0991 - val_accuracy: 0.2039\n",
            "Epoch 1948/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9296 - accuracy: 0.6199\n",
            "Epoch 01948: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9296 - accuracy: 0.6199 - val_loss: 2.1120 - val_accuracy: 0.2058\n",
            "Epoch 1949/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.6267\n",
            "Epoch 01949: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9149 - accuracy: 0.6267 - val_loss: 1.9485 - val_accuracy: 0.2657\n",
            "Epoch 1950/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.6286\n",
            "Epoch 01950: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9142 - accuracy: 0.6286 - val_loss: 1.9860 - val_accuracy: 0.2394\n",
            "Epoch 1951/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9160 - accuracy: 0.6260\n",
            "Epoch 01951: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9155 - accuracy: 0.6261 - val_loss: 2.0495 - val_accuracy: 0.2211\n",
            "Epoch 1952/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9158 - accuracy: 0.6260\n",
            "Epoch 01952: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9158 - accuracy: 0.6260 - val_loss: 1.9514 - val_accuracy: 0.2561\n",
            "Epoch 1953/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9346 - accuracy: 0.6166\n",
            "Epoch 01953: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9344 - accuracy: 0.6168 - val_loss: 1.9359 - val_accuracy: 0.2591\n",
            "Epoch 1954/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9246 - accuracy: 0.6214\n",
            "Epoch 01954: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9246 - accuracy: 0.6214 - val_loss: 2.0020 - val_accuracy: 0.2507\n",
            "Epoch 1955/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9144 - accuracy: 0.6255\n",
            "Epoch 01955: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9150 - accuracy: 0.6252 - val_loss: 2.0504 - val_accuracy: 0.2175\n",
            "Epoch 1956/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9180 - accuracy: 0.6248\n",
            "Epoch 01956: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9182 - accuracy: 0.6246 - val_loss: 1.9962 - val_accuracy: 0.2522\n",
            "Epoch 1957/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9144 - accuracy: 0.6261\n",
            "Epoch 01957: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9141 - accuracy: 0.6262 - val_loss: 2.0342 - val_accuracy: 0.2184\n",
            "Epoch 1958/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9176 - accuracy: 0.6264\n",
            "Epoch 01958: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9176 - accuracy: 0.6264 - val_loss: 2.0563 - val_accuracy: 0.2084\n",
            "Epoch 1959/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9241 - accuracy: 0.6226\n",
            "Epoch 01959: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9241 - accuracy: 0.6226 - val_loss: 1.9540 - val_accuracy: 0.2532\n",
            "Epoch 1960/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9206 - accuracy: 0.6232\n",
            "Epoch 01960: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9214 - accuracy: 0.6229 - val_loss: 1.8848 - val_accuracy: 0.2765\n",
            "Epoch 1961/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9155 - accuracy: 0.6272\n",
            "Epoch 01961: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9155 - accuracy: 0.6272 - val_loss: 2.0461 - val_accuracy: 0.2162\n",
            "Epoch 1962/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.6231\n",
            "Epoch 01962: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9220 - accuracy: 0.6229 - val_loss: 1.9646 - val_accuracy: 0.2594\n",
            "Epoch 1963/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9334 - accuracy: 0.6154\n",
            "Epoch 01963: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9335 - accuracy: 0.6153 - val_loss: 1.9358 - val_accuracy: 0.2671\n",
            "Epoch 1964/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9132 - accuracy: 0.6275\n",
            "Epoch 01964: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9132 - accuracy: 0.6275 - val_loss: 2.0799 - val_accuracy: 0.2064\n",
            "Epoch 1965/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9145 - accuracy: 0.6260\n",
            "Epoch 01965: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9145 - accuracy: 0.6260 - val_loss: 2.0240 - val_accuracy: 0.2287\n",
            "Epoch 1966/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9227 - accuracy: 0.6217\n",
            "Epoch 01966: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6217 - val_loss: 1.9739 - val_accuracy: 0.2492\n",
            "Epoch 1967/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9148 - accuracy: 0.6259\n",
            "Epoch 01967: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9148 - accuracy: 0.6259 - val_loss: 1.9999 - val_accuracy: 0.2526\n",
            "Epoch 1968/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9227 - accuracy: 0.6223\n",
            "Epoch 01968: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9227 - accuracy: 0.6223 - val_loss: 1.9733 - val_accuracy: 0.2372\n",
            "Epoch 1969/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9259 - accuracy: 0.6205\n",
            "Epoch 01969: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9259 - accuracy: 0.6205 - val_loss: 1.9970 - val_accuracy: 0.2465\n",
            "Epoch 1970/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9322 - accuracy: 0.6152\n",
            "Epoch 01970: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9320 - accuracy: 0.6153 - val_loss: 1.9174 - val_accuracy: 0.2804\n",
            "Epoch 1971/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9243 - accuracy: 0.6215\n",
            "Epoch 01971: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9243 - accuracy: 0.6215 - val_loss: 1.9889 - val_accuracy: 0.2342\n",
            "Epoch 1972/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9240 - accuracy: 0.6209\n",
            "Epoch 01972: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9240 - accuracy: 0.6209 - val_loss: 1.9206 - val_accuracy: 0.2764\n",
            "Epoch 1973/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9157 - accuracy: 0.6248\n",
            "Epoch 01973: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9157 - accuracy: 0.6248 - val_loss: 1.9947 - val_accuracy: 0.2441\n",
            "Epoch 1974/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9188 - accuracy: 0.6261\n",
            "Epoch 01974: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9188 - accuracy: 0.6261 - val_loss: 2.0512 - val_accuracy: 0.2164\n",
            "Epoch 1975/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9220 - accuracy: 0.6218\n",
            "Epoch 01975: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9220 - accuracy: 0.6218 - val_loss: 1.9719 - val_accuracy: 0.2583\n",
            "Epoch 1976/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9364 - accuracy: 0.6150\n",
            "Epoch 01976: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9364 - accuracy: 0.6150 - val_loss: 1.8825 - val_accuracy: 0.3053\n",
            "Epoch 1977/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9238 - accuracy: 0.6218\n",
            "Epoch 01977: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9242 - accuracy: 0.6216 - val_loss: 2.0144 - val_accuracy: 0.2388\n",
            "Epoch 1978/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9253 - accuracy: 0.6205\n",
            "Epoch 01978: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9248 - accuracy: 0.6208 - val_loss: 1.9272 - val_accuracy: 0.2705\n",
            "Epoch 1979/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.6187\n",
            "Epoch 01979: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9295 - accuracy: 0.6187 - val_loss: 1.9468 - val_accuracy: 0.2643\n",
            "Epoch 1980/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6252\n",
            "Epoch 01980: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9150 - accuracy: 0.6252 - val_loss: 2.0038 - val_accuracy: 0.2530\n",
            "Epoch 1981/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9358 - accuracy: 0.6160\n",
            "Epoch 01981: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9358 - accuracy: 0.6160 - val_loss: 2.0496 - val_accuracy: 0.2097\n",
            "Epoch 1982/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9328 - accuracy: 0.6201\n",
            "Epoch 01982: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9322 - accuracy: 0.6204 - val_loss: 1.9980 - val_accuracy: 0.2530\n",
            "Epoch 1983/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9253 - accuracy: 0.6208\n",
            "Epoch 01983: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9253 - accuracy: 0.6208 - val_loss: 1.9349 - val_accuracy: 0.2749\n",
            "Epoch 1984/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9287 - accuracy: 0.6204\n",
            "Epoch 01984: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9279 - accuracy: 0.6208 - val_loss: 1.9777 - val_accuracy: 0.2635\n",
            "Epoch 1985/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9159 - accuracy: 0.6275\n",
            "Epoch 01985: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9156 - accuracy: 0.6277 - val_loss: 2.0363 - val_accuracy: 0.2262\n",
            "Epoch 1986/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9143 - accuracy: 0.6257\n",
            "Epoch 01986: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9143 - accuracy: 0.6257 - val_loss: 2.0054 - val_accuracy: 0.2259\n",
            "Epoch 1987/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9177 - accuracy: 0.6238\n",
            "Epoch 01987: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9177 - accuracy: 0.6238 - val_loss: 1.9799 - val_accuracy: 0.2618\n",
            "Epoch 1988/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9246 - accuracy: 0.6220\n",
            "Epoch 01988: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9247 - accuracy: 0.6219 - val_loss: 1.8984 - val_accuracy: 0.2935\n",
            "Epoch 1989/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9436 - accuracy: 0.6110\n",
            "Epoch 01989: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9436 - accuracy: 0.6110 - val_loss: 2.0096 - val_accuracy: 0.2439\n",
            "Epoch 1990/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9285 - accuracy: 0.6210\n",
            "Epoch 01990: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9283 - accuracy: 0.6211 - val_loss: 2.0153 - val_accuracy: 0.2318\n",
            "Epoch 1991/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9227 - accuracy: 0.6230\n",
            "Epoch 01991: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9220 - accuracy: 0.6234 - val_loss: 2.0061 - val_accuracy: 0.2303\n",
            "Epoch 1992/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9161 - accuracy: 0.6254\n",
            "Epoch 01992: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9167 - accuracy: 0.6249 - val_loss: 2.0842 - val_accuracy: 0.2046\n",
            "Epoch 1993/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9210 - accuracy: 0.6225\n",
            "Epoch 01993: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9210 - accuracy: 0.6225 - val_loss: 2.0066 - val_accuracy: 0.2420\n",
            "Epoch 1994/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9136 - accuracy: 0.6292\n",
            "Epoch 01994: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9143 - accuracy: 0.6288 - val_loss: 2.0058 - val_accuracy: 0.2463\n",
            "Epoch 1995/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9296 - accuracy: 0.6185\n",
            "Epoch 01995: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9293 - accuracy: 0.6188 - val_loss: 1.9564 - val_accuracy: 0.2641\n",
            "Epoch 1996/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9153 - accuracy: 0.6263\n",
            "Epoch 01996: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9153 - accuracy: 0.6263 - val_loss: 1.9636 - val_accuracy: 0.2635\n",
            "Epoch 1997/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9176 - accuracy: 0.6240\n",
            "Epoch 01997: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 44ms/step - loss: 0.9176 - accuracy: 0.6240 - val_loss: 1.9566 - val_accuracy: 0.2539\n",
            "Epoch 1998/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9216 - accuracy: 0.6247\n",
            "Epoch 01998: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9216 - accuracy: 0.6247 - val_loss: 1.9644 - val_accuracy: 0.2608\n",
            "Epoch 1999/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9271 - accuracy: 0.6208\n",
            "Epoch 01999: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9271 - accuracy: 0.6208 - val_loss: 2.0123 - val_accuracy: 0.2284\n",
            "Epoch 2000/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9291 - accuracy: 0.6198\n",
            "Epoch 02000: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9291 - accuracy: 0.6195 - val_loss: 1.9379 - val_accuracy: 0.2510\n",
            "Epoch 2001/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9137 - accuracy: 0.6277\n",
            "Epoch 02001: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9149 - accuracy: 0.6271 - val_loss: 2.0421 - val_accuracy: 0.2256\n",
            "Epoch 2002/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9172 - accuracy: 0.6242\n",
            "Epoch 02002: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9168 - accuracy: 0.6244 - val_loss: 1.9601 - val_accuracy: 0.2607\n",
            "Epoch 2003/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9150 - accuracy: 0.6269\n",
            "Epoch 02003: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9152 - accuracy: 0.6266 - val_loss: 1.9762 - val_accuracy: 0.2582\n",
            "Epoch 2004/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9207 - accuracy: 0.6231\n",
            "Epoch 02004: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9208 - accuracy: 0.6231 - val_loss: 2.0218 - val_accuracy: 0.2131\n",
            "Epoch 2005/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6267\n",
            "Epoch 02005: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9150 - accuracy: 0.6267 - val_loss: 1.9798 - val_accuracy: 0.2503\n",
            "Epoch 2006/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9189 - accuracy: 0.6257\n",
            "Epoch 02006: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9189 - accuracy: 0.6257 - val_loss: 2.0124 - val_accuracy: 0.2402\n",
            "Epoch 2007/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9193 - accuracy: 0.6229\n",
            "Epoch 02007: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9202 - accuracy: 0.6224 - val_loss: 2.0059 - val_accuracy: 0.2372\n",
            "Epoch 2008/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9273 - accuracy: 0.6187\n",
            "Epoch 02008: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9282 - accuracy: 0.6183 - val_loss: 1.8512 - val_accuracy: 0.3048\n",
            "Epoch 2009/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6275\n",
            "Epoch 02009: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9159 - accuracy: 0.6275 - val_loss: 1.9768 - val_accuracy: 0.2484\n",
            "Epoch 2010/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9170 - accuracy: 0.6250\n",
            "Epoch 02010: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9170 - accuracy: 0.6250 - val_loss: 1.9895 - val_accuracy: 0.2363\n",
            "Epoch 2011/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9152 - accuracy: 0.6258\n",
            "Epoch 02011: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6260 - val_loss: 2.0354 - val_accuracy: 0.2209\n",
            "Epoch 2012/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9132 - accuracy: 0.6284\n",
            "Epoch 02012: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9132 - accuracy: 0.6283 - val_loss: 1.8269 - val_accuracy: 0.3104\n",
            "Epoch 2013/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.6229\n",
            "Epoch 02013: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9241 - accuracy: 0.6232 - val_loss: 1.9474 - val_accuracy: 0.2560\n",
            "Epoch 2014/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9232 - accuracy: 0.6222\n",
            "Epoch 02014: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9232 - accuracy: 0.6222 - val_loss: 1.9716 - val_accuracy: 0.2504\n",
            "Epoch 2015/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9181 - accuracy: 0.6253\n",
            "Epoch 02015: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9179 - accuracy: 0.6252 - val_loss: 1.9824 - val_accuracy: 0.2492\n",
            "Epoch 2016/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9133 - accuracy: 0.6279\n",
            "Epoch 02016: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9133 - accuracy: 0.6279 - val_loss: 2.0377 - val_accuracy: 0.2270\n",
            "Epoch 2017/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9148 - accuracy: 0.6273\n",
            "Epoch 02017: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9149 - accuracy: 0.6272 - val_loss: 1.9915 - val_accuracy: 0.2433\n",
            "Epoch 2018/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9323 - accuracy: 0.6160\n",
            "Epoch 02018: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9321 - accuracy: 0.6161 - val_loss: 2.0388 - val_accuracy: 0.2247\n",
            "Epoch 2019/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9250 - accuracy: 0.6218\n",
            "Epoch 02019: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9255 - accuracy: 0.6217 - val_loss: 1.8307 - val_accuracy: 0.2769\n",
            "Epoch 2020/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9196 - accuracy: 0.6238\n",
            "Epoch 02020: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9195 - accuracy: 0.6240 - val_loss: 1.9820 - val_accuracy: 0.2507\n",
            "Epoch 2021/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9116 - accuracy: 0.6299\n",
            "Epoch 02021: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9117 - accuracy: 0.6299 - val_loss: 2.0301 - val_accuracy: 0.2396\n",
            "Epoch 2022/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9222 - accuracy: 0.6230\n",
            "Epoch 02022: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9222 - accuracy: 0.6230 - val_loss: 1.9179 - val_accuracy: 0.2731\n",
            "Epoch 2023/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9181 - accuracy: 0.6245\n",
            "Epoch 02023: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9188 - accuracy: 0.6241 - val_loss: 1.8989 - val_accuracy: 0.2940\n",
            "Epoch 2024/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9166 - accuracy: 0.6261\n",
            "Epoch 02024: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9165 - accuracy: 0.6262 - val_loss: 2.0526 - val_accuracy: 0.2229\n",
            "Epoch 2025/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9140 - accuracy: 0.6261\n",
            "Epoch 02025: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9140 - accuracy: 0.6261 - val_loss: 2.0242 - val_accuracy: 0.2450\n",
            "Epoch 2026/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9224 - accuracy: 0.6231\n",
            "Epoch 02026: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6229 - val_loss: 2.0171 - val_accuracy: 0.2381\n",
            "Epoch 2027/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9236 - accuracy: 0.6226\n",
            "Epoch 02027: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6229 - val_loss: 1.9968 - val_accuracy: 0.2488\n",
            "Epoch 2028/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9208 - accuracy: 0.6244\n",
            "Epoch 02028: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9208 - accuracy: 0.6244 - val_loss: 2.0366 - val_accuracy: 0.2287\n",
            "Epoch 2029/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9222 - accuracy: 0.6228\n",
            "Epoch 02029: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9222 - accuracy: 0.6228 - val_loss: 1.9426 - val_accuracy: 0.2580\n",
            "Epoch 2030/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9321 - accuracy: 0.6151\n",
            "Epoch 02030: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9321 - accuracy: 0.6151 - val_loss: 1.9637 - val_accuracy: 0.2506\n",
            "Epoch 2031/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.6254\n",
            "Epoch 02031: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9218 - accuracy: 0.6254 - val_loss: 1.9361 - val_accuracy: 0.2698\n",
            "Epoch 2032/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9209 - accuracy: 0.6242\n",
            "Epoch 02032: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9208 - accuracy: 0.6242 - val_loss: 1.9788 - val_accuracy: 0.2507\n",
            "Epoch 2033/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9145 - accuracy: 0.6262\n",
            "Epoch 02033: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9145 - accuracy: 0.6262 - val_loss: 2.0552 - val_accuracy: 0.2285\n",
            "Epoch 2034/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9222 - accuracy: 0.6246\n",
            "Epoch 02034: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9222 - accuracy: 0.6246 - val_loss: 2.0200 - val_accuracy: 0.2112\n",
            "Epoch 2035/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9202 - accuracy: 0.6227\n",
            "Epoch 02035: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9202 - accuracy: 0.6227 - val_loss: 2.0406 - val_accuracy: 0.2292\n",
            "Epoch 2036/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9215 - accuracy: 0.6226\n",
            "Epoch 02036: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9215 - accuracy: 0.6226 - val_loss: 1.9847 - val_accuracy: 0.2479\n",
            "Epoch 2037/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9107 - accuracy: 0.6304\n",
            "Epoch 02037: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9107 - accuracy: 0.6303 - val_loss: 1.9417 - val_accuracy: 0.2483\n",
            "Epoch 2038/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9171 - accuracy: 0.6252\n",
            "Epoch 02038: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9166 - accuracy: 0.6252 - val_loss: 1.9518 - val_accuracy: 0.2596\n",
            "Epoch 2039/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9193 - accuracy: 0.6239\n",
            "Epoch 02039: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9192 - accuracy: 0.6240 - val_loss: 2.0214 - val_accuracy: 0.2294\n",
            "Epoch 2040/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.6216\n",
            "Epoch 02040: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9223 - accuracy: 0.6216 - val_loss: 2.0766 - val_accuracy: 0.2032\n",
            "Epoch 2041/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9370 - accuracy: 0.6146\n",
            "Epoch 02041: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9370 - accuracy: 0.6146 - val_loss: 1.9865 - val_accuracy: 0.2416\n",
            "Epoch 2042/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9267 - accuracy: 0.6221\n",
            "Epoch 02042: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9267 - accuracy: 0.6221 - val_loss: 1.9858 - val_accuracy: 0.2392\n",
            "Epoch 2043/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9248 - accuracy: 0.6219\n",
            "Epoch 02043: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9248 - accuracy: 0.6219 - val_loss: 1.9727 - val_accuracy: 0.2554\n",
            "Epoch 2044/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9285 - accuracy: 0.6195\n",
            "Epoch 02044: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9286 - accuracy: 0.6194 - val_loss: 1.9498 - val_accuracy: 0.2841\n",
            "Epoch 2045/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9119 - accuracy: 0.6274\n",
            "Epoch 02045: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9119 - accuracy: 0.6274 - val_loss: 1.9628 - val_accuracy: 0.2577\n",
            "Epoch 2046/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9186 - accuracy: 0.6263\n",
            "Epoch 02046: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9185 - accuracy: 0.6263 - val_loss: 1.9900 - val_accuracy: 0.2243\n",
            "Epoch 2047/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.6216\n",
            "Epoch 02047: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9245 - accuracy: 0.6217 - val_loss: 1.9374 - val_accuracy: 0.2637\n",
            "Epoch 2048/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9201 - accuracy: 0.6222\n",
            "Epoch 02048: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9201 - accuracy: 0.6222 - val_loss: 1.9608 - val_accuracy: 0.2585\n",
            "Epoch 2049/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6259\n",
            "Epoch 02049: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9159 - accuracy: 0.6259 - val_loss: 2.0167 - val_accuracy: 0.2428\n",
            "Epoch 2050/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9205 - accuracy: 0.6233\n",
            "Epoch 02050: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9203 - accuracy: 0.6234 - val_loss: 2.0275 - val_accuracy: 0.2232\n",
            "Epoch 2051/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9264 - accuracy: 0.6193\n",
            "Epoch 02051: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9264 - accuracy: 0.6193 - val_loss: 1.9707 - val_accuracy: 0.2445\n",
            "Epoch 2052/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.6271\n",
            "Epoch 02052: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9142 - accuracy: 0.6271 - val_loss: 2.0110 - val_accuracy: 0.2507\n",
            "Epoch 2053/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9240 - accuracy: 0.6224\n",
            "Epoch 02053: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9240 - accuracy: 0.6225 - val_loss: 1.9935 - val_accuracy: 0.2463\n",
            "Epoch 2054/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.6275\n",
            "Epoch 02054: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9146 - accuracy: 0.6275 - val_loss: 1.9104 - val_accuracy: 0.2677\n",
            "Epoch 2055/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9213 - accuracy: 0.6223\n",
            "Epoch 02055: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9216 - accuracy: 0.6223 - val_loss: 2.0816 - val_accuracy: 0.2169\n",
            "Epoch 2056/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9182 - accuracy: 0.6260\n",
            "Epoch 02056: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9180 - accuracy: 0.6260 - val_loss: 1.9450 - val_accuracy: 0.2642\n",
            "Epoch 2057/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9244 - accuracy: 0.6203\n",
            "Epoch 02057: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9236 - accuracy: 0.6208 - val_loss: 1.9856 - val_accuracy: 0.2465\n",
            "Epoch 2058/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9446 - accuracy: 0.6113\n",
            "Epoch 02058: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9446 - accuracy: 0.6113 - val_loss: 2.0050 - val_accuracy: 0.2209\n",
            "Epoch 2059/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9247 - accuracy: 0.6212\n",
            "Epoch 02059: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9247 - accuracy: 0.6212 - val_loss: 2.0325 - val_accuracy: 0.2261\n",
            "Epoch 2060/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9217 - accuracy: 0.6235\n",
            "Epoch 02060: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9217 - accuracy: 0.6235 - val_loss: 2.0380 - val_accuracy: 0.2322\n",
            "Epoch 2061/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9242 - accuracy: 0.6228\n",
            "Epoch 02061: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9236 - accuracy: 0.6230 - val_loss: 2.0068 - val_accuracy: 0.2296\n",
            "Epoch 2062/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9299 - accuracy: 0.6192\n",
            "Epoch 02062: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9299 - accuracy: 0.6192 - val_loss: 1.9706 - val_accuracy: 0.2422\n",
            "Epoch 2063/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9390 - accuracy: 0.6148\n",
            "Epoch 02063: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9390 - accuracy: 0.6148 - val_loss: 2.0444 - val_accuracy: 0.2226\n",
            "Epoch 2064/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6310\n",
            "Epoch 02064: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9108 - accuracy: 0.6310 - val_loss: 1.9613 - val_accuracy: 0.2580\n",
            "Epoch 2065/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9196 - accuracy: 0.6241\n",
            "Epoch 02065: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9196 - accuracy: 0.6241 - val_loss: 2.0301 - val_accuracy: 0.2123\n",
            "Epoch 2066/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9434 - accuracy: 0.6114\n",
            "Epoch 02066: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9434 - accuracy: 0.6114 - val_loss: 2.1187 - val_accuracy: 0.1916\n",
            "Epoch 2067/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.6204\n",
            "Epoch 02067: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9260 - accuracy: 0.6204 - val_loss: 1.9682 - val_accuracy: 0.2539\n",
            "Epoch 2068/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9162 - accuracy: 0.6261\n",
            "Epoch 02068: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9156 - accuracy: 0.6266 - val_loss: 2.0831 - val_accuracy: 0.2035\n",
            "Epoch 2069/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9192 - accuracy: 0.6249\n",
            "Epoch 02069: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9194 - accuracy: 0.6249 - val_loss: 1.9550 - val_accuracy: 0.2608\n",
            "Epoch 2070/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9155 - accuracy: 0.6262\n",
            "Epoch 02070: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9155 - accuracy: 0.6261 - val_loss: 1.9966 - val_accuracy: 0.2464\n",
            "Epoch 2071/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9193 - accuracy: 0.6259\n",
            "Epoch 02071: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9195 - accuracy: 0.6260 - val_loss: 1.9901 - val_accuracy: 0.2541\n",
            "Epoch 2072/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9178 - accuracy: 0.6250\n",
            "Epoch 02072: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9174 - accuracy: 0.6249 - val_loss: 1.9380 - val_accuracy: 0.2567\n",
            "Epoch 2073/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9195 - accuracy: 0.6231\n",
            "Epoch 02073: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9195 - accuracy: 0.6229 - val_loss: 1.9505 - val_accuracy: 0.2528\n",
            "Epoch 2074/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9188 - accuracy: 0.6234\n",
            "Epoch 02074: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9188 - accuracy: 0.6234 - val_loss: 2.0435 - val_accuracy: 0.2220\n",
            "Epoch 2075/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9112 - accuracy: 0.6276\n",
            "Epoch 02075: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9112 - accuracy: 0.6276 - val_loss: 2.0071 - val_accuracy: 0.2391\n",
            "Epoch 2076/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9238 - accuracy: 0.6222\n",
            "Epoch 02076: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9238 - accuracy: 0.6222 - val_loss: 1.9962 - val_accuracy: 0.2353\n",
            "Epoch 2077/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9175 - accuracy: 0.6262\n",
            "Epoch 02077: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9171 - accuracy: 0.6263 - val_loss: 1.9823 - val_accuracy: 0.2565\n",
            "Epoch 2078/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9297 - accuracy: 0.6193\n",
            "Epoch 02078: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9297 - accuracy: 0.6193 - val_loss: 1.9401 - val_accuracy: 0.2585\n",
            "Epoch 2079/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.6222\n",
            "Epoch 02079: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9223 - accuracy: 0.6222 - val_loss: 2.0066 - val_accuracy: 0.2565\n",
            "Epoch 2080/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9211 - accuracy: 0.6224\n",
            "Epoch 02080: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9211 - accuracy: 0.6224 - val_loss: 1.9208 - val_accuracy: 0.2726\n",
            "Epoch 2081/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9160 - accuracy: 0.6261\n",
            "Epoch 02081: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9161 - accuracy: 0.6260 - val_loss: 1.9895 - val_accuracy: 0.2486\n",
            "Epoch 2082/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9292 - accuracy: 0.6186\n",
            "Epoch 02082: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9292 - accuracy: 0.6186 - val_loss: 1.9731 - val_accuracy: 0.2437\n",
            "Epoch 2083/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9239 - accuracy: 0.6209\n",
            "Epoch 02083: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9242 - accuracy: 0.6211 - val_loss: 1.9713 - val_accuracy: 0.2502\n",
            "Epoch 2084/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9144 - accuracy: 0.6283\n",
            "Epoch 02084: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9144 - accuracy: 0.6283 - val_loss: 2.0043 - val_accuracy: 0.2288\n",
            "Epoch 2085/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6266\n",
            "Epoch 02085: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9159 - accuracy: 0.6266 - val_loss: 2.1072 - val_accuracy: 0.2077\n",
            "Epoch 2086/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9227 - accuracy: 0.6213\n",
            "Epoch 02086: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6213 - val_loss: 2.0808 - val_accuracy: 0.2041\n",
            "Epoch 2087/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9145 - accuracy: 0.6259\n",
            "Epoch 02087: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9145 - accuracy: 0.6259 - val_loss: 2.0379 - val_accuracy: 0.2238\n",
            "Epoch 2088/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9321 - accuracy: 0.6164\n",
            "Epoch 02088: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9320 - accuracy: 0.6164 - val_loss: 1.9723 - val_accuracy: 0.2539\n",
            "Epoch 2089/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.6246\n",
            "Epoch 02089: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9210 - accuracy: 0.6242 - val_loss: 1.9938 - val_accuracy: 0.2311\n",
            "Epoch 2090/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9210 - accuracy: 0.6217\n",
            "Epoch 02090: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9210 - accuracy: 0.6217 - val_loss: 1.9649 - val_accuracy: 0.2580\n",
            "Epoch 2091/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9167 - accuracy: 0.6267\n",
            "Epoch 02091: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9171 - accuracy: 0.6263 - val_loss: 1.9637 - val_accuracy: 0.2448\n",
            "Epoch 2092/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9422 - accuracy: 0.6108\n",
            "Epoch 02092: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9422 - accuracy: 0.6108 - val_loss: 1.9839 - val_accuracy: 0.2317\n",
            "Epoch 2093/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.6192\n",
            "Epoch 02093: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9277 - accuracy: 0.6191 - val_loss: 2.0281 - val_accuracy: 0.2334\n",
            "Epoch 2094/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9152 - accuracy: 0.6253\n",
            "Epoch 02094: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9159 - accuracy: 0.6252 - val_loss: 1.9821 - val_accuracy: 0.2368\n",
            "Epoch 2095/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9338 - accuracy: 0.6170\n",
            "Epoch 02095: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9338 - accuracy: 0.6170 - val_loss: 1.9679 - val_accuracy: 0.2587\n",
            "Epoch 2096/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9368 - accuracy: 0.6155\n",
            "Epoch 02096: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9368 - accuracy: 0.6155 - val_loss: 1.9282 - val_accuracy: 0.2565\n",
            "Epoch 2097/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.6277\n",
            "Epoch 02097: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9113 - accuracy: 0.6277 - val_loss: 2.1176 - val_accuracy: 0.1982\n",
            "Epoch 2098/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9359 - accuracy: 0.6151\n",
            "Epoch 02098: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9359 - accuracy: 0.6151 - val_loss: 1.9573 - val_accuracy: 0.2465\n",
            "Epoch 2099/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9230 - accuracy: 0.6215\n",
            "Epoch 02099: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9230 - accuracy: 0.6215 - val_loss: 2.1172 - val_accuracy: 0.1918\n",
            "Epoch 2100/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9201 - accuracy: 0.6260\n",
            "Epoch 02100: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9198 - accuracy: 0.6263 - val_loss: 1.9650 - val_accuracy: 0.2525\n",
            "Epoch 2101/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9193 - accuracy: 0.6250\n",
            "Epoch 02101: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9197 - accuracy: 0.6248 - val_loss: 1.9100 - val_accuracy: 0.2725\n",
            "Epoch 2102/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9184 - accuracy: 0.6253\n",
            "Epoch 02102: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9182 - accuracy: 0.6254 - val_loss: 2.0260 - val_accuracy: 0.2228\n",
            "Epoch 2103/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9229 - accuracy: 0.6225\n",
            "Epoch 02103: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9229 - accuracy: 0.6225 - val_loss: 2.0258 - val_accuracy: 0.2342\n",
            "Epoch 2104/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9189 - accuracy: 0.6253\n",
            "Epoch 02104: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9189 - accuracy: 0.6250 - val_loss: 1.9998 - val_accuracy: 0.2373\n",
            "Epoch 2105/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9140 - accuracy: 0.6266\n",
            "Epoch 02105: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9140 - accuracy: 0.6266 - val_loss: 1.9803 - val_accuracy: 0.2352\n",
            "Epoch 2106/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9246 - accuracy: 0.6220\n",
            "Epoch 02106: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9246 - accuracy: 0.6220 - val_loss: 1.9941 - val_accuracy: 0.2326\n",
            "Epoch 2107/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9280 - accuracy: 0.6192\n",
            "Epoch 02107: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9279 - accuracy: 0.6194 - val_loss: 2.0616 - val_accuracy: 0.2048\n",
            "Epoch 2108/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.6270\n",
            "Epoch 02108: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9138 - accuracy: 0.6270 - val_loss: 1.9152 - val_accuracy: 0.2737\n",
            "Epoch 2109/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9136 - accuracy: 0.6283\n",
            "Epoch 02109: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9136 - accuracy: 0.6283 - val_loss: 1.9773 - val_accuracy: 0.2496\n",
            "Epoch 2110/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9120 - accuracy: 0.6268\n",
            "Epoch 02110: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9115 - accuracy: 0.6268 - val_loss: 2.0074 - val_accuracy: 0.2473\n",
            "Epoch 2111/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9165 - accuracy: 0.6252\n",
            "Epoch 02111: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9165 - accuracy: 0.6252 - val_loss: 1.9929 - val_accuracy: 0.2590\n",
            "Epoch 2112/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9147 - accuracy: 0.6279\n",
            "Epoch 02112: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9147 - accuracy: 0.6279 - val_loss: 2.0247 - val_accuracy: 0.2342\n",
            "Epoch 2113/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9168 - accuracy: 0.6227\n",
            "Epoch 02113: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9171 - accuracy: 0.6229 - val_loss: 1.9052 - val_accuracy: 0.2800\n",
            "Epoch 2114/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9273 - accuracy: 0.6187\n",
            "Epoch 02114: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9273 - accuracy: 0.6187 - val_loss: 2.0201 - val_accuracy: 0.2297\n",
            "Epoch 2115/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9226 - accuracy: 0.6221\n",
            "Epoch 02115: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9226 - accuracy: 0.6221 - val_loss: 2.1173 - val_accuracy: 0.2049\n",
            "Epoch 2116/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9153 - accuracy: 0.6263\n",
            "Epoch 02116: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9153 - accuracy: 0.6263 - val_loss: 2.0399 - val_accuracy: 0.2318\n",
            "Epoch 2117/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9211 - accuracy: 0.6233\n",
            "Epoch 02117: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9211 - accuracy: 0.6233 - val_loss: 1.9613 - val_accuracy: 0.2357\n",
            "Epoch 2118/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.6272\n",
            "Epoch 02118: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9161 - accuracy: 0.6272 - val_loss: 2.0605 - val_accuracy: 0.2198\n",
            "Epoch 2119/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9390 - accuracy: 0.6151\n",
            "Epoch 02119: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 43ms/step - loss: 0.9387 - accuracy: 0.6151 - val_loss: 1.9223 - val_accuracy: 0.2798\n",
            "Epoch 2120/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.6247\n",
            "Epoch 02120: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9182 - accuracy: 0.6247 - val_loss: 2.0422 - val_accuracy: 0.2264\n",
            "Epoch 2121/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6274\n",
            "Epoch 02121: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9159 - accuracy: 0.6274 - val_loss: 2.0419 - val_accuracy: 0.2287\n",
            "Epoch 2122/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9206 - accuracy: 0.6239\n",
            "Epoch 02122: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9207 - accuracy: 0.6239 - val_loss: 2.0804 - val_accuracy: 0.2198\n",
            "Epoch 2123/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9151 - accuracy: 0.6259\n",
            "Epoch 02123: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9147 - accuracy: 0.6259 - val_loss: 1.9843 - val_accuracy: 0.2477\n",
            "Epoch 2124/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9162 - accuracy: 0.6241\n",
            "Epoch 02124: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9166 - accuracy: 0.6240 - val_loss: 2.0841 - val_accuracy: 0.2090\n",
            "Epoch 2125/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9204 - accuracy: 0.6231\n",
            "Epoch 02125: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9205 - accuracy: 0.6228 - val_loss: 1.9792 - val_accuracy: 0.2465\n",
            "Epoch 2126/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.6272\n",
            "Epoch 02126: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9135 - accuracy: 0.6272 - val_loss: 1.9608 - val_accuracy: 0.2654\n",
            "Epoch 2127/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9097 - accuracy: 0.6277\n",
            "Epoch 02127: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9105 - accuracy: 0.6272 - val_loss: 2.0223 - val_accuracy: 0.2398\n",
            "Epoch 2128/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9117 - accuracy: 0.6276\n",
            "Epoch 02128: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9117 - accuracy: 0.6276 - val_loss: 1.9480 - val_accuracy: 0.2622\n",
            "Epoch 2129/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9177 - accuracy: 0.6252\n",
            "Epoch 02129: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9177 - accuracy: 0.6252 - val_loss: 2.0616 - val_accuracy: 0.2200\n",
            "Epoch 2130/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9204 - accuracy: 0.6237\n",
            "Epoch 02130: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9199 - accuracy: 0.6238 - val_loss: 1.9799 - val_accuracy: 0.2404\n",
            "Epoch 2131/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9262 - accuracy: 0.6195\n",
            "Epoch 02131: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9262 - accuracy: 0.6195 - val_loss: 2.0158 - val_accuracy: 0.2094\n",
            "Epoch 2132/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.6271\n",
            "Epoch 02132: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9113 - accuracy: 0.6271 - val_loss: 1.9994 - val_accuracy: 0.2445\n",
            "Epoch 2133/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9154 - accuracy: 0.6255\n",
            "Epoch 02133: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9151 - accuracy: 0.6256 - val_loss: 2.0532 - val_accuracy: 0.2065\n",
            "Epoch 2134/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9180 - accuracy: 0.6226\n",
            "Epoch 02134: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9183 - accuracy: 0.6224 - val_loss: 2.0024 - val_accuracy: 0.2315\n",
            "Epoch 2135/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9176 - accuracy: 0.6255\n",
            "Epoch 02135: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9176 - accuracy: 0.6255 - val_loss: 2.0119 - val_accuracy: 0.2410\n",
            "Epoch 2136/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9125 - accuracy: 0.6277\n",
            "Epoch 02136: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9123 - accuracy: 0.6279 - val_loss: 1.9999 - val_accuracy: 0.2467\n",
            "Epoch 2137/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.6242\n",
            "Epoch 02137: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9149 - accuracy: 0.6242 - val_loss: 1.9807 - val_accuracy: 0.2402\n",
            "Epoch 2138/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9125 - accuracy: 0.6279\n",
            "Epoch 02138: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9125 - accuracy: 0.6279 - val_loss: 1.9561 - val_accuracy: 0.2610\n",
            "Epoch 2139/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9144 - accuracy: 0.6260\n",
            "Epoch 02139: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9140 - accuracy: 0.6261 - val_loss: 2.0277 - val_accuracy: 0.2225\n",
            "Epoch 2140/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6270\n",
            "Epoch 02140: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9145 - accuracy: 0.6269 - val_loss: 1.9486 - val_accuracy: 0.2515\n",
            "Epoch 2141/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9167 - accuracy: 0.6253\n",
            "Epoch 02141: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9164 - accuracy: 0.6256 - val_loss: 2.0478 - val_accuracy: 0.2195\n",
            "Epoch 2142/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9140 - accuracy: 0.6267\n",
            "Epoch 02142: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9140 - accuracy: 0.6267 - val_loss: 1.9513 - val_accuracy: 0.2567\n",
            "Epoch 2143/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9127 - accuracy: 0.6287\n",
            "Epoch 02143: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9124 - accuracy: 0.6288 - val_loss: 1.9940 - val_accuracy: 0.2331\n",
            "Epoch 2144/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9257 - accuracy: 0.6201\n",
            "Epoch 02144: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9258 - accuracy: 0.6200 - val_loss: 1.9681 - val_accuracy: 0.2448\n",
            "Epoch 2145/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9290 - accuracy: 0.6194\n",
            "Epoch 02145: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9297 - accuracy: 0.6191 - val_loss: 2.1131 - val_accuracy: 0.2059\n",
            "Epoch 2146/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9252 - accuracy: 0.6230\n",
            "Epoch 02146: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9252 - accuracy: 0.6230 - val_loss: 1.9679 - val_accuracy: 0.2466\n",
            "Epoch 2147/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.6284\n",
            "Epoch 02147: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9113 - accuracy: 0.6284 - val_loss: 1.9176 - val_accuracy: 0.2623\n",
            "Epoch 2148/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9210 - accuracy: 0.6220\n",
            "Epoch 02148: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9209 - accuracy: 0.6222 - val_loss: 2.1262 - val_accuracy: 0.1934\n",
            "Epoch 2149/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.6266\n",
            "Epoch 02149: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9146 - accuracy: 0.6266 - val_loss: 1.9980 - val_accuracy: 0.2433\n",
            "Epoch 2150/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9179 - accuracy: 0.6259\n",
            "Epoch 02150: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9187 - accuracy: 0.6256 - val_loss: 1.9806 - val_accuracy: 0.2475\n",
            "Epoch 2151/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6257\n",
            "Epoch 02151: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9172 - accuracy: 0.6257 - val_loss: 2.0030 - val_accuracy: 0.2482\n",
            "Epoch 2152/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9215 - accuracy: 0.6221\n",
            "Epoch 02152: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9222 - accuracy: 0.6216 - val_loss: 1.9333 - val_accuracy: 0.2600\n",
            "Epoch 2153/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9163 - accuracy: 0.6259\n",
            "Epoch 02153: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9162 - accuracy: 0.6260 - val_loss: 2.0736 - val_accuracy: 0.2188\n",
            "Epoch 2154/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9141 - accuracy: 0.6260\n",
            "Epoch 02154: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9141 - accuracy: 0.6260 - val_loss: 1.9825 - val_accuracy: 0.2425\n",
            "Epoch 2155/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9235 - accuracy: 0.6211\n",
            "Epoch 02155: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9235 - accuracy: 0.6211 - val_loss: 2.0000 - val_accuracy: 0.2398\n",
            "Epoch 2156/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.6203\n",
            "Epoch 02156: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9230 - accuracy: 0.6207 - val_loss: 1.9886 - val_accuracy: 0.2444\n",
            "Epoch 2157/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9247 - accuracy: 0.6207\n",
            "Epoch 02157: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9248 - accuracy: 0.6206 - val_loss: 2.0865 - val_accuracy: 0.2038\n",
            "Epoch 2158/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6290\n",
            "Epoch 02158: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9151 - accuracy: 0.6286 - val_loss: 1.9312 - val_accuracy: 0.2604\n",
            "Epoch 2159/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9178 - accuracy: 0.6246\n",
            "Epoch 02159: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9175 - accuracy: 0.6246 - val_loss: 1.9686 - val_accuracy: 0.2431\n",
            "Epoch 2160/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9235 - accuracy: 0.6219\n",
            "Epoch 02160: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9234 - accuracy: 0.6221 - val_loss: 2.0442 - val_accuracy: 0.2277\n",
            "Epoch 2161/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9164 - accuracy: 0.6268\n",
            "Epoch 02161: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9164 - accuracy: 0.6268 - val_loss: 1.9918 - val_accuracy: 0.2240\n",
            "Epoch 2162/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9122 - accuracy: 0.6265\n",
            "Epoch 02162: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9125 - accuracy: 0.6264 - val_loss: 1.9648 - val_accuracy: 0.2392\n",
            "Epoch 2163/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9185 - accuracy: 0.6246\n",
            "Epoch 02163: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9185 - accuracy: 0.6246 - val_loss: 1.9967 - val_accuracy: 0.2279\n",
            "Epoch 2164/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6281\n",
            "Epoch 02164: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9108 - accuracy: 0.6281 - val_loss: 2.0714 - val_accuracy: 0.2079\n",
            "Epoch 2165/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9171 - accuracy: 0.6255\n",
            "Epoch 02165: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9172 - accuracy: 0.6255 - val_loss: 2.0279 - val_accuracy: 0.2287\n",
            "Epoch 2166/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6257\n",
            "Epoch 02166: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9159 - accuracy: 0.6257 - val_loss: 2.0249 - val_accuracy: 0.2210\n",
            "Epoch 2167/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9178 - accuracy: 0.6239\n",
            "Epoch 02167: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9182 - accuracy: 0.6236 - val_loss: 2.0626 - val_accuracy: 0.1945\n",
            "Epoch 2168/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9187 - accuracy: 0.6267\n",
            "Epoch 02168: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9187 - accuracy: 0.6267 - val_loss: 2.0314 - val_accuracy: 0.2356\n",
            "Epoch 2169/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9336 - accuracy: 0.6150\n",
            "Epoch 02169: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9336 - accuracy: 0.6150 - val_loss: 1.9978 - val_accuracy: 0.2537\n",
            "Epoch 2170/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9398 - accuracy: 0.6138\n",
            "Epoch 02170: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9398 - accuracy: 0.6138 - val_loss: 2.0624 - val_accuracy: 0.2045\n",
            "Epoch 2171/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9180 - accuracy: 0.6241\n",
            "Epoch 02171: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9175 - accuracy: 0.6242 - val_loss: 2.0202 - val_accuracy: 0.2311\n",
            "Epoch 2172/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9188 - accuracy: 0.6231\n",
            "Epoch 02172: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9183 - accuracy: 0.6234 - val_loss: 2.0095 - val_accuracy: 0.2337\n",
            "Epoch 2173/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9129 - accuracy: 0.6287\n",
            "Epoch 02173: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9131 - accuracy: 0.6286 - val_loss: 1.9622 - val_accuracy: 0.2509\n",
            "Epoch 2174/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.6191\n",
            "Epoch 02174: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9275 - accuracy: 0.6189 - val_loss: 1.9810 - val_accuracy: 0.2342\n",
            "Epoch 2175/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9217 - accuracy: 0.6224\n",
            "Epoch 02175: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9217 - accuracy: 0.6224 - val_loss: 1.9166 - val_accuracy: 0.2748\n",
            "Epoch 2176/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9376 - accuracy: 0.6158\n",
            "Epoch 02176: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9376 - accuracy: 0.6158 - val_loss: 1.9897 - val_accuracy: 0.2525\n",
            "Epoch 2177/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.6235\n",
            "Epoch 02177: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9182 - accuracy: 0.6235 - val_loss: 2.0005 - val_accuracy: 0.2441\n",
            "Epoch 2178/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9336 - accuracy: 0.6171\n",
            "Epoch 02178: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9329 - accuracy: 0.6172 - val_loss: 2.0619 - val_accuracy: 0.2103\n",
            "Epoch 2179/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9147 - accuracy: 0.6275\n",
            "Epoch 02179: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9147 - accuracy: 0.6275 - val_loss: 2.0523 - val_accuracy: 0.2131\n",
            "Epoch 2180/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9135 - accuracy: 0.6277\n",
            "Epoch 02180: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9138 - accuracy: 0.6275 - val_loss: 2.0741 - val_accuracy: 0.2170\n",
            "Epoch 2181/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9089 - accuracy: 0.6294\n",
            "Epoch 02181: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9088 - accuracy: 0.6295 - val_loss: 1.9713 - val_accuracy: 0.2478\n",
            "Epoch 2182/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9108 - accuracy: 0.6298\n",
            "Epoch 02182: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9103 - accuracy: 0.6300 - val_loss: 1.9446 - val_accuracy: 0.2705\n",
            "Epoch 2183/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.6258\n",
            "Epoch 02183: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6258 - val_loss: 1.9998 - val_accuracy: 0.2431\n",
            "Epoch 2184/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.6258\n",
            "Epoch 02184: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9149 - accuracy: 0.6258 - val_loss: 1.9846 - val_accuracy: 0.2369\n",
            "Epoch 2185/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9157 - accuracy: 0.6264\n",
            "Epoch 02185: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9163 - accuracy: 0.6260 - val_loss: 1.9963 - val_accuracy: 0.2368\n",
            "Epoch 2186/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9225 - accuracy: 0.6217\n",
            "Epoch 02186: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9226 - accuracy: 0.6217 - val_loss: 2.0400 - val_accuracy: 0.2278\n",
            "Epoch 2187/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9125 - accuracy: 0.6296\n",
            "Epoch 02187: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9125 - accuracy: 0.6296 - val_loss: 2.0126 - val_accuracy: 0.2439\n",
            "Epoch 2188/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9165 - accuracy: 0.6240\n",
            "Epoch 02188: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9168 - accuracy: 0.6239 - val_loss: 2.0308 - val_accuracy: 0.2346\n",
            "Epoch 2189/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9176 - accuracy: 0.6261\n",
            "Epoch 02189: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9176 - accuracy: 0.6261 - val_loss: 1.9475 - val_accuracy: 0.2420\n",
            "Epoch 2190/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6268\n",
            "Epoch 02190: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9150 - accuracy: 0.6268 - val_loss: 2.0350 - val_accuracy: 0.2285\n",
            "Epoch 2191/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.6281\n",
            "Epoch 02191: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9122 - accuracy: 0.6281 - val_loss: 2.0165 - val_accuracy: 0.2265\n",
            "Epoch 2192/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9151 - accuracy: 0.6267\n",
            "Epoch 02192: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9151 - accuracy: 0.6268 - val_loss: 2.0880 - val_accuracy: 0.2098\n",
            "Epoch 2193/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9190 - accuracy: 0.6236\n",
            "Epoch 02193: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9190 - accuracy: 0.6236 - val_loss: 2.0036 - val_accuracy: 0.2469\n",
            "Epoch 2194/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9195 - accuracy: 0.6231\n",
            "Epoch 02194: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9195 - accuracy: 0.6231 - val_loss: 1.9985 - val_accuracy: 0.2309\n",
            "Epoch 2195/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9212 - accuracy: 0.6218\n",
            "Epoch 02195: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9211 - accuracy: 0.6217 - val_loss: 1.9889 - val_accuracy: 0.2361\n",
            "Epoch 2196/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9292 - accuracy: 0.6181\n",
            "Epoch 02196: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9296 - accuracy: 0.6178 - val_loss: 1.9928 - val_accuracy: 0.2466\n",
            "Epoch 2197/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9264 - accuracy: 0.6180\n",
            "Epoch 02197: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9264 - accuracy: 0.6180 - val_loss: 2.0184 - val_accuracy: 0.2294\n",
            "Epoch 2198/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9318 - accuracy: 0.6161\n",
            "Epoch 02198: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9318 - accuracy: 0.6161 - val_loss: 2.0177 - val_accuracy: 0.2242\n",
            "Epoch 2199/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9213 - accuracy: 0.6227\n",
            "Epoch 02199: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9212 - accuracy: 0.6228 - val_loss: 2.0515 - val_accuracy: 0.2149\n",
            "Epoch 2200/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9170 - accuracy: 0.6242\n",
            "Epoch 02200: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9167 - accuracy: 0.6244 - val_loss: 2.0009 - val_accuracy: 0.2420\n",
            "Epoch 2201/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9140 - accuracy: 0.6264\n",
            "Epoch 02201: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9141 - accuracy: 0.6262 - val_loss: 1.9771 - val_accuracy: 0.2343\n",
            "Epoch 2202/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9235 - accuracy: 0.6218\n",
            "Epoch 02202: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9235 - accuracy: 0.6218 - val_loss: 1.9840 - val_accuracy: 0.2442\n",
            "Epoch 2203/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9110 - accuracy: 0.6274\n",
            "Epoch 02203: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9114 - accuracy: 0.6271 - val_loss: 2.0261 - val_accuracy: 0.2326\n",
            "Epoch 2204/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9091 - accuracy: 0.6280\n",
            "Epoch 02204: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9091 - accuracy: 0.6280 - val_loss: 2.0185 - val_accuracy: 0.2469\n",
            "Epoch 2205/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9415 - accuracy: 0.6109\n",
            "Epoch 02205: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9415 - accuracy: 0.6109 - val_loss: 2.0910 - val_accuracy: 0.2052\n",
            "Epoch 2206/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9154 - accuracy: 0.6264\n",
            "Epoch 02206: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9151 - accuracy: 0.6262 - val_loss: 1.9093 - val_accuracy: 0.2700\n",
            "Epoch 2207/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6254\n",
            "Epoch 02207: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9172 - accuracy: 0.6254 - val_loss: 2.0348 - val_accuracy: 0.2284\n",
            "Epoch 2208/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9173 - accuracy: 0.6254\n",
            "Epoch 02208: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9170 - accuracy: 0.6255 - val_loss: 2.0214 - val_accuracy: 0.2291\n",
            "Epoch 2209/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9272 - accuracy: 0.6188\n",
            "Epoch 02209: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9272 - accuracy: 0.6188 - val_loss: 1.9644 - val_accuracy: 0.2461\n",
            "Epoch 2210/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9268 - accuracy: 0.6210\n",
            "Epoch 02210: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9268 - accuracy: 0.6210 - val_loss: 2.0451 - val_accuracy: 0.2282\n",
            "Epoch 2211/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9116 - accuracy: 0.6273\n",
            "Epoch 02211: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9113 - accuracy: 0.6274 - val_loss: 2.0135 - val_accuracy: 0.2379\n",
            "Epoch 2212/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.6279\n",
            "Epoch 02212: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6279 - val_loss: 2.0468 - val_accuracy: 0.2327\n",
            "Epoch 2213/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9147 - accuracy: 0.6271\n",
            "Epoch 02213: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9147 - accuracy: 0.6271 - val_loss: 2.0476 - val_accuracy: 0.2201\n",
            "Epoch 2214/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9098 - accuracy: 0.6279\n",
            "Epoch 02214: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9092 - accuracy: 0.6278 - val_loss: 2.0147 - val_accuracy: 0.2275\n",
            "Epoch 2215/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9143 - accuracy: 0.6250\n",
            "Epoch 02215: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9143 - accuracy: 0.6250 - val_loss: 2.0270 - val_accuracy: 0.2315\n",
            "Epoch 2216/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9212 - accuracy: 0.6226\n",
            "Epoch 02216: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9210 - accuracy: 0.6227 - val_loss: 2.0649 - val_accuracy: 0.2175\n",
            "Epoch 2217/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.6290\n",
            "Epoch 02217: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9097 - accuracy: 0.6290 - val_loss: 2.0057 - val_accuracy: 0.2286\n",
            "Epoch 2218/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.6250\n",
            "Epoch 02218: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9149 - accuracy: 0.6250 - val_loss: 2.0566 - val_accuracy: 0.2094\n",
            "Epoch 2219/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9175 - accuracy: 0.6258\n",
            "Epoch 02219: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9174 - accuracy: 0.6258 - val_loss: 1.9682 - val_accuracy: 0.2391\n",
            "Epoch 2220/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.6260\n",
            "Epoch 02220: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6260 - val_loss: 1.9690 - val_accuracy: 0.2526\n",
            "Epoch 2221/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9201 - accuracy: 0.6239\n",
            "Epoch 02221: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9201 - accuracy: 0.6239 - val_loss: 1.9543 - val_accuracy: 0.2593\n",
            "Epoch 2222/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9185 - accuracy: 0.6232\n",
            "Epoch 02222: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9184 - accuracy: 0.6233 - val_loss: 2.0671 - val_accuracy: 0.2032\n",
            "Epoch 2223/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9227 - accuracy: 0.6243\n",
            "Epoch 02223: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9226 - accuracy: 0.6242 - val_loss: 2.0603 - val_accuracy: 0.2118\n",
            "Epoch 2224/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9094 - accuracy: 0.6290\n",
            "Epoch 02224: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9094 - accuracy: 0.6289 - val_loss: 2.0186 - val_accuracy: 0.2361\n",
            "Epoch 2225/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9085 - accuracy: 0.6302\n",
            "Epoch 02225: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9085 - accuracy: 0.6302 - val_loss: 1.9308 - val_accuracy: 0.2709\n",
            "Epoch 2226/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9181 - accuracy: 0.6234\n",
            "Epoch 02226: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9182 - accuracy: 0.6236 - val_loss: 1.9969 - val_accuracy: 0.2599\n",
            "Epoch 2227/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9232 - accuracy: 0.6200\n",
            "Epoch 02227: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9232 - accuracy: 0.6200 - val_loss: 2.0288 - val_accuracy: 0.2303\n",
            "Epoch 2228/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9191 - accuracy: 0.6242\n",
            "Epoch 02228: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9189 - accuracy: 0.6243 - val_loss: 2.0409 - val_accuracy: 0.2151\n",
            "Epoch 2229/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9199 - accuracy: 0.6242\n",
            "Epoch 02229: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9200 - accuracy: 0.6240 - val_loss: 2.0338 - val_accuracy: 0.2315\n",
            "Epoch 2230/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6258\n",
            "Epoch 02230: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9174 - accuracy: 0.6258 - val_loss: 1.9610 - val_accuracy: 0.2466\n",
            "Epoch 2231/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6268\n",
            "Epoch 02231: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9148 - accuracy: 0.6269 - val_loss: 2.0302 - val_accuracy: 0.2347\n",
            "Epoch 2232/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9228 - accuracy: 0.6218\n",
            "Epoch 02232: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9228 - accuracy: 0.6218 - val_loss: 2.0590 - val_accuracy: 0.2214\n",
            "Epoch 2233/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9213 - accuracy: 0.6198\n",
            "Epoch 02233: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9216 - accuracy: 0.6195 - val_loss: 2.0730 - val_accuracy: 0.2162\n",
            "Epoch 2234/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9101 - accuracy: 0.6283\n",
            "Epoch 02234: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9101 - accuracy: 0.6281 - val_loss: 1.9399 - val_accuracy: 0.2637\n",
            "Epoch 2235/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9278 - accuracy: 0.6181\n",
            "Epoch 02235: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9278 - accuracy: 0.6181 - val_loss: 2.0237 - val_accuracy: 0.2334\n",
            "Epoch 2236/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9224 - accuracy: 0.6211\n",
            "Epoch 02236: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9228 - accuracy: 0.6210 - val_loss: 2.0218 - val_accuracy: 0.2180\n",
            "Epoch 2237/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6239\n",
            "Epoch 02237: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9172 - accuracy: 0.6239 - val_loss: 2.0127 - val_accuracy: 0.2383\n",
            "Epoch 2238/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.6236\n",
            "Epoch 02238: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9214 - accuracy: 0.6236 - val_loss: 2.0261 - val_accuracy: 0.2175\n",
            "Epoch 2239/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9137 - accuracy: 0.6258\n",
            "Epoch 02239: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9137 - accuracy: 0.6258 - val_loss: 1.9873 - val_accuracy: 0.2537\n",
            "Epoch 2240/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9216 - accuracy: 0.6210\n",
            "Epoch 02240: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9218 - accuracy: 0.6210 - val_loss: 2.0552 - val_accuracy: 0.2127\n",
            "Epoch 2241/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6278\n",
            "Epoch 02241: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9108 - accuracy: 0.6278 - val_loss: 2.0334 - val_accuracy: 0.2261\n",
            "Epoch 2242/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9163 - accuracy: 0.6223\n",
            "Epoch 02242: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9163 - accuracy: 0.6223 - val_loss: 2.0086 - val_accuracy: 0.2272\n",
            "Epoch 2243/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9186 - accuracy: 0.6258\n",
            "Epoch 02243: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9186 - accuracy: 0.6258 - val_loss: 2.0470 - val_accuracy: 0.2220\n",
            "Epoch 2244/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.6290\n",
            "Epoch 02244: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9081 - accuracy: 0.6290 - val_loss: 1.9794 - val_accuracy: 0.2550\n",
            "Epoch 2245/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9117 - accuracy: 0.6286\n",
            "Epoch 02245: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9117 - accuracy: 0.6286 - val_loss: 2.0121 - val_accuracy: 0.2486\n",
            "Epoch 2246/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.6273\n",
            "Epoch 02246: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9142 - accuracy: 0.6273 - val_loss: 1.9972 - val_accuracy: 0.2447\n",
            "Epoch 2247/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.6267\n",
            "Epoch 02247: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9135 - accuracy: 0.6267 - val_loss: 2.0474 - val_accuracy: 0.2201\n",
            "Epoch 2248/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9078 - accuracy: 0.6292\n",
            "Epoch 02248: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9085 - accuracy: 0.6289 - val_loss: 1.9248 - val_accuracy: 0.2848\n",
            "Epoch 2249/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9146 - accuracy: 0.6246\n",
            "Epoch 02249: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9153 - accuracy: 0.6242 - val_loss: 2.0480 - val_accuracy: 0.2153\n",
            "Epoch 2250/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9201 - accuracy: 0.6221\n",
            "Epoch 02250: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9201 - accuracy: 0.6222 - val_loss: 2.0407 - val_accuracy: 0.2181\n",
            "Epoch 2251/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9112 - accuracy: 0.6275\n",
            "Epoch 02251: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9120 - accuracy: 0.6272 - val_loss: 2.0282 - val_accuracy: 0.2302\n",
            "Epoch 2252/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6240\n",
            "Epoch 02252: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9174 - accuracy: 0.6240 - val_loss: 2.0287 - val_accuracy: 0.2248\n",
            "Epoch 2253/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9191 - accuracy: 0.6258\n",
            "Epoch 02253: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9191 - accuracy: 0.6258 - val_loss: 1.9315 - val_accuracy: 0.2475\n",
            "Epoch 2254/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9318 - accuracy: 0.6186\n",
            "Epoch 02254: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9318 - accuracy: 0.6186 - val_loss: 1.9725 - val_accuracy: 0.2510\n",
            "Epoch 2255/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9092 - accuracy: 0.6283\n",
            "Epoch 02255: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9094 - accuracy: 0.6283 - val_loss: 1.9923 - val_accuracy: 0.2373\n",
            "Epoch 2256/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9114 - accuracy: 0.6271\n",
            "Epoch 02256: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9114 - accuracy: 0.6271 - val_loss: 1.9891 - val_accuracy: 0.2434\n",
            "Epoch 2257/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9126 - accuracy: 0.6269\n",
            "Epoch 02257: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9126 - accuracy: 0.6266 - val_loss: 2.0152 - val_accuracy: 0.2281\n",
            "Epoch 2258/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9137 - accuracy: 0.6268\n",
            "Epoch 02258: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9137 - accuracy: 0.6269 - val_loss: 2.0207 - val_accuracy: 0.2352\n",
            "Epoch 2259/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9047 - accuracy: 0.6309\n",
            "Epoch 02259: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9047 - accuracy: 0.6309 - val_loss: 2.0496 - val_accuracy: 0.2279\n",
            "Epoch 2260/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9206 - accuracy: 0.6221\n",
            "Epoch 02260: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9206 - accuracy: 0.6221 - val_loss: 2.0165 - val_accuracy: 0.2201\n",
            "Epoch 2261/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9201 - accuracy: 0.6222\n",
            "Epoch 02261: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9195 - accuracy: 0.6224 - val_loss: 2.0684 - val_accuracy: 0.2057\n",
            "Epoch 2262/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.6282\n",
            "Epoch 02262: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9149 - accuracy: 0.6282 - val_loss: 1.9856 - val_accuracy: 0.2487\n",
            "Epoch 2263/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9146 - accuracy: 0.6271\n",
            "Epoch 02263: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6270 - val_loss: 1.9992 - val_accuracy: 0.2329\n",
            "Epoch 2264/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9144 - accuracy: 0.6264\n",
            "Epoch 02264: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9144 - accuracy: 0.6264 - val_loss: 2.0170 - val_accuracy: 0.2407\n",
            "Epoch 2265/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9089 - accuracy: 0.6286\n",
            "Epoch 02265: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9101 - accuracy: 0.6279 - val_loss: 2.1159 - val_accuracy: 0.2029\n",
            "Epoch 2266/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9213 - accuracy: 0.6230\n",
            "Epoch 02266: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9215 - accuracy: 0.6228 - val_loss: 1.9897 - val_accuracy: 0.2563\n",
            "Epoch 2267/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.6204\n",
            "Epoch 02267: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9233 - accuracy: 0.6204 - val_loss: 1.9802 - val_accuracy: 0.2514\n",
            "Epoch 2268/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6289\n",
            "Epoch 02268: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9121 - accuracy: 0.6289 - val_loss: 2.0271 - val_accuracy: 0.2385\n",
            "Epoch 2269/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9116 - accuracy: 0.6286\n",
            "Epoch 02269: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6286 - val_loss: 2.0497 - val_accuracy: 0.2146\n",
            "Epoch 2270/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9043 - accuracy: 0.6317\n",
            "Epoch 02270: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9043 - accuracy: 0.6317 - val_loss: 2.1039 - val_accuracy: 0.2057\n",
            "Epoch 2271/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9106 - accuracy: 0.6292\n",
            "Epoch 02271: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9108 - accuracy: 0.6292 - val_loss: 2.0739 - val_accuracy: 0.2234\n",
            "Epoch 2272/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9161 - accuracy: 0.6262\n",
            "Epoch 02272: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9159 - accuracy: 0.6263 - val_loss: 1.9741 - val_accuracy: 0.2602\n",
            "Epoch 2273/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9244 - accuracy: 0.6198\n",
            "Epoch 02273: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9248 - accuracy: 0.6193 - val_loss: 2.0194 - val_accuracy: 0.2271\n",
            "Epoch 2274/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9139 - accuracy: 0.6262\n",
            "Epoch 02274: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9139 - accuracy: 0.6262 - val_loss: 1.9615 - val_accuracy: 0.2491\n",
            "Epoch 2275/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9096 - accuracy: 0.6282\n",
            "Epoch 02275: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9096 - accuracy: 0.6278 - val_loss: 2.0531 - val_accuracy: 0.2264\n",
            "Epoch 2276/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9130 - accuracy: 0.6241\n",
            "Epoch 02276: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9136 - accuracy: 0.6236 - val_loss: 2.0660 - val_accuracy: 0.1991\n",
            "Epoch 2277/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9127 - accuracy: 0.6276\n",
            "Epoch 02277: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9127 - accuracy: 0.6276 - val_loss: 2.0002 - val_accuracy: 0.2363\n",
            "Epoch 2278/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9191 - accuracy: 0.6236\n",
            "Epoch 02278: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9191 - accuracy: 0.6236 - val_loss: 2.0659 - val_accuracy: 0.2170\n",
            "Epoch 2279/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9115 - accuracy: 0.6282\n",
            "Epoch 02279: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9115 - accuracy: 0.6282 - val_loss: 1.9447 - val_accuracy: 0.2608\n",
            "Epoch 2280/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9088 - accuracy: 0.6297\n",
            "Epoch 02280: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9088 - accuracy: 0.6297 - val_loss: 1.9817 - val_accuracy: 0.2523\n",
            "Epoch 2281/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9140 - accuracy: 0.6283\n",
            "Epoch 02281: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9140 - accuracy: 0.6283 - val_loss: 2.0057 - val_accuracy: 0.2378\n",
            "Epoch 2282/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9148 - accuracy: 0.6270\n",
            "Epoch 02282: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9149 - accuracy: 0.6271 - val_loss: 2.0016 - val_accuracy: 0.2303\n",
            "Epoch 2283/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9117 - accuracy: 0.6284\n",
            "Epoch 02283: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9115 - accuracy: 0.6285 - val_loss: 1.9088 - val_accuracy: 0.2812\n",
            "Epoch 2284/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9169 - accuracy: 0.6245\n",
            "Epoch 02284: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9168 - accuracy: 0.6246 - val_loss: 1.9897 - val_accuracy: 0.2600\n",
            "Epoch 2285/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9306 - accuracy: 0.6181\n",
            "Epoch 02285: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9309 - accuracy: 0.6181 - val_loss: 2.0658 - val_accuracy: 0.2181\n",
            "Epoch 2286/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9199 - accuracy: 0.6224\n",
            "Epoch 02286: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9199 - accuracy: 0.6224 - val_loss: 2.0453 - val_accuracy: 0.2087\n",
            "Epoch 2287/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9162 - accuracy: 0.6252\n",
            "Epoch 02287: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9162 - accuracy: 0.6252 - val_loss: 2.0348 - val_accuracy: 0.2431\n",
            "Epoch 2288/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9099 - accuracy: 0.6287\n",
            "Epoch 02288: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9099 - accuracy: 0.6287 - val_loss: 2.0190 - val_accuracy: 0.2226\n",
            "Epoch 2289/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9128 - accuracy: 0.6258\n",
            "Epoch 02289: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9125 - accuracy: 0.6258 - val_loss: 1.9858 - val_accuracy: 0.2446\n",
            "Epoch 2290/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9224 - accuracy: 0.6221\n",
            "Epoch 02290: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9224 - accuracy: 0.6221 - val_loss: 1.9073 - val_accuracy: 0.2732\n",
            "Epoch 2291/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9275 - accuracy: 0.6186\n",
            "Epoch 02291: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9275 - accuracy: 0.6186 - val_loss: 2.0367 - val_accuracy: 0.2286\n",
            "Epoch 2292/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6272\n",
            "Epoch 02292: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9124 - accuracy: 0.6272 - val_loss: 2.0111 - val_accuracy: 0.2355\n",
            "Epoch 2293/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.6268\n",
            "Epoch 02293: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9161 - accuracy: 0.6268 - val_loss: 1.9797 - val_accuracy: 0.2510\n",
            "Epoch 2294/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6255\n",
            "Epoch 02294: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6253 - val_loss: 2.0737 - val_accuracy: 0.2077\n",
            "Epoch 2295/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9140 - accuracy: 0.6260\n",
            "Epoch 02295: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9141 - accuracy: 0.6261 - val_loss: 2.0215 - val_accuracy: 0.2485\n",
            "Epoch 2296/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9145 - accuracy: 0.6251\n",
            "Epoch 02296: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9145 - accuracy: 0.6251 - val_loss: 2.0780 - val_accuracy: 0.2095\n",
            "Epoch 2297/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9057 - accuracy: 0.6286\n",
            "Epoch 02297: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9059 - accuracy: 0.6285 - val_loss: 2.0806 - val_accuracy: 0.2059\n",
            "Epoch 2298/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9301 - accuracy: 0.6199\n",
            "Epoch 02298: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9297 - accuracy: 0.6199 - val_loss: 1.9465 - val_accuracy: 0.2510\n",
            "Epoch 2299/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.6208\n",
            "Epoch 02299: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9235 - accuracy: 0.6207 - val_loss: 2.0706 - val_accuracy: 0.1986\n",
            "Epoch 2300/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9298 - accuracy: 0.6181\n",
            "Epoch 02300: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9293 - accuracy: 0.6180 - val_loss: 2.0272 - val_accuracy: 0.2308\n",
            "Epoch 2301/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9241 - accuracy: 0.6213\n",
            "Epoch 02301: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9241 - accuracy: 0.6213 - val_loss: 2.0665 - val_accuracy: 0.2116\n",
            "Epoch 2302/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.6276\n",
            "Epoch 02302: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9120 - accuracy: 0.6276 - val_loss: 2.0239 - val_accuracy: 0.2224\n",
            "Epoch 2303/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.6310\n",
            "Epoch 02303: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9081 - accuracy: 0.6310 - val_loss: 2.0256 - val_accuracy: 0.2370\n",
            "Epoch 2304/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9139 - accuracy: 0.6294\n",
            "Epoch 02304: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9139 - accuracy: 0.6294 - val_loss: 1.9461 - val_accuracy: 0.2609\n",
            "Epoch 2305/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9376 - accuracy: 0.6135\n",
            "Epoch 02305: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9365 - accuracy: 0.6140 - val_loss: 2.0273 - val_accuracy: 0.2218\n",
            "Epoch 2306/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9088 - accuracy: 0.6305\n",
            "Epoch 02306: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9091 - accuracy: 0.6302 - val_loss: 2.0966 - val_accuracy: 0.2174\n",
            "Epoch 2307/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9116 - accuracy: 0.6271\n",
            "Epoch 02307: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6271 - val_loss: 2.0563 - val_accuracy: 0.2186\n",
            "Epoch 2308/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9206 - accuracy: 0.6244\n",
            "Epoch 02308: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9206 - accuracy: 0.6248 - val_loss: 1.9546 - val_accuracy: 0.2603\n",
            "Epoch 2309/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9086 - accuracy: 0.6298\n",
            "Epoch 02309: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9086 - accuracy: 0.6298 - val_loss: 2.0456 - val_accuracy: 0.2247\n",
            "Epoch 2310/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9078 - accuracy: 0.6300\n",
            "Epoch 02310: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9075 - accuracy: 0.6301 - val_loss: 2.0525 - val_accuracy: 0.2202\n",
            "Epoch 2311/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.6239\n",
            "Epoch 02311: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9198 - accuracy: 0.6243 - val_loss: 2.0988 - val_accuracy: 0.2120\n",
            "Epoch 2312/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9127 - accuracy: 0.6263\n",
            "Epoch 02312: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9131 - accuracy: 0.6261 - val_loss: 1.9768 - val_accuracy: 0.2509\n",
            "Epoch 2313/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9240 - accuracy: 0.6216\n",
            "Epoch 02313: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9240 - accuracy: 0.6216 - val_loss: 2.0057 - val_accuracy: 0.2473\n",
            "Epoch 2314/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9215 - accuracy: 0.6224\n",
            "Epoch 02314: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9215 - accuracy: 0.6224 - val_loss: 2.0465 - val_accuracy: 0.2209\n",
            "Epoch 2315/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9173 - accuracy: 0.6236\n",
            "Epoch 02315: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9173 - accuracy: 0.6236 - val_loss: 1.9445 - val_accuracy: 0.2598\n",
            "Epoch 2316/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9153 - accuracy: 0.6258\n",
            "Epoch 02316: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9153 - accuracy: 0.6258 - val_loss: 2.0735 - val_accuracy: 0.2015\n",
            "Epoch 2317/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9305 - accuracy: 0.6173\n",
            "Epoch 02317: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9305 - accuracy: 0.6173 - val_loss: 2.0928 - val_accuracy: 0.1923\n",
            "Epoch 2318/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6270\n",
            "Epoch 02318: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9133 - accuracy: 0.6273 - val_loss: 2.0007 - val_accuracy: 0.2452\n",
            "Epoch 2319/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9248 - accuracy: 0.6191\n",
            "Epoch 02319: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9248 - accuracy: 0.6191 - val_loss: 2.0427 - val_accuracy: 0.2192\n",
            "Epoch 2320/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6282\n",
            "Epoch 02320: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9121 - accuracy: 0.6282 - val_loss: 1.9848 - val_accuracy: 0.2407\n",
            "Epoch 2321/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9128 - accuracy: 0.6264\n",
            "Epoch 02321: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9128 - accuracy: 0.6266 - val_loss: 1.9762 - val_accuracy: 0.2587\n",
            "Epoch 2322/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9230 - accuracy: 0.6212\n",
            "Epoch 02322: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9226 - accuracy: 0.6215 - val_loss: 2.0195 - val_accuracy: 0.2379\n",
            "Epoch 2323/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9156 - accuracy: 0.6256\n",
            "Epoch 02323: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9156 - accuracy: 0.6256 - val_loss: 1.9966 - val_accuracy: 0.2454\n",
            "Epoch 2324/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9118 - accuracy: 0.6273\n",
            "Epoch 02324: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9118 - accuracy: 0.6273 - val_loss: 2.0834 - val_accuracy: 0.2040\n",
            "Epoch 2325/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9095 - accuracy: 0.6281\n",
            "Epoch 02325: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9093 - accuracy: 0.6284 - val_loss: 2.0485 - val_accuracy: 0.2245\n",
            "Epoch 2326/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9094 - accuracy: 0.6292\n",
            "Epoch 02326: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9094 - accuracy: 0.6292 - val_loss: 1.9851 - val_accuracy: 0.2651\n",
            "Epoch 2327/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9160 - accuracy: 0.6261\n",
            "Epoch 02327: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9160 - accuracy: 0.6261 - val_loss: 1.9982 - val_accuracy: 0.2464\n",
            "Epoch 2328/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9098 - accuracy: 0.6294\n",
            "Epoch 02328: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9098 - accuracy: 0.6294 - val_loss: 2.0001 - val_accuracy: 0.2465\n",
            "Epoch 2329/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.6251\n",
            "Epoch 02329: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9161 - accuracy: 0.6251 - val_loss: 2.0439 - val_accuracy: 0.2093\n",
            "Epoch 2330/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.6294\n",
            "Epoch 02330: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9092 - accuracy: 0.6294 - val_loss: 1.9979 - val_accuracy: 0.2413\n",
            "Epoch 2331/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9266 - accuracy: 0.6195\n",
            "Epoch 02331: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9266 - accuracy: 0.6195 - val_loss: 2.0133 - val_accuracy: 0.2248\n",
            "Epoch 2332/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9099 - accuracy: 0.6286\n",
            "Epoch 02332: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9102 - accuracy: 0.6285 - val_loss: 1.9552 - val_accuracy: 0.2522\n",
            "Epoch 2333/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9125 - accuracy: 0.6254\n",
            "Epoch 02333: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9125 - accuracy: 0.6254 - val_loss: 2.0359 - val_accuracy: 0.2189\n",
            "Epoch 2334/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.6270\n",
            "Epoch 02334: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9138 - accuracy: 0.6270 - val_loss: 2.0331 - val_accuracy: 0.2218\n",
            "Epoch 2335/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9074 - accuracy: 0.6298\n",
            "Epoch 02335: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9074 - accuracy: 0.6298 - val_loss: 2.0378 - val_accuracy: 0.2214\n",
            "Epoch 2336/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9173 - accuracy: 0.6259\n",
            "Epoch 02336: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9180 - accuracy: 0.6256 - val_loss: 2.0620 - val_accuracy: 0.2123\n",
            "Epoch 2337/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6272\n",
            "Epoch 02337: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9108 - accuracy: 0.6272 - val_loss: 2.0510 - val_accuracy: 0.2112\n",
            "Epoch 2338/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9114 - accuracy: 0.6280\n",
            "Epoch 02338: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9112 - accuracy: 0.6283 - val_loss: 1.9491 - val_accuracy: 0.2653\n",
            "Epoch 2339/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9152 - accuracy: 0.6251\n",
            "Epoch 02339: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9152 - accuracy: 0.6251 - val_loss: 2.0752 - val_accuracy: 0.2068\n",
            "Epoch 2340/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9168 - accuracy: 0.6242\n",
            "Epoch 02340: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9167 - accuracy: 0.6243 - val_loss: 2.0471 - val_accuracy: 0.2077\n",
            "Epoch 2341/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.6211\n",
            "Epoch 02341: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9223 - accuracy: 0.6211 - val_loss: 2.0697 - val_accuracy: 0.2195\n",
            "Epoch 2342/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9114 - accuracy: 0.6296\n",
            "Epoch 02342: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9114 - accuracy: 0.6296 - val_loss: 1.9012 - val_accuracy: 0.2728\n",
            "Epoch 2343/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9200 - accuracy: 0.6243\n",
            "Epoch 02343: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9200 - accuracy: 0.6243 - val_loss: 2.1021 - val_accuracy: 0.2096\n",
            "Epoch 2344/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9122 - accuracy: 0.6268\n",
            "Epoch 02344: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6272 - val_loss: 1.9863 - val_accuracy: 0.2477\n",
            "Epoch 2345/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9070 - accuracy: 0.6303\n",
            "Epoch 02345: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9070 - accuracy: 0.6302 - val_loss: 1.9956 - val_accuracy: 0.2350\n",
            "Epoch 2346/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9148 - accuracy: 0.6257\n",
            "Epoch 02346: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9144 - accuracy: 0.6260 - val_loss: 1.9866 - val_accuracy: 0.2344\n",
            "Epoch 2347/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9170 - accuracy: 0.6243\n",
            "Epoch 02347: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9170 - accuracy: 0.6243 - val_loss: 2.0477 - val_accuracy: 0.2233\n",
            "Epoch 2348/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9115 - accuracy: 0.6270\n",
            "Epoch 02348: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9115 - accuracy: 0.6270 - val_loss: 2.0684 - val_accuracy: 0.2187\n",
            "Epoch 2349/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9119 - accuracy: 0.6260\n",
            "Epoch 02349: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9126 - accuracy: 0.6257 - val_loss: 1.9851 - val_accuracy: 0.2425\n",
            "Epoch 2350/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.6269\n",
            "Epoch 02350: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9138 - accuracy: 0.6269 - val_loss: 2.0121 - val_accuracy: 0.2398\n",
            "Epoch 2351/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9094 - accuracy: 0.6291\n",
            "Epoch 02351: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9090 - accuracy: 0.6294 - val_loss: 2.0038 - val_accuracy: 0.2415\n",
            "Epoch 2352/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9223 - accuracy: 0.6227\n",
            "Epoch 02352: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9223 - accuracy: 0.6226 - val_loss: 2.0806 - val_accuracy: 0.2031\n",
            "Epoch 2353/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9167 - accuracy: 0.6239\n",
            "Epoch 02353: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9175 - accuracy: 0.6235 - val_loss: 2.0261 - val_accuracy: 0.2292\n",
            "Epoch 2354/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9159 - accuracy: 0.6247\n",
            "Epoch 02354: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9160 - accuracy: 0.6249 - val_loss: 2.0309 - val_accuracy: 0.2383\n",
            "Epoch 2355/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9213 - accuracy: 0.6225\n",
            "Epoch 02355: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9213 - accuracy: 0.6225 - val_loss: 2.0237 - val_accuracy: 0.2271\n",
            "Epoch 2356/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9171 - accuracy: 0.6245\n",
            "Epoch 02356: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9170 - accuracy: 0.6246 - val_loss: 1.9721 - val_accuracy: 0.2435\n",
            "Epoch 2357/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9258 - accuracy: 0.6197\n",
            "Epoch 02357: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9255 - accuracy: 0.6199 - val_loss: 2.1049 - val_accuracy: 0.2023\n",
            "Epoch 2358/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9112 - accuracy: 0.6272\n",
            "Epoch 02358: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9112 - accuracy: 0.6272 - val_loss: 2.0276 - val_accuracy: 0.2256\n",
            "Epoch 2359/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9190 - accuracy: 0.6223\n",
            "Epoch 02359: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9190 - accuracy: 0.6223 - val_loss: 2.0495 - val_accuracy: 0.2070\n",
            "Epoch 2360/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9168 - accuracy: 0.6252\n",
            "Epoch 02360: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9168 - accuracy: 0.6252 - val_loss: 2.0070 - val_accuracy: 0.2410\n",
            "Epoch 2361/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6276\n",
            "Epoch 02361: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9137 - accuracy: 0.6276 - val_loss: 2.0008 - val_accuracy: 0.2426\n",
            "Epoch 2362/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6274\n",
            "Epoch 02362: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 43ms/step - loss: 0.9125 - accuracy: 0.6274 - val_loss: 2.0299 - val_accuracy: 0.2366\n",
            "Epoch 2363/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9170 - accuracy: 0.6270\n",
            "Epoch 02363: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9170 - accuracy: 0.6270 - val_loss: 2.0288 - val_accuracy: 0.2188\n",
            "Epoch 2364/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9174 - accuracy: 0.6241\n",
            "Epoch 02364: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9172 - accuracy: 0.6241 - val_loss: 2.0291 - val_accuracy: 0.2439\n",
            "Epoch 2365/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9115 - accuracy: 0.6284\n",
            "Epoch 02365: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9112 - accuracy: 0.6286 - val_loss: 2.0252 - val_accuracy: 0.2362\n",
            "Epoch 2366/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9068 - accuracy: 0.6302\n",
            "Epoch 02366: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9068 - accuracy: 0.6302 - val_loss: 2.0116 - val_accuracy: 0.2277\n",
            "Epoch 2367/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9115 - accuracy: 0.6283\n",
            "Epoch 02367: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9114 - accuracy: 0.6283 - val_loss: 2.0135 - val_accuracy: 0.2420\n",
            "Epoch 2368/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.6269\n",
            "Epoch 02368: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9122 - accuracy: 0.6269 - val_loss: 2.0030 - val_accuracy: 0.2384\n",
            "Epoch 2369/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9275 - accuracy: 0.6217\n",
            "Epoch 02369: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9272 - accuracy: 0.6218 - val_loss: 2.0392 - val_accuracy: 0.2190\n",
            "Epoch 2370/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9168 - accuracy: 0.6241\n",
            "Epoch 02370: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9168 - accuracy: 0.6241 - val_loss: 2.1124 - val_accuracy: 0.2073\n",
            "Epoch 2371/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9230 - accuracy: 0.6216\n",
            "Epoch 02371: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9230 - accuracy: 0.6216 - val_loss: 2.0104 - val_accuracy: 0.2387\n",
            "Epoch 2372/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9205 - accuracy: 0.6237\n",
            "Epoch 02372: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9205 - accuracy: 0.6237 - val_loss: 1.9775 - val_accuracy: 0.2383\n",
            "Epoch 2373/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9071 - accuracy: 0.6285\n",
            "Epoch 02373: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9068 - accuracy: 0.6290 - val_loss: 2.0794 - val_accuracy: 0.2040\n",
            "Epoch 2374/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9208 - accuracy: 0.6224\n",
            "Epoch 02374: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9208 - accuracy: 0.6224 - val_loss: 1.9599 - val_accuracy: 0.2615\n",
            "Epoch 2375/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6271\n",
            "Epoch 02375: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9108 - accuracy: 0.6271 - val_loss: 2.0118 - val_accuracy: 0.2383\n",
            "Epoch 2376/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9162 - accuracy: 0.6248\n",
            "Epoch 02376: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9153 - accuracy: 0.6251 - val_loss: 2.0750 - val_accuracy: 0.2027\n",
            "Epoch 2377/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9093 - accuracy: 0.6304\n",
            "Epoch 02377: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9093 - accuracy: 0.6304 - val_loss: 2.0453 - val_accuracy: 0.2278\n",
            "Epoch 2378/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9050 - accuracy: 0.6315\n",
            "Epoch 02378: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9052 - accuracy: 0.6314 - val_loss: 2.0323 - val_accuracy: 0.2294\n",
            "Epoch 2379/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9156 - accuracy: 0.6257\n",
            "Epoch 02379: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9155 - accuracy: 0.6258 - val_loss: 2.0122 - val_accuracy: 0.2365\n",
            "Epoch 2380/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6235\n",
            "Epoch 02380: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9174 - accuracy: 0.6235 - val_loss: 2.0886 - val_accuracy: 0.2136\n",
            "Epoch 2381/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9119 - accuracy: 0.6293\n",
            "Epoch 02381: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9117 - accuracy: 0.6294 - val_loss: 1.9860 - val_accuracy: 0.2414\n",
            "Epoch 2382/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9074 - accuracy: 0.6287\n",
            "Epoch 02382: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9072 - accuracy: 0.6289 - val_loss: 2.0336 - val_accuracy: 0.2310\n",
            "Epoch 2383/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9157 - accuracy: 0.6254\n",
            "Epoch 02383: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9157 - accuracy: 0.6254 - val_loss: 2.0084 - val_accuracy: 0.2248\n",
            "Epoch 2384/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9207 - accuracy: 0.6216\n",
            "Epoch 02384: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9211 - accuracy: 0.6216 - val_loss: 2.0388 - val_accuracy: 0.2266\n",
            "Epoch 2385/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9114 - accuracy: 0.6285\n",
            "Epoch 02385: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6285 - val_loss: 2.0760 - val_accuracy: 0.2246\n",
            "Epoch 2386/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6263\n",
            "Epoch 02386: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9150 - accuracy: 0.6263 - val_loss: 2.0163 - val_accuracy: 0.2512\n",
            "Epoch 2387/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6277\n",
            "Epoch 02387: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9124 - accuracy: 0.6277 - val_loss: 2.0425 - val_accuracy: 0.2233\n",
            "Epoch 2388/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9232 - accuracy: 0.6222\n",
            "Epoch 02388: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9236 - accuracy: 0.6218 - val_loss: 2.0348 - val_accuracy: 0.2256\n",
            "Epoch 2389/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9089 - accuracy: 0.6284\n",
            "Epoch 02389: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9089 - accuracy: 0.6284 - val_loss: 2.0374 - val_accuracy: 0.2118\n",
            "Epoch 2390/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9218 - accuracy: 0.6215\n",
            "Epoch 02390: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9218 - accuracy: 0.6215 - val_loss: 1.9906 - val_accuracy: 0.2472\n",
            "Epoch 2391/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9244 - accuracy: 0.6237\n",
            "Epoch 02391: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9244 - accuracy: 0.6237 - val_loss: 1.9998 - val_accuracy: 0.2298\n",
            "Epoch 2392/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9185 - accuracy: 0.6219\n",
            "Epoch 02392: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9186 - accuracy: 0.6218 - val_loss: 1.9956 - val_accuracy: 0.2527\n",
            "Epoch 2393/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9078 - accuracy: 0.6285\n",
            "Epoch 02393: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9076 - accuracy: 0.6286 - val_loss: 2.0062 - val_accuracy: 0.2423\n",
            "Epoch 2394/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9066 - accuracy: 0.6295\n",
            "Epoch 02394: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9067 - accuracy: 0.6294 - val_loss: 1.9662 - val_accuracy: 0.2554\n",
            "Epoch 2395/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9066 - accuracy: 0.6312\n",
            "Epoch 02395: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9066 - accuracy: 0.6309 - val_loss: 2.0249 - val_accuracy: 0.2345\n",
            "Epoch 2396/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6248\n",
            "Epoch 02396: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9159 - accuracy: 0.6248 - val_loss: 1.9827 - val_accuracy: 0.2459\n",
            "Epoch 2397/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9134 - accuracy: 0.6259\n",
            "Epoch 02397: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9141 - accuracy: 0.6254 - val_loss: 1.9491 - val_accuracy: 0.2455\n",
            "Epoch 2398/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9086 - accuracy: 0.6286\n",
            "Epoch 02398: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9088 - accuracy: 0.6289 - val_loss: 2.0132 - val_accuracy: 0.2346\n",
            "Epoch 2399/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9156 - accuracy: 0.6240\n",
            "Epoch 02399: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9150 - accuracy: 0.6244 - val_loss: 2.0000 - val_accuracy: 0.2434\n",
            "Epoch 2400/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6250\n",
            "Epoch 02400: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9159 - accuracy: 0.6250 - val_loss: 2.0593 - val_accuracy: 0.2137\n",
            "Epoch 2401/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9149 - accuracy: 0.6253\n",
            "Epoch 02401: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9148 - accuracy: 0.6251 - val_loss: 2.0687 - val_accuracy: 0.2105\n",
            "Epoch 2402/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9063 - accuracy: 0.6308\n",
            "Epoch 02402: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9063 - accuracy: 0.6308 - val_loss: 2.0007 - val_accuracy: 0.2491\n",
            "Epoch 2403/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9171 - accuracy: 0.6262\n",
            "Epoch 02403: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9171 - accuracy: 0.6262 - val_loss: 2.0121 - val_accuracy: 0.2391\n",
            "Epoch 2404/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9097 - accuracy: 0.6291\n",
            "Epoch 02404: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9094 - accuracy: 0.6291 - val_loss: 2.0178 - val_accuracy: 0.2367\n",
            "Epoch 2405/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9153 - accuracy: 0.6260\n",
            "Epoch 02405: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9153 - accuracy: 0.6260 - val_loss: 2.0246 - val_accuracy: 0.2360\n",
            "Epoch 2406/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.6262\n",
            "Epoch 02406: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9138 - accuracy: 0.6262 - val_loss: 2.0059 - val_accuracy: 0.2355\n",
            "Epoch 2407/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9095 - accuracy: 0.6287\n",
            "Epoch 02407: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9097 - accuracy: 0.6286 - val_loss: 1.9864 - val_accuracy: 0.2358\n",
            "Epoch 2408/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9225 - accuracy: 0.6223\n",
            "Epoch 02408: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9229 - accuracy: 0.6220 - val_loss: 2.0664 - val_accuracy: 0.2123\n",
            "Epoch 2409/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9178 - accuracy: 0.6254\n",
            "Epoch 02409: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9178 - accuracy: 0.6254 - val_loss: 1.9828 - val_accuracy: 0.2405\n",
            "Epoch 2410/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9158 - accuracy: 0.6263\n",
            "Epoch 02410: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9160 - accuracy: 0.6262 - val_loss: 1.9646 - val_accuracy: 0.2490\n",
            "Epoch 2411/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9086 - accuracy: 0.6295\n",
            "Epoch 02411: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9086 - accuracy: 0.6295 - val_loss: 2.0659 - val_accuracy: 0.2238\n",
            "Epoch 2412/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.6191\n",
            "Epoch 02412: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9254 - accuracy: 0.6191 - val_loss: 2.0240 - val_accuracy: 0.2182\n",
            "Epoch 2413/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9123 - accuracy: 0.6272\n",
            "Epoch 02413: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9126 - accuracy: 0.6271 - val_loss: 1.9549 - val_accuracy: 0.2629\n",
            "Epoch 2414/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9228 - accuracy: 0.6214\n",
            "Epoch 02414: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9228 - accuracy: 0.6214 - val_loss: 2.1178 - val_accuracy: 0.2001\n",
            "Epoch 2415/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6261\n",
            "Epoch 02415: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9172 - accuracy: 0.6261 - val_loss: 2.0873 - val_accuracy: 0.2023\n",
            "Epoch 2416/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6260\n",
            "Epoch 02416: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9142 - accuracy: 0.6261 - val_loss: 2.0452 - val_accuracy: 0.2270\n",
            "Epoch 2417/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9034 - accuracy: 0.6316\n",
            "Epoch 02417: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9034 - accuracy: 0.6316 - val_loss: 2.0273 - val_accuracy: 0.2283\n",
            "Epoch 2418/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.6299\n",
            "Epoch 02418: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9097 - accuracy: 0.6299 - val_loss: 2.0217 - val_accuracy: 0.2312\n",
            "Epoch 2419/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9066 - accuracy: 0.6312\n",
            "Epoch 02419: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9066 - accuracy: 0.6312 - val_loss: 2.0275 - val_accuracy: 0.2286\n",
            "Epoch 2420/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9058 - accuracy: 0.6306\n",
            "Epoch 02420: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9060 - accuracy: 0.6304 - val_loss: 2.0994 - val_accuracy: 0.2029\n",
            "Epoch 2421/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9059 - accuracy: 0.6320\n",
            "Epoch 02421: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6320 - val_loss: 2.1859 - val_accuracy: 0.1762\n",
            "Epoch 2422/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9170 - accuracy: 0.6247\n",
            "Epoch 02422: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9172 - accuracy: 0.6245 - val_loss: 2.1176 - val_accuracy: 0.1992\n",
            "Epoch 2423/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9082 - accuracy: 0.6297\n",
            "Epoch 02423: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9082 - accuracy: 0.6297 - val_loss: 2.0660 - val_accuracy: 0.2171\n",
            "Epoch 2424/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9219 - accuracy: 0.6219\n",
            "Epoch 02424: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9219 - accuracy: 0.6219 - val_loss: 2.0781 - val_accuracy: 0.2148\n",
            "Epoch 2425/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9160 - accuracy: 0.6250\n",
            "Epoch 02425: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9161 - accuracy: 0.6250 - val_loss: 1.9966 - val_accuracy: 0.2413\n",
            "Epoch 2426/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9097 - accuracy: 0.6296\n",
            "Epoch 02426: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9105 - accuracy: 0.6293 - val_loss: 2.1198 - val_accuracy: 0.1941\n",
            "Epoch 2427/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9333 - accuracy: 0.6162\n",
            "Epoch 02427: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9333 - accuracy: 0.6162 - val_loss: 2.0747 - val_accuracy: 0.2190\n",
            "Epoch 2428/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9125 - accuracy: 0.6261\n",
            "Epoch 02428: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9120 - accuracy: 0.6264 - val_loss: 2.0445 - val_accuracy: 0.2180\n",
            "Epoch 2429/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9187 - accuracy: 0.6228\n",
            "Epoch 02429: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9187 - accuracy: 0.6228 - val_loss: 2.0429 - val_accuracy: 0.2208\n",
            "Epoch 2430/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9106 - accuracy: 0.6282\n",
            "Epoch 02430: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9110 - accuracy: 0.6279 - val_loss: 1.8562 - val_accuracy: 0.2985\n",
            "Epoch 2431/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9161 - accuracy: 0.6251\n",
            "Epoch 02431: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9156 - accuracy: 0.6253 - val_loss: 1.9998 - val_accuracy: 0.2431\n",
            "Epoch 2432/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6283\n",
            "Epoch 02432: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9121 - accuracy: 0.6283 - val_loss: 1.9889 - val_accuracy: 0.2426\n",
            "Epoch 2433/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.6259\n",
            "Epoch 02433: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6259 - val_loss: 2.0699 - val_accuracy: 0.2155\n",
            "Epoch 2434/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9043 - accuracy: 0.6308\n",
            "Epoch 02434: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9048 - accuracy: 0.6307 - val_loss: 1.9898 - val_accuracy: 0.2472\n",
            "Epoch 2435/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9180 - accuracy: 0.6243\n",
            "Epoch 02435: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9177 - accuracy: 0.6245 - val_loss: 2.1117 - val_accuracy: 0.2007\n",
            "Epoch 2436/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9103 - accuracy: 0.6278\n",
            "Epoch 02436: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9103 - accuracy: 0.6278 - val_loss: 2.0681 - val_accuracy: 0.2170\n",
            "Epoch 2437/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9122 - accuracy: 0.6272\n",
            "Epoch 02437: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9121 - accuracy: 0.6272 - val_loss: 2.0660 - val_accuracy: 0.2162\n",
            "Epoch 2438/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9126 - accuracy: 0.6267\n",
            "Epoch 02438: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9127 - accuracy: 0.6265 - val_loss: 2.0718 - val_accuracy: 0.2112\n",
            "Epoch 2439/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9110 - accuracy: 0.6278\n",
            "Epoch 02439: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9110 - accuracy: 0.6278 - val_loss: 2.0105 - val_accuracy: 0.2559\n",
            "Epoch 2440/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.6286\n",
            "Epoch 02440: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9081 - accuracy: 0.6286 - val_loss: 2.0484 - val_accuracy: 0.2225\n",
            "Epoch 2441/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9100 - accuracy: 0.6284\n",
            "Epoch 02441: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9096 - accuracy: 0.6286 - val_loss: 2.0078 - val_accuracy: 0.2461\n",
            "Epoch 2442/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9395 - accuracy: 0.6139\n",
            "Epoch 02442: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9396 - accuracy: 0.6140 - val_loss: 2.1052 - val_accuracy: 0.2105\n",
            "Epoch 2443/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9219 - accuracy: 0.6228\n",
            "Epoch 02443: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9219 - accuracy: 0.6228 - val_loss: 2.0905 - val_accuracy: 0.2073\n",
            "Epoch 2444/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9167 - accuracy: 0.6252\n",
            "Epoch 02444: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9168 - accuracy: 0.6250 - val_loss: 2.1165 - val_accuracy: 0.1847\n",
            "Epoch 2445/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6254\n",
            "Epoch 02445: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9174 - accuracy: 0.6254 - val_loss: 2.0796 - val_accuracy: 0.2182\n",
            "Epoch 2446/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9202 - accuracy: 0.6208\n",
            "Epoch 02446: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9202 - accuracy: 0.6208 - val_loss: 2.0676 - val_accuracy: 0.2190\n",
            "Epoch 2447/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9116 - accuracy: 0.6270\n",
            "Epoch 02447: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6270 - val_loss: 2.0974 - val_accuracy: 0.1945\n",
            "Epoch 2448/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.6264\n",
            "Epoch 02448: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9134 - accuracy: 0.6264 - val_loss: 2.0679 - val_accuracy: 0.2105\n",
            "Epoch 2449/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.6269\n",
            "Epoch 02449: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9097 - accuracy: 0.6271 - val_loss: 2.0168 - val_accuracy: 0.2231\n",
            "Epoch 2450/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.6279\n",
            "Epoch 02450: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9102 - accuracy: 0.6279 - val_loss: 1.9663 - val_accuracy: 0.2435\n",
            "Epoch 2451/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9223 - accuracy: 0.6221\n",
            "Epoch 02451: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9221 - accuracy: 0.6222 - val_loss: 2.1084 - val_accuracy: 0.1980\n",
            "Epoch 2452/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.6320\n",
            "Epoch 02452: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9048 - accuracy: 0.6320 - val_loss: 1.9634 - val_accuracy: 0.2686\n",
            "Epoch 2453/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9087 - accuracy: 0.6278\n",
            "Epoch 02453: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9084 - accuracy: 0.6279 - val_loss: 2.0220 - val_accuracy: 0.2288\n",
            "Epoch 2454/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9062 - accuracy: 0.6307\n",
            "Epoch 02454: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9057 - accuracy: 0.6311 - val_loss: 2.0807 - val_accuracy: 0.2075\n",
            "Epoch 2455/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9134 - accuracy: 0.6257\n",
            "Epoch 02455: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9142 - accuracy: 0.6256 - val_loss: 2.0411 - val_accuracy: 0.2241\n",
            "Epoch 2456/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6271\n",
            "Epoch 02456: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9124 - accuracy: 0.6271 - val_loss: 2.0757 - val_accuracy: 0.2125\n",
            "Epoch 2457/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9064 - accuracy: 0.6292\n",
            "Epoch 02457: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9064 - accuracy: 0.6292 - val_loss: 2.0330 - val_accuracy: 0.2195\n",
            "Epoch 2458/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9087 - accuracy: 0.6289\n",
            "Epoch 02458: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9082 - accuracy: 0.6292 - val_loss: 2.0735 - val_accuracy: 0.2127\n",
            "Epoch 2459/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9200 - accuracy: 0.6247\n",
            "Epoch 02459: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9198 - accuracy: 0.6246 - val_loss: 2.0075 - val_accuracy: 0.2397\n",
            "Epoch 2460/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9143 - accuracy: 0.6266\n",
            "Epoch 02460: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9143 - accuracy: 0.6266 - val_loss: 1.9834 - val_accuracy: 0.2383\n",
            "Epoch 2461/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9091 - accuracy: 0.6286\n",
            "Epoch 02461: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9091 - accuracy: 0.6286 - val_loss: 2.0538 - val_accuracy: 0.2275\n",
            "Epoch 2462/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9132 - accuracy: 0.6263\n",
            "Epoch 02462: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9137 - accuracy: 0.6261 - val_loss: 2.0152 - val_accuracy: 0.2361\n",
            "Epoch 2463/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9086 - accuracy: 0.6285\n",
            "Epoch 02463: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9091 - accuracy: 0.6281 - val_loss: 2.1422 - val_accuracy: 0.1851\n",
            "Epoch 2464/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9053 - accuracy: 0.6307\n",
            "Epoch 02464: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9055 - accuracy: 0.6306 - val_loss: 2.0440 - val_accuracy: 0.2303\n",
            "Epoch 2465/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9267 - accuracy: 0.6180\n",
            "Epoch 02465: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9267 - accuracy: 0.6180 - val_loss: 1.9862 - val_accuracy: 0.2562\n",
            "Epoch 2466/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9293 - accuracy: 0.6184\n",
            "Epoch 02466: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9288 - accuracy: 0.6188 - val_loss: 2.1169 - val_accuracy: 0.1973\n",
            "Epoch 2467/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.6287\n",
            "Epoch 02467: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9081 - accuracy: 0.6287 - val_loss: 2.0202 - val_accuracy: 0.2382\n",
            "Epoch 2468/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9128 - accuracy: 0.6271\n",
            "Epoch 02468: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9128 - accuracy: 0.6271 - val_loss: 2.0852 - val_accuracy: 0.1971\n",
            "Epoch 2469/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9215 - accuracy: 0.6235\n",
            "Epoch 02469: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9216 - accuracy: 0.6237 - val_loss: 2.0457 - val_accuracy: 0.2183\n",
            "Epoch 2470/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9225 - accuracy: 0.6207\n",
            "Epoch 02470: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9225 - accuracy: 0.6207 - val_loss: 1.8913 - val_accuracy: 0.2747\n",
            "Epoch 2471/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9181 - accuracy: 0.6246\n",
            "Epoch 02471: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9181 - accuracy: 0.6246 - val_loss: 2.0479 - val_accuracy: 0.2233\n",
            "Epoch 2472/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9083 - accuracy: 0.6306\n",
            "Epoch 02472: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9083 - accuracy: 0.6306 - val_loss: 2.0508 - val_accuracy: 0.2198\n",
            "Epoch 2473/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9097 - accuracy: 0.6290\n",
            "Epoch 02473: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9093 - accuracy: 0.6289 - val_loss: 2.0478 - val_accuracy: 0.2229\n",
            "Epoch 2474/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9155 - accuracy: 0.6252\n",
            "Epoch 02474: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9155 - accuracy: 0.6252 - val_loss: 2.0528 - val_accuracy: 0.2254\n",
            "Epoch 2475/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9032 - accuracy: 0.6332\n",
            "Epoch 02475: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9026 - accuracy: 0.6334 - val_loss: 2.1423 - val_accuracy: 0.1924\n",
            "Epoch 2476/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.6273\n",
            "Epoch 02476: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9129 - accuracy: 0.6273 - val_loss: 2.0575 - val_accuracy: 0.2193\n",
            "Epoch 2477/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9074 - accuracy: 0.6287\n",
            "Epoch 02477: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9074 - accuracy: 0.6287 - val_loss: 2.0065 - val_accuracy: 0.2348\n",
            "Epoch 2478/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9092 - accuracy: 0.6289\n",
            "Epoch 02478: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9095 - accuracy: 0.6290 - val_loss: 1.9360 - val_accuracy: 0.2584\n",
            "Epoch 2479/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9240 - accuracy: 0.6215\n",
            "Epoch 02479: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9237 - accuracy: 0.6217 - val_loss: 1.9772 - val_accuracy: 0.2424\n",
            "Epoch 2480/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9065 - accuracy: 0.6295\n",
            "Epoch 02480: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9065 - accuracy: 0.6295 - val_loss: 2.0469 - val_accuracy: 0.2277\n",
            "Epoch 2481/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9118 - accuracy: 0.6272\n",
            "Epoch 02481: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9113 - accuracy: 0.6273 - val_loss: 2.0917 - val_accuracy: 0.2157\n",
            "Epoch 2482/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9204 - accuracy: 0.6225\n",
            "Epoch 02482: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9205 - accuracy: 0.6225 - val_loss: 1.9744 - val_accuracy: 0.2322\n",
            "Epoch 2483/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9137 - accuracy: 0.6268\n",
            "Epoch 02483: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9137 - accuracy: 0.6268 - val_loss: 2.1297 - val_accuracy: 0.1978\n",
            "Epoch 2484/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9186 - accuracy: 0.6227\n",
            "Epoch 02484: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9186 - accuracy: 0.6227 - val_loss: 2.0700 - val_accuracy: 0.2273\n",
            "Epoch 2485/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9130 - accuracy: 0.6265\n",
            "Epoch 02485: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9130 - accuracy: 0.6265 - val_loss: 2.0981 - val_accuracy: 0.1955\n",
            "Epoch 2486/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6237\n",
            "Epoch 02486: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9150 - accuracy: 0.6237 - val_loss: 2.1190 - val_accuracy: 0.1894\n",
            "Epoch 2487/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9446 - accuracy: 0.6098\n",
            "Epoch 02487: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9443 - accuracy: 0.6101 - val_loss: 2.1151 - val_accuracy: 0.1963\n",
            "Epoch 2488/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9099 - accuracy: 0.6282\n",
            "Epoch 02488: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9103 - accuracy: 0.6280 - val_loss: 2.0941 - val_accuracy: 0.2086\n",
            "Epoch 2489/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6234\n",
            "Epoch 02489: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9172 - accuracy: 0.6234 - val_loss: 1.9815 - val_accuracy: 0.2363\n",
            "Epoch 2490/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9224 - accuracy: 0.6218\n",
            "Epoch 02490: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9224 - accuracy: 0.6219 - val_loss: 2.0837 - val_accuracy: 0.2003\n",
            "Epoch 2491/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9106 - accuracy: 0.6298\n",
            "Epoch 02491: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9106 - accuracy: 0.6298 - val_loss: 2.0390 - val_accuracy: 0.2196\n",
            "Epoch 2492/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9104 - accuracy: 0.6280\n",
            "Epoch 02492: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9104 - accuracy: 0.6280 - val_loss: 2.0723 - val_accuracy: 0.2157\n",
            "Epoch 2493/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9103 - accuracy: 0.6265\n",
            "Epoch 02493: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9105 - accuracy: 0.6266 - val_loss: 2.0915 - val_accuracy: 0.2162\n",
            "Epoch 2494/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9354 - accuracy: 0.6151\n",
            "Epoch 02494: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9354 - accuracy: 0.6151 - val_loss: 2.1611 - val_accuracy: 0.1820\n",
            "Epoch 2495/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9095 - accuracy: 0.6294\n",
            "Epoch 02495: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9095 - accuracy: 0.6294 - val_loss: 1.9796 - val_accuracy: 0.2407\n",
            "Epoch 2496/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.6270\n",
            "Epoch 02496: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9097 - accuracy: 0.6270 - val_loss: 2.0824 - val_accuracy: 0.2090\n",
            "Epoch 2497/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.6179\n",
            "Epoch 02497: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9270 - accuracy: 0.6182 - val_loss: 2.1691 - val_accuracy: 0.1797\n",
            "Epoch 2498/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9076 - accuracy: 0.6292\n",
            "Epoch 02498: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9076 - accuracy: 0.6292 - val_loss: 1.9704 - val_accuracy: 0.2706\n",
            "Epoch 2499/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6264\n",
            "Epoch 02499: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9124 - accuracy: 0.6264 - val_loss: 2.0793 - val_accuracy: 0.2118\n",
            "Epoch 2500/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9141 - accuracy: 0.6271\n",
            "Epoch 02500: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9136 - accuracy: 0.6274 - val_loss: 2.1589 - val_accuracy: 0.1879\n",
            "Epoch 2501/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9133 - accuracy: 0.6280\n",
            "Epoch 02501: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9133 - accuracy: 0.6280 - val_loss: 2.0524 - val_accuracy: 0.2214\n",
            "Epoch 2502/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8993 - accuracy: 0.6338\n",
            "Epoch 02502: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.8993 - accuracy: 0.6338 - val_loss: 2.0625 - val_accuracy: 0.2235\n",
            "Epoch 2503/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9119 - accuracy: 0.6291\n",
            "Epoch 02503: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9115 - accuracy: 0.6293 - val_loss: 2.0128 - val_accuracy: 0.2346\n",
            "Epoch 2504/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9055 - accuracy: 0.6309\n",
            "Epoch 02504: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6309 - val_loss: 2.1873 - val_accuracy: 0.1761\n",
            "Epoch 2505/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9087 - accuracy: 0.6287\n",
            "Epoch 02505: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9087 - accuracy: 0.6287 - val_loss: 2.0248 - val_accuracy: 0.2287\n",
            "Epoch 2506/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9186 - accuracy: 0.6228\n",
            "Epoch 02506: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9176 - accuracy: 0.6234 - val_loss: 2.1386 - val_accuracy: 0.1913\n",
            "Epoch 2507/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6291\n",
            "Epoch 02507: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9084 - accuracy: 0.6291 - val_loss: 1.9284 - val_accuracy: 0.2726\n",
            "Epoch 2508/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9057 - accuracy: 0.6319\n",
            "Epoch 02508: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9052 - accuracy: 0.6324 - val_loss: 2.0719 - val_accuracy: 0.2205\n",
            "Epoch 2509/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9119 - accuracy: 0.6264\n",
            "Epoch 02509: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9119 - accuracy: 0.6264 - val_loss: 2.0664 - val_accuracy: 0.2130\n",
            "Epoch 2510/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9103 - accuracy: 0.6279\n",
            "Epoch 02510: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9103 - accuracy: 0.6279 - val_loss: 2.1203 - val_accuracy: 0.1979\n",
            "Epoch 2511/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.6185\n",
            "Epoch 02511: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9312 - accuracy: 0.6185 - val_loss: 2.0522 - val_accuracy: 0.2033\n",
            "Epoch 2512/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9283 - accuracy: 0.6186\n",
            "Epoch 02512: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9288 - accuracy: 0.6184 - val_loss: 2.2028 - val_accuracy: 0.1815\n",
            "Epoch 2513/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9366 - accuracy: 0.6172\n",
            "Epoch 02513: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9366 - accuracy: 0.6172 - val_loss: 2.1499 - val_accuracy: 0.1933\n",
            "Epoch 2514/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9165 - accuracy: 0.6266\n",
            "Epoch 02514: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9161 - accuracy: 0.6267 - val_loss: 2.0520 - val_accuracy: 0.2211\n",
            "Epoch 2515/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9215 - accuracy: 0.6213\n",
            "Epoch 02515: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9215 - accuracy: 0.6213 - val_loss: 1.9989 - val_accuracy: 0.2318\n",
            "Epoch 2516/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6287\n",
            "Epoch 02516: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9124 - accuracy: 0.6287 - val_loss: 1.9908 - val_accuracy: 0.2423\n",
            "Epoch 2517/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9153 - accuracy: 0.6251\n",
            "Epoch 02517: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9153 - accuracy: 0.6251 - val_loss: 2.0384 - val_accuracy: 0.2313\n",
            "Epoch 2518/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6276\n",
            "Epoch 02518: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9125 - accuracy: 0.6276 - val_loss: 1.9991 - val_accuracy: 0.2530\n",
            "Epoch 2519/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9100 - accuracy: 0.6278\n",
            "Epoch 02519: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9100 - accuracy: 0.6278 - val_loss: 2.0690 - val_accuracy: 0.2248\n",
            "Epoch 2520/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9253 - accuracy: 0.6199\n",
            "Epoch 02520: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 43ms/step - loss: 0.9253 - accuracy: 0.6199 - val_loss: 1.9782 - val_accuracy: 0.2389\n",
            "Epoch 2521/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9204 - accuracy: 0.6231\n",
            "Epoch 02521: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9204 - accuracy: 0.6231 - val_loss: 2.1114 - val_accuracy: 0.1929\n",
            "Epoch 2522/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9075 - accuracy: 0.6302\n",
            "Epoch 02522: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9072 - accuracy: 0.6303 - val_loss: 2.0518 - val_accuracy: 0.2239\n",
            "Epoch 2523/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9109 - accuracy: 0.6277\n",
            "Epoch 02523: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9110 - accuracy: 0.6276 - val_loss: 1.9963 - val_accuracy: 0.2326\n",
            "Epoch 2524/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.6277\n",
            "Epoch 02524: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9103 - accuracy: 0.6278 - val_loss: 2.0776 - val_accuracy: 0.2214\n",
            "Epoch 2525/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9098 - accuracy: 0.6283\n",
            "Epoch 02525: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9091 - accuracy: 0.6285 - val_loss: 2.1200 - val_accuracy: 0.1936\n",
            "Epoch 2526/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9083 - accuracy: 0.6289\n",
            "Epoch 02526: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9086 - accuracy: 0.6289 - val_loss: 2.0833 - val_accuracy: 0.2120\n",
            "Epoch 2527/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9095 - accuracy: 0.6280\n",
            "Epoch 02527: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9095 - accuracy: 0.6280 - val_loss: 1.9978 - val_accuracy: 0.2320\n",
            "Epoch 2528/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9114 - accuracy: 0.6292\n",
            "Epoch 02528: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9114 - accuracy: 0.6292 - val_loss: 2.0521 - val_accuracy: 0.2185\n",
            "Epoch 2529/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9023 - accuracy: 0.6330\n",
            "Epoch 02529: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9026 - accuracy: 0.6329 - val_loss: 1.9816 - val_accuracy: 0.2464\n",
            "Epoch 2530/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9178 - accuracy: 0.6231\n",
            "Epoch 02530: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9178 - accuracy: 0.6231 - val_loss: 1.9125 - val_accuracy: 0.2669\n",
            "Epoch 2531/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9197 - accuracy: 0.6231\n",
            "Epoch 02531: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9200 - accuracy: 0.6230 - val_loss: 1.9755 - val_accuracy: 0.2552\n",
            "Epoch 2532/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9077 - accuracy: 0.6301\n",
            "Epoch 02532: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9084 - accuracy: 0.6298 - val_loss: 1.9716 - val_accuracy: 0.2456\n",
            "Epoch 2533/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9209 - accuracy: 0.6225\n",
            "Epoch 02533: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9209 - accuracy: 0.6225 - val_loss: 2.0126 - val_accuracy: 0.2363\n",
            "Epoch 2534/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6297\n",
            "Epoch 02534: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9084 - accuracy: 0.6297 - val_loss: 1.9744 - val_accuracy: 0.2531\n",
            "Epoch 2535/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9514 - accuracy: 0.6054\n",
            "Epoch 02535: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9514 - accuracy: 0.6054 - val_loss: 1.9848 - val_accuracy: 0.2425\n",
            "Epoch 2536/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9090 - accuracy: 0.6271\n",
            "Epoch 02536: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9090 - accuracy: 0.6269 - val_loss: 2.0500 - val_accuracy: 0.2260\n",
            "Epoch 2537/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9101 - accuracy: 0.6291\n",
            "Epoch 02537: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9101 - accuracy: 0.6291 - val_loss: 1.9815 - val_accuracy: 0.2506\n",
            "Epoch 2538/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9090 - accuracy: 0.6294\n",
            "Epoch 02538: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9090 - accuracy: 0.6294 - val_loss: 2.0489 - val_accuracy: 0.2246\n",
            "Epoch 2539/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9062 - accuracy: 0.6309\n",
            "Epoch 02539: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6310 - val_loss: 2.1072 - val_accuracy: 0.1985\n",
            "Epoch 2540/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9058 - accuracy: 0.6290\n",
            "Epoch 02540: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9058 - accuracy: 0.6290 - val_loss: 2.0799 - val_accuracy: 0.2108\n",
            "Epoch 2541/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9305 - accuracy: 0.6176\n",
            "Epoch 02541: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9305 - accuracy: 0.6176 - val_loss: 2.0217 - val_accuracy: 0.2218\n",
            "Epoch 2542/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9228 - accuracy: 0.6205\n",
            "Epoch 02542: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9227 - accuracy: 0.6206 - val_loss: 2.0364 - val_accuracy: 0.2296\n",
            "Epoch 2543/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9077 - accuracy: 0.6299\n",
            "Epoch 02543: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9077 - accuracy: 0.6299 - val_loss: 2.0439 - val_accuracy: 0.2217\n",
            "Epoch 2544/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9069 - accuracy: 0.6290\n",
            "Epoch 02544: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9069 - accuracy: 0.6290 - val_loss: 2.0274 - val_accuracy: 0.2227\n",
            "Epoch 2545/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9116 - accuracy: 0.6286\n",
            "Epoch 02545: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6286 - val_loss: 2.0130 - val_accuracy: 0.2389\n",
            "Epoch 2546/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9135 - accuracy: 0.6271\n",
            "Epoch 02546: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9141 - accuracy: 0.6268 - val_loss: 2.0792 - val_accuracy: 0.1995\n",
            "Epoch 2547/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.6219\n",
            "Epoch 02547: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9246 - accuracy: 0.6222 - val_loss: 2.0027 - val_accuracy: 0.2426\n",
            "Epoch 2548/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9103 - accuracy: 0.6281\n",
            "Epoch 02548: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9101 - accuracy: 0.6284 - val_loss: 2.1224 - val_accuracy: 0.2012\n",
            "Epoch 2549/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6281\n",
            "Epoch 02549: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9108 - accuracy: 0.6281 - val_loss: 2.0078 - val_accuracy: 0.2410\n",
            "Epoch 2550/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9054 - accuracy: 0.6297\n",
            "Epoch 02550: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9054 - accuracy: 0.6297 - val_loss: 2.0459 - val_accuracy: 0.2227\n",
            "Epoch 2551/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9028 - accuracy: 0.6312\n",
            "Epoch 02551: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9032 - accuracy: 0.6309 - val_loss: 2.0827 - val_accuracy: 0.2133\n",
            "Epoch 2552/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9164 - accuracy: 0.6259\n",
            "Epoch 02552: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9164 - accuracy: 0.6259 - val_loss: 2.0309 - val_accuracy: 0.2292\n",
            "Epoch 2553/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9289 - accuracy: 0.6194\n",
            "Epoch 02553: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9289 - accuracy: 0.6194 - val_loss: 2.0840 - val_accuracy: 0.2054\n",
            "Epoch 2554/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9032 - accuracy: 0.6321\n",
            "Epoch 02554: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9028 - accuracy: 0.6323 - val_loss: 2.0070 - val_accuracy: 0.2281\n",
            "Epoch 2555/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9173 - accuracy: 0.6240\n",
            "Epoch 02555: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9173 - accuracy: 0.6239 - val_loss: 2.0120 - val_accuracy: 0.2498\n",
            "Epoch 2556/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.6273\n",
            "Epoch 02556: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9151 - accuracy: 0.6273 - val_loss: 2.0954 - val_accuracy: 0.2092\n",
            "Epoch 2557/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9154 - accuracy: 0.6250\n",
            "Epoch 02557: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9154 - accuracy: 0.6250 - val_loss: 2.0888 - val_accuracy: 0.2061\n",
            "Epoch 2558/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9239 - accuracy: 0.6205\n",
            "Epoch 02558: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9234 - accuracy: 0.6209 - val_loss: 2.0514 - val_accuracy: 0.2220\n",
            "Epoch 2559/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6236\n",
            "Epoch 02559: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9174 - accuracy: 0.6236 - val_loss: 2.1219 - val_accuracy: 0.2050\n",
            "Epoch 2560/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9058 - accuracy: 0.6316\n",
            "Epoch 02560: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9055 - accuracy: 0.6317 - val_loss: 2.0036 - val_accuracy: 0.2394\n",
            "Epoch 2561/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9074 - accuracy: 0.6290\n",
            "Epoch 02561: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9074 - accuracy: 0.6290 - val_loss: 2.0866 - val_accuracy: 0.2093\n",
            "Epoch 2562/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.6301\n",
            "Epoch 02562: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9081 - accuracy: 0.6301 - val_loss: 1.9706 - val_accuracy: 0.2448\n",
            "Epoch 2563/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9186 - accuracy: 0.6238\n",
            "Epoch 02563: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9181 - accuracy: 0.6242 - val_loss: 2.0701 - val_accuracy: 0.2144\n",
            "Epoch 2564/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9150 - accuracy: 0.6269\n",
            "Epoch 02564: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9153 - accuracy: 0.6266 - val_loss: 2.0832 - val_accuracy: 0.2083\n",
            "Epoch 2565/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9103 - accuracy: 0.6288\n",
            "Epoch 02565: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9103 - accuracy: 0.6288 - val_loss: 2.0490 - val_accuracy: 0.2238\n",
            "Epoch 2566/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9111 - accuracy: 0.6291\n",
            "Epoch 02566: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9110 - accuracy: 0.6290 - val_loss: 2.1993 - val_accuracy: 0.1723\n",
            "Epoch 2567/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6261\n",
            "Epoch 02567: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9130 - accuracy: 0.6256 - val_loss: 2.0920 - val_accuracy: 0.2056\n",
            "Epoch 2568/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9087 - accuracy: 0.6267\n",
            "Epoch 02568: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9085 - accuracy: 0.6266 - val_loss: 2.0197 - val_accuracy: 0.2393\n",
            "Epoch 2569/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6265\n",
            "Epoch 02569: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9140 - accuracy: 0.6265 - val_loss: 2.0718 - val_accuracy: 0.2134\n",
            "Epoch 2570/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9038 - accuracy: 0.6316\n",
            "Epoch 02570: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9038 - accuracy: 0.6316 - val_loss: 1.9947 - val_accuracy: 0.2448\n",
            "Epoch 2571/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9199 - accuracy: 0.6228\n",
            "Epoch 02571: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9204 - accuracy: 0.6226 - val_loss: 2.0834 - val_accuracy: 0.2060\n",
            "Epoch 2572/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9085 - accuracy: 0.6307\n",
            "Epoch 02572: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9082 - accuracy: 0.6308 - val_loss: 2.0717 - val_accuracy: 0.2049\n",
            "Epoch 2573/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9192 - accuracy: 0.6240\n",
            "Epoch 02573: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9192 - accuracy: 0.6240 - val_loss: 1.9943 - val_accuracy: 0.2513\n",
            "Epoch 2574/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.6286\n",
            "Epoch 02574: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9111 - accuracy: 0.6286 - val_loss: 2.0752 - val_accuracy: 0.2086\n",
            "Epoch 2575/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9118 - accuracy: 0.6276\n",
            "Epoch 02575: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9118 - accuracy: 0.6276 - val_loss: 2.0117 - val_accuracy: 0.2346\n",
            "Epoch 2576/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9191 - accuracy: 0.6252\n",
            "Epoch 02576: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9191 - accuracy: 0.6252 - val_loss: 2.0944 - val_accuracy: 0.2062\n",
            "Epoch 2577/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.6291\n",
            "Epoch 02577: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9111 - accuracy: 0.6288 - val_loss: 2.1661 - val_accuracy: 0.1723\n",
            "Epoch 2578/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9093 - accuracy: 0.6284\n",
            "Epoch 02578: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9093 - accuracy: 0.6284 - val_loss: 1.9990 - val_accuracy: 0.2405\n",
            "Epoch 2579/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9080 - accuracy: 0.6285\n",
            "Epoch 02579: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9083 - accuracy: 0.6284 - val_loss: 2.0004 - val_accuracy: 0.2445\n",
            "Epoch 2580/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9176 - accuracy: 0.6242\n",
            "Epoch 02580: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9177 - accuracy: 0.6243 - val_loss: 2.0807 - val_accuracy: 0.2071\n",
            "Epoch 2581/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9148 - accuracy: 0.6257\n",
            "Epoch 02581: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9148 - accuracy: 0.6257 - val_loss: 1.9799 - val_accuracy: 0.2412\n",
            "Epoch 2582/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9222 - accuracy: 0.6219\n",
            "Epoch 02582: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9222 - accuracy: 0.6219 - val_loss: 2.0248 - val_accuracy: 0.2308\n",
            "Epoch 2583/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9128 - accuracy: 0.6267\n",
            "Epoch 02583: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9127 - accuracy: 0.6268 - val_loss: 2.0538 - val_accuracy: 0.2218\n",
            "Epoch 2584/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9039 - accuracy: 0.6324\n",
            "Epoch 02584: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9043 - accuracy: 0.6323 - val_loss: 1.9599 - val_accuracy: 0.2550\n",
            "Epoch 2585/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.6312\n",
            "Epoch 02585: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9055 - accuracy: 0.6312 - val_loss: 2.0554 - val_accuracy: 0.2234\n",
            "Epoch 2586/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9123 - accuracy: 0.6258\n",
            "Epoch 02586: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9130 - accuracy: 0.6255 - val_loss: 1.8564 - val_accuracy: 0.2825\n",
            "Epoch 2587/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.6262\n",
            "Epoch 02587: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9151 - accuracy: 0.6262 - val_loss: 2.1392 - val_accuracy: 0.1910\n",
            "Epoch 2588/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9116 - accuracy: 0.6273\n",
            "Epoch 02588: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6273 - val_loss: 2.0671 - val_accuracy: 0.2131\n",
            "Epoch 2589/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9036 - accuracy: 0.6302\n",
            "Epoch 02589: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9036 - accuracy: 0.6302 - val_loss: 1.9718 - val_accuracy: 0.2466\n",
            "Epoch 2590/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6238\n",
            "Epoch 02590: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9172 - accuracy: 0.6238 - val_loss: 2.0474 - val_accuracy: 0.2277\n",
            "Epoch 2591/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8996 - accuracy: 0.6328\n",
            "Epoch 02591: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.8996 - accuracy: 0.6328 - val_loss: 2.0955 - val_accuracy: 0.1997\n",
            "Epoch 2592/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9190 - accuracy: 0.6237\n",
            "Epoch 02592: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9192 - accuracy: 0.6236 - val_loss: 2.1342 - val_accuracy: 0.1940\n",
            "Epoch 2593/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9038 - accuracy: 0.6330\n",
            "Epoch 02593: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9034 - accuracy: 0.6329 - val_loss: 2.0792 - val_accuracy: 0.2088\n",
            "Epoch 2594/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.6305\n",
            "Epoch 02594: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9048 - accuracy: 0.6305 - val_loss: 2.0102 - val_accuracy: 0.2271\n",
            "Epoch 2595/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9266 - accuracy: 0.6199\n",
            "Epoch 02595: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9275 - accuracy: 0.6196 - val_loss: 2.0383 - val_accuracy: 0.2366\n",
            "Epoch 2596/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9200 - accuracy: 0.6239\n",
            "Epoch 02596: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9200 - accuracy: 0.6239 - val_loss: 2.0245 - val_accuracy: 0.2391\n",
            "Epoch 2597/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9113 - accuracy: 0.6280\n",
            "Epoch 02597: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6279 - val_loss: 2.0141 - val_accuracy: 0.2248\n",
            "Epoch 2598/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9066 - accuracy: 0.6285\n",
            "Epoch 02598: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9066 - accuracy: 0.6285 - val_loss: 2.0663 - val_accuracy: 0.2097\n",
            "Epoch 2599/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9051 - accuracy: 0.6311\n",
            "Epoch 02599: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9057 - accuracy: 0.6310 - val_loss: 2.0201 - val_accuracy: 0.2273\n",
            "Epoch 2600/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9136 - accuracy: 0.6258\n",
            "Epoch 02600: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9131 - accuracy: 0.6259 - val_loss: 2.0846 - val_accuracy: 0.2193\n",
            "Epoch 2601/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9236 - accuracy: 0.6203\n",
            "Epoch 02601: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9236 - accuracy: 0.6203 - val_loss: 2.0446 - val_accuracy: 0.2264\n",
            "Epoch 2602/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9132 - accuracy: 0.6267\n",
            "Epoch 02602: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9132 - accuracy: 0.6267 - val_loss: 2.0509 - val_accuracy: 0.2181\n",
            "Epoch 2603/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9110 - accuracy: 0.6270\n",
            "Epoch 02603: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9110 - accuracy: 0.6270 - val_loss: 2.0462 - val_accuracy: 0.2268\n",
            "Epoch 2604/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9029 - accuracy: 0.6323\n",
            "Epoch 02604: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9029 - accuracy: 0.6323 - val_loss: 2.0260 - val_accuracy: 0.2289\n",
            "Epoch 2605/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9123 - accuracy: 0.6246\n",
            "Epoch 02605: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9128 - accuracy: 0.6244 - val_loss: 2.2038 - val_accuracy: 0.1619\n",
            "Epoch 2606/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9180 - accuracy: 0.6248\n",
            "Epoch 02606: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9176 - accuracy: 0.6251 - val_loss: 2.0514 - val_accuracy: 0.2255\n",
            "Epoch 2607/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9199 - accuracy: 0.6229\n",
            "Epoch 02607: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9199 - accuracy: 0.6229 - val_loss: 2.0282 - val_accuracy: 0.2170\n",
            "Epoch 2608/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9070 - accuracy: 0.6311\n",
            "Epoch 02608: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9070 - accuracy: 0.6311 - val_loss: 2.0732 - val_accuracy: 0.2168\n",
            "Epoch 2609/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9083 - accuracy: 0.6285\n",
            "Epoch 02609: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9080 - accuracy: 0.6285 - val_loss: 2.0657 - val_accuracy: 0.2124\n",
            "Epoch 2610/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9062 - accuracy: 0.6301\n",
            "Epoch 02610: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9062 - accuracy: 0.6301 - val_loss: 2.0691 - val_accuracy: 0.2183\n",
            "Epoch 2611/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9093 - accuracy: 0.6282\n",
            "Epoch 02611: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9091 - accuracy: 0.6282 - val_loss: 2.1017 - val_accuracy: 0.1984\n",
            "Epoch 2612/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9089 - accuracy: 0.6290\n",
            "Epoch 02612: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9089 - accuracy: 0.6290 - val_loss: 2.0455 - val_accuracy: 0.2131\n",
            "Epoch 2613/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9094 - accuracy: 0.6280\n",
            "Epoch 02613: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9096 - accuracy: 0.6279 - val_loss: 2.0294 - val_accuracy: 0.2335\n",
            "Epoch 2614/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6314\n",
            "Epoch 02614: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9017 - accuracy: 0.6314 - val_loss: 1.9903 - val_accuracy: 0.2459\n",
            "Epoch 2615/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9098 - accuracy: 0.6275\n",
            "Epoch 02615: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9097 - accuracy: 0.6275 - val_loss: 2.0237 - val_accuracy: 0.2320\n",
            "Epoch 2616/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9085 - accuracy: 0.6301\n",
            "Epoch 02616: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9085 - accuracy: 0.6301 - val_loss: 1.9890 - val_accuracy: 0.2471\n",
            "Epoch 2617/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9153 - accuracy: 0.6253\n",
            "Epoch 02617: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9151 - accuracy: 0.6255 - val_loss: 1.9746 - val_accuracy: 0.2499\n",
            "Epoch 2618/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9066 - accuracy: 0.6310\n",
            "Epoch 02618: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9066 - accuracy: 0.6310 - val_loss: 1.9958 - val_accuracy: 0.2430\n",
            "Epoch 2619/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.6213\n",
            "Epoch 02619: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9233 - accuracy: 0.6213 - val_loss: 2.0844 - val_accuracy: 0.1983\n",
            "Epoch 2620/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9245 - accuracy: 0.6214\n",
            "Epoch 02620: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9238 - accuracy: 0.6216 - val_loss: 2.0840 - val_accuracy: 0.2086\n",
            "Epoch 2621/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9137 - accuracy: 0.6257\n",
            "Epoch 02621: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9137 - accuracy: 0.6258 - val_loss: 2.1159 - val_accuracy: 0.1962\n",
            "Epoch 2622/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9094 - accuracy: 0.6297\n",
            "Epoch 02622: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9094 - accuracy: 0.6297 - val_loss: 2.0636 - val_accuracy: 0.2330\n",
            "Epoch 2623/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9130 - accuracy: 0.6268\n",
            "Epoch 02623: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9128 - accuracy: 0.6268 - val_loss: 2.0678 - val_accuracy: 0.2227\n",
            "Epoch 2624/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9074 - accuracy: 0.6299\n",
            "Epoch 02624: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9071 - accuracy: 0.6299 - val_loss: 1.9908 - val_accuracy: 0.2443\n",
            "Epoch 2625/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9051 - accuracy: 0.6313\n",
            "Epoch 02625: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9054 - accuracy: 0.6311 - val_loss: 2.0176 - val_accuracy: 0.2351\n",
            "Epoch 2626/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.6257\n",
            "Epoch 02626: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9134 - accuracy: 0.6257 - val_loss: 2.1583 - val_accuracy: 0.1854\n",
            "Epoch 2627/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9172 - accuracy: 0.6259\n",
            "Epoch 02627: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9172 - accuracy: 0.6259 - val_loss: 2.1121 - val_accuracy: 0.1937\n",
            "Epoch 2628/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9072 - accuracy: 0.6289\n",
            "Epoch 02628: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9072 - accuracy: 0.6289 - val_loss: 2.0289 - val_accuracy: 0.2295\n",
            "Epoch 2629/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9115 - accuracy: 0.6277\n",
            "Epoch 02629: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9105 - accuracy: 0.6282 - val_loss: 2.0286 - val_accuracy: 0.2195\n",
            "Epoch 2630/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9249 - accuracy: 0.6197\n",
            "Epoch 02630: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9249 - accuracy: 0.6197 - val_loss: 1.9425 - val_accuracy: 0.2551\n",
            "Epoch 2631/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9090 - accuracy: 0.6304\n",
            "Epoch 02631: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9090 - accuracy: 0.6304 - val_loss: 2.0512 - val_accuracy: 0.2182\n",
            "Epoch 2632/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9137 - accuracy: 0.6249\n",
            "Epoch 02632: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9137 - accuracy: 0.6249 - val_loss: 2.0089 - val_accuracy: 0.2246\n",
            "Epoch 2633/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.6290\n",
            "Epoch 02633: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9102 - accuracy: 0.6290 - val_loss: 2.0348 - val_accuracy: 0.2140\n",
            "Epoch 2634/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9163 - accuracy: 0.6242\n",
            "Epoch 02634: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9163 - accuracy: 0.6242 - val_loss: 2.1305 - val_accuracy: 0.1820\n",
            "Epoch 2635/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9086 - accuracy: 0.6281\n",
            "Epoch 02635: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9094 - accuracy: 0.6279 - val_loss: 2.0720 - val_accuracy: 0.2136\n",
            "Epoch 2636/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9002 - accuracy: 0.6341\n",
            "Epoch 02636: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9000 - accuracy: 0.6343 - val_loss: 2.1089 - val_accuracy: 0.1958\n",
            "Epoch 2637/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9077 - accuracy: 0.6307\n",
            "Epoch 02637: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9079 - accuracy: 0.6305 - val_loss: 1.9988 - val_accuracy: 0.2339\n",
            "Epoch 2638/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9090 - accuracy: 0.6294\n",
            "Epoch 02638: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9090 - accuracy: 0.6294 - val_loss: 2.0111 - val_accuracy: 0.2362\n",
            "Epoch 2639/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9076 - accuracy: 0.6292\n",
            "Epoch 02639: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9078 - accuracy: 0.6292 - val_loss: 2.0955 - val_accuracy: 0.2137\n",
            "Epoch 2640/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.6250\n",
            "Epoch 02640: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9161 - accuracy: 0.6250 - val_loss: 1.9327 - val_accuracy: 0.2468\n",
            "Epoch 2641/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6283\n",
            "Epoch 02641: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9121 - accuracy: 0.6283 - val_loss: 2.0264 - val_accuracy: 0.2273\n",
            "Epoch 2642/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9069 - accuracy: 0.6302\n",
            "Epoch 02642: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9069 - accuracy: 0.6302 - val_loss: 2.0328 - val_accuracy: 0.2427\n",
            "Epoch 2643/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9173 - accuracy: 0.6232\n",
            "Epoch 02643: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9173 - accuracy: 0.6232 - val_loss: 2.1017 - val_accuracy: 0.1949\n",
            "Epoch 2644/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9019 - accuracy: 0.6327\n",
            "Epoch 02644: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9017 - accuracy: 0.6329 - val_loss: 2.0675 - val_accuracy: 0.2092\n",
            "Epoch 2645/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9085 - accuracy: 0.6302\n",
            "Epoch 02645: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9079 - accuracy: 0.6304 - val_loss: 2.0526 - val_accuracy: 0.2205\n",
            "Epoch 2646/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9071 - accuracy: 0.6298\n",
            "Epoch 02646: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9071 - accuracy: 0.6298 - val_loss: 2.2011 - val_accuracy: 0.1674\n",
            "Epoch 2647/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9071 - accuracy: 0.6297\n",
            "Epoch 02647: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9069 - accuracy: 0.6297 - val_loss: 2.0613 - val_accuracy: 0.2183\n",
            "Epoch 2648/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9087 - accuracy: 0.6284\n",
            "Epoch 02648: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9094 - accuracy: 0.6280 - val_loss: 1.9800 - val_accuracy: 0.2399\n",
            "Epoch 2649/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9109 - accuracy: 0.6274\n",
            "Epoch 02649: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9109 - accuracy: 0.6274 - val_loss: 2.0644 - val_accuracy: 0.2101\n",
            "Epoch 2650/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9098 - accuracy: 0.6300\n",
            "Epoch 02650: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9098 - accuracy: 0.6300 - val_loss: 1.9775 - val_accuracy: 0.2445\n",
            "Epoch 2651/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6261\n",
            "Epoch 02651: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9172 - accuracy: 0.6261 - val_loss: 2.0385 - val_accuracy: 0.2401\n",
            "Epoch 2652/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9144 - accuracy: 0.6263\n",
            "Epoch 02652: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9144 - accuracy: 0.6263 - val_loss: 2.0942 - val_accuracy: 0.2043\n",
            "Epoch 2653/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9192 - accuracy: 0.6234\n",
            "Epoch 02653: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9192 - accuracy: 0.6234 - val_loss: 2.1213 - val_accuracy: 0.1970\n",
            "Epoch 2654/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9075 - accuracy: 0.6289\n",
            "Epoch 02654: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9080 - accuracy: 0.6287 - val_loss: 1.9449 - val_accuracy: 0.2544\n",
            "Epoch 2655/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9355 - accuracy: 0.6148\n",
            "Epoch 02655: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9355 - accuracy: 0.6148 - val_loss: 2.2420 - val_accuracy: 0.1616\n",
            "Epoch 2656/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9156 - accuracy: 0.6244\n",
            "Epoch 02656: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9152 - accuracy: 0.6246 - val_loss: 2.0525 - val_accuracy: 0.2135\n",
            "Epoch 2657/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9086 - accuracy: 0.6294\n",
            "Epoch 02657: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9086 - accuracy: 0.6294 - val_loss: 2.0849 - val_accuracy: 0.2065\n",
            "Epoch 2658/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9359 - accuracy: 0.6176\n",
            "Epoch 02658: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9359 - accuracy: 0.6176 - val_loss: 2.0491 - val_accuracy: 0.2180\n",
            "Epoch 2659/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6265\n",
            "Epoch 02659: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9150 - accuracy: 0.6265 - val_loss: 2.0156 - val_accuracy: 0.2316\n",
            "Epoch 2660/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9077 - accuracy: 0.6312\n",
            "Epoch 02660: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9077 - accuracy: 0.6312 - val_loss: 2.0337 - val_accuracy: 0.2281\n",
            "Epoch 2661/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9032 - accuracy: 0.6323\n",
            "Epoch 02661: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9032 - accuracy: 0.6323 - val_loss: 2.0415 - val_accuracy: 0.2235\n",
            "Epoch 2662/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9097 - accuracy: 0.6279\n",
            "Epoch 02662: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9092 - accuracy: 0.6280 - val_loss: 1.9889 - val_accuracy: 0.2477\n",
            "Epoch 2663/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6281\n",
            "Epoch 02663: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9108 - accuracy: 0.6281 - val_loss: 2.0646 - val_accuracy: 0.2163\n",
            "Epoch 2664/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9053 - accuracy: 0.6299\n",
            "Epoch 02664: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9053 - accuracy: 0.6299 - val_loss: 2.0775 - val_accuracy: 0.2128\n",
            "Epoch 2665/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9079 - accuracy: 0.6309\n",
            "Epoch 02665: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9079 - accuracy: 0.6309 - val_loss: 2.0683 - val_accuracy: 0.2159\n",
            "Epoch 2666/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9029 - accuracy: 0.6326\n",
            "Epoch 02666: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9029 - accuracy: 0.6326 - val_loss: 2.0928 - val_accuracy: 0.2061\n",
            "Epoch 2667/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9099 - accuracy: 0.6295\n",
            "Epoch 02667: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9099 - accuracy: 0.6295 - val_loss: 2.0338 - val_accuracy: 0.2309\n",
            "Epoch 2668/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9141 - accuracy: 0.6259\n",
            "Epoch 02668: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9141 - accuracy: 0.6259 - val_loss: 2.0605 - val_accuracy: 0.2117\n",
            "Epoch 2669/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9060 - accuracy: 0.6312\n",
            "Epoch 02669: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9060 - accuracy: 0.6312 - val_loss: 2.0452 - val_accuracy: 0.2085\n",
            "Epoch 2670/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9046 - accuracy: 0.6309\n",
            "Epoch 02670: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9044 - accuracy: 0.6307 - val_loss: 2.0933 - val_accuracy: 0.2030\n",
            "Epoch 2671/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9201 - accuracy: 0.6216\n",
            "Epoch 02671: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9201 - accuracy: 0.6216 - val_loss: 1.9591 - val_accuracy: 0.2400\n",
            "Epoch 2672/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9238 - accuracy: 0.6176\n",
            "Epoch 02672: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9238 - accuracy: 0.6176 - val_loss: 2.0963 - val_accuracy: 0.2025\n",
            "Epoch 2673/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9165 - accuracy: 0.6238\n",
            "Epoch 02673: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9165 - accuracy: 0.6238 - val_loss: 2.0547 - val_accuracy: 0.2206\n",
            "Epoch 2674/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6306\n",
            "Epoch 02674: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9084 - accuracy: 0.6306 - val_loss: 2.0092 - val_accuracy: 0.2397\n",
            "Epoch 2675/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.6296\n",
            "Epoch 02675: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9055 - accuracy: 0.6296 - val_loss: 2.0520 - val_accuracy: 0.2144\n",
            "Epoch 2676/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8996 - accuracy: 0.6350\n",
            "Epoch 02676: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8993 - accuracy: 0.6350 - val_loss: 2.0394 - val_accuracy: 0.2078\n",
            "Epoch 2677/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9126 - accuracy: 0.6269\n",
            "Epoch 02677: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9133 - accuracy: 0.6267 - val_loss: 2.0752 - val_accuracy: 0.2174\n",
            "Epoch 2678/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9024 - accuracy: 0.6337\n",
            "Epoch 02678: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9023 - accuracy: 0.6338 - val_loss: 2.0719 - val_accuracy: 0.2063\n",
            "Epoch 2679/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9108 - accuracy: 0.6263\n",
            "Epoch 02679: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9112 - accuracy: 0.6261 - val_loss: 2.0666 - val_accuracy: 0.2019\n",
            "Epoch 2680/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9049 - accuracy: 0.6315\n",
            "Epoch 02680: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9056 - accuracy: 0.6311 - val_loss: 2.0150 - val_accuracy: 0.2359\n",
            "Epoch 2681/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9564 - accuracy: 0.6034\n",
            "Epoch 02681: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9564 - accuracy: 0.6034 - val_loss: 2.1185 - val_accuracy: 0.1923\n",
            "Epoch 2682/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9311 - accuracy: 0.6186\n",
            "Epoch 02682: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9311 - accuracy: 0.6186 - val_loss: 2.0734 - val_accuracy: 0.2185\n",
            "Epoch 2683/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9192 - accuracy: 0.6262\n",
            "Epoch 02683: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9189 - accuracy: 0.6263 - val_loss: 2.1056 - val_accuracy: 0.1990\n",
            "Epoch 2684/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.6290\n",
            "Epoch 02684: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9111 - accuracy: 0.6290 - val_loss: 2.0375 - val_accuracy: 0.2335\n",
            "Epoch 2685/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6302\n",
            "Epoch 02685: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9084 - accuracy: 0.6302 - val_loss: 2.0065 - val_accuracy: 0.2431\n",
            "Epoch 2686/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6269\n",
            "Epoch 02686: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9124 - accuracy: 0.6269 - val_loss: 2.1055 - val_accuracy: 0.1939\n",
            "Epoch 2687/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9374 - accuracy: 0.6173\n",
            "Epoch 02687: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9374 - accuracy: 0.6173 - val_loss: 2.1112 - val_accuracy: 0.2101\n",
            "Epoch 2688/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9167 - accuracy: 0.6260\n",
            "Epoch 02688: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9167 - accuracy: 0.6260 - val_loss: 2.0631 - val_accuracy: 0.2268\n",
            "Epoch 2689/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9059 - accuracy: 0.6308\n",
            "Epoch 02689: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6308 - val_loss: 1.9996 - val_accuracy: 0.2446\n",
            "Epoch 2690/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9077 - accuracy: 0.6295\n",
            "Epoch 02690: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9077 - accuracy: 0.6295 - val_loss: 2.0825 - val_accuracy: 0.2072\n",
            "Epoch 2691/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9082 - accuracy: 0.6288\n",
            "Epoch 02691: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9081 - accuracy: 0.6289 - val_loss: 2.0172 - val_accuracy: 0.2422\n",
            "Epoch 2692/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6269\n",
            "Epoch 02692: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9125 - accuracy: 0.6270 - val_loss: 2.0161 - val_accuracy: 0.2328\n",
            "Epoch 2693/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8997 - accuracy: 0.6347\n",
            "Epoch 02693: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.8999 - accuracy: 0.6347 - val_loss: 2.0391 - val_accuracy: 0.2236\n",
            "Epoch 2694/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9280 - accuracy: 0.6179\n",
            "Epoch 02694: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9279 - accuracy: 0.6179 - val_loss: 2.0681 - val_accuracy: 0.2060\n",
            "Epoch 2695/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9115 - accuracy: 0.6278\n",
            "Epoch 02695: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9115 - accuracy: 0.6278 - val_loss: 2.0215 - val_accuracy: 0.2273\n",
            "Epoch 2696/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9081 - accuracy: 0.6298\n",
            "Epoch 02696: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9083 - accuracy: 0.6298 - val_loss: 2.1669 - val_accuracy: 0.1967\n",
            "Epoch 2697/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9131 - accuracy: 0.6269\n",
            "Epoch 02697: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9127 - accuracy: 0.6270 - val_loss: 2.1193 - val_accuracy: 0.1977\n",
            "Epoch 2698/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9084 - accuracy: 0.6277\n",
            "Epoch 02698: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9084 - accuracy: 0.6275 - val_loss: 1.9901 - val_accuracy: 0.2452\n",
            "Epoch 2699/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9158 - accuracy: 0.6247\n",
            "Epoch 02699: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9156 - accuracy: 0.6248 - val_loss: 1.9321 - val_accuracy: 0.2596\n",
            "Epoch 2700/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9399 - accuracy: 0.6127\n",
            "Epoch 02700: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9399 - accuracy: 0.6127 - val_loss: 2.1507 - val_accuracy: 0.1806\n",
            "Epoch 2701/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9213 - accuracy: 0.6231\n",
            "Epoch 02701: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9213 - accuracy: 0.6231 - val_loss: 2.0275 - val_accuracy: 0.2051\n",
            "Epoch 2702/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9051 - accuracy: 0.6293\n",
            "Epoch 02702: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9049 - accuracy: 0.6295 - val_loss: 2.0153 - val_accuracy: 0.2249\n",
            "Epoch 2703/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8996 - accuracy: 0.6355\n",
            "Epoch 02703: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8996 - accuracy: 0.6355 - val_loss: 2.1196 - val_accuracy: 0.1993\n",
            "Epoch 2704/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9082 - accuracy: 0.6282\n",
            "Epoch 02704: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9082 - accuracy: 0.6282 - val_loss: 2.1077 - val_accuracy: 0.2036\n",
            "Epoch 2705/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9050 - accuracy: 0.6316\n",
            "Epoch 02705: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9050 - accuracy: 0.6316 - val_loss: 2.0564 - val_accuracy: 0.2301\n",
            "Epoch 2706/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9155 - accuracy: 0.6265\n",
            "Epoch 02706: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9155 - accuracy: 0.6265 - val_loss: 2.0660 - val_accuracy: 0.2159\n",
            "Epoch 2707/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9182 - accuracy: 0.6253\n",
            "Epoch 02707: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9178 - accuracy: 0.6256 - val_loss: 2.0532 - val_accuracy: 0.2185\n",
            "Epoch 2708/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9123 - accuracy: 0.6291\n",
            "Epoch 02708: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9119 - accuracy: 0.6294 - val_loss: 2.0730 - val_accuracy: 0.2173\n",
            "Epoch 2709/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9093 - accuracy: 0.6285\n",
            "Epoch 02709: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9093 - accuracy: 0.6285 - val_loss: 2.0194 - val_accuracy: 0.2242\n",
            "Epoch 2710/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9016 - accuracy: 0.6330\n",
            "Epoch 02710: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9016 - accuracy: 0.6330 - val_loss: 2.0330 - val_accuracy: 0.2294\n",
            "Epoch 2711/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9032 - accuracy: 0.6324\n",
            "Epoch 02711: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9032 - accuracy: 0.6324 - val_loss: 1.9489 - val_accuracy: 0.2483\n",
            "Epoch 2712/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9149 - accuracy: 0.6263\n",
            "Epoch 02712: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9146 - accuracy: 0.6266 - val_loss: 2.0803 - val_accuracy: 0.2123\n",
            "Epoch 2713/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.6273\n",
            "Epoch 02713: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9113 - accuracy: 0.6273 - val_loss: 2.1595 - val_accuracy: 0.1949\n",
            "Epoch 2714/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9090 - accuracy: 0.6309\n",
            "Epoch 02714: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9090 - accuracy: 0.6309 - val_loss: 2.1158 - val_accuracy: 0.2054\n",
            "Epoch 2715/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9016 - accuracy: 0.6318\n",
            "Epoch 02715: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9019 - accuracy: 0.6319 - val_loss: 2.0305 - val_accuracy: 0.2335\n",
            "Epoch 2716/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9026 - accuracy: 0.6305\n",
            "Epoch 02716: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9029 - accuracy: 0.6303 - val_loss: 2.0755 - val_accuracy: 0.2110\n",
            "Epoch 2717/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.6269\n",
            "Epoch 02717: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9100 - accuracy: 0.6270 - val_loss: 1.9930 - val_accuracy: 0.2442\n",
            "Epoch 2718/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9169 - accuracy: 0.6244\n",
            "Epoch 02718: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9166 - accuracy: 0.6244 - val_loss: 2.0269 - val_accuracy: 0.2305\n",
            "Epoch 2719/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9049 - accuracy: 0.6313\n",
            "Epoch 02719: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9054 - accuracy: 0.6311 - val_loss: 2.0076 - val_accuracy: 0.2333\n",
            "Epoch 2720/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.6262\n",
            "Epoch 02720: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9129 - accuracy: 0.6262 - val_loss: 1.9885 - val_accuracy: 0.2366\n",
            "Epoch 2721/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9077 - accuracy: 0.6304\n",
            "Epoch 02721: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9077 - accuracy: 0.6304 - val_loss: 1.9907 - val_accuracy: 0.2377\n",
            "Epoch 2722/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9077 - accuracy: 0.6276\n",
            "Epoch 02722: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9077 - accuracy: 0.6276 - val_loss: 2.0218 - val_accuracy: 0.2278\n",
            "Epoch 2723/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9066 - accuracy: 0.6317\n",
            "Epoch 02723: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9066 - accuracy: 0.6317 - val_loss: 2.0071 - val_accuracy: 0.2348\n",
            "Epoch 2724/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9053 - accuracy: 0.6317\n",
            "Epoch 02724: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9049 - accuracy: 0.6321 - val_loss: 2.0797 - val_accuracy: 0.2123\n",
            "Epoch 2725/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9172 - accuracy: 0.6243\n",
            "Epoch 02725: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9177 - accuracy: 0.6240 - val_loss: 2.0701 - val_accuracy: 0.2097\n",
            "Epoch 2726/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9330 - accuracy: 0.6173\n",
            "Epoch 02726: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9330 - accuracy: 0.6173 - val_loss: 2.0436 - val_accuracy: 0.2219\n",
            "Epoch 2727/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9060 - accuracy: 0.6311\n",
            "Epoch 02727: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6313 - val_loss: 2.0327 - val_accuracy: 0.2374\n",
            "Epoch 2728/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.6289\n",
            "Epoch 02728: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9097 - accuracy: 0.6289 - val_loss: 2.0454 - val_accuracy: 0.2170\n",
            "Epoch 2729/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9136 - accuracy: 0.6244\n",
            "Epoch 02729: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9129 - accuracy: 0.6248 - val_loss: 2.1366 - val_accuracy: 0.1992\n",
            "Epoch 2730/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9101 - accuracy: 0.6277\n",
            "Epoch 02730: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9101 - accuracy: 0.6277 - val_loss: 1.9602 - val_accuracy: 0.2383\n",
            "Epoch 2731/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.6259\n",
            "Epoch 02731: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9134 - accuracy: 0.6259 - val_loss: 2.1125 - val_accuracy: 0.1962\n",
            "Epoch 2732/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6238\n",
            "Epoch 02732: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9124 - accuracy: 0.6240 - val_loss: 1.9934 - val_accuracy: 0.2290\n",
            "Epoch 2733/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9092 - accuracy: 0.6289\n",
            "Epoch 02733: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9092 - accuracy: 0.6289 - val_loss: 2.0466 - val_accuracy: 0.2196\n",
            "Epoch 2734/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9029 - accuracy: 0.6343\n",
            "Epoch 02734: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9029 - accuracy: 0.6343 - val_loss: 2.0720 - val_accuracy: 0.2129\n",
            "Epoch 2735/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9101 - accuracy: 0.6283\n",
            "Epoch 02735: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9099 - accuracy: 0.6286 - val_loss: 2.0241 - val_accuracy: 0.2207\n",
            "Epoch 2736/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9062 - accuracy: 0.6293\n",
            "Epoch 02736: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9062 - accuracy: 0.6293 - val_loss: 2.0612 - val_accuracy: 0.2137\n",
            "Epoch 2737/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9020 - accuracy: 0.6330\n",
            "Epoch 02737: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9025 - accuracy: 0.6326 - val_loss: 2.0870 - val_accuracy: 0.2066\n",
            "Epoch 2738/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.6276\n",
            "Epoch 02738: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9092 - accuracy: 0.6276 - val_loss: 2.0353 - val_accuracy: 0.2183\n",
            "Epoch 2739/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9321 - accuracy: 0.6161\n",
            "Epoch 02739: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9321 - accuracy: 0.6161 - val_loss: 2.0642 - val_accuracy: 0.2132\n",
            "Epoch 2740/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9153 - accuracy: 0.6239\n",
            "Epoch 02740: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9153 - accuracy: 0.6239 - val_loss: 1.9831 - val_accuracy: 0.2336\n",
            "Epoch 2741/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9165 - accuracy: 0.6257\n",
            "Epoch 02741: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9165 - accuracy: 0.6257 - val_loss: 2.0629 - val_accuracy: 0.2121\n",
            "Epoch 2742/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8957 - accuracy: 0.6330\n",
            "Epoch 02742: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8959 - accuracy: 0.6329 - val_loss: 2.0310 - val_accuracy: 0.2339\n",
            "Epoch 2743/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9162 - accuracy: 0.6270\n",
            "Epoch 02743: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9162 - accuracy: 0.6270 - val_loss: 2.0581 - val_accuracy: 0.2198\n",
            "Epoch 2744/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9047 - accuracy: 0.6334\n",
            "Epoch 02744: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9047 - accuracy: 0.6334 - val_loss: 2.0326 - val_accuracy: 0.2232\n",
            "Epoch 2745/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9125 - accuracy: 0.6288\n",
            "Epoch 02745: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9125 - accuracy: 0.6288 - val_loss: 2.0269 - val_accuracy: 0.2332\n",
            "Epoch 2746/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.6289\n",
            "Epoch 02746: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9081 - accuracy: 0.6289 - val_loss: 1.9762 - val_accuracy: 0.2406\n",
            "Epoch 2747/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.6309\n",
            "Epoch 02747: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9048 - accuracy: 0.6309 - val_loss: 2.1088 - val_accuracy: 0.1964\n",
            "Epoch 2748/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9044 - accuracy: 0.6316\n",
            "Epoch 02748: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9044 - accuracy: 0.6316 - val_loss: 2.0362 - val_accuracy: 0.2313\n",
            "Epoch 2749/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9085 - accuracy: 0.6278\n",
            "Epoch 02749: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9085 - accuracy: 0.6278 - val_loss: 2.0740 - val_accuracy: 0.2118\n",
            "Epoch 2750/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9047 - accuracy: 0.6305\n",
            "Epoch 02750: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9048 - accuracy: 0.6304 - val_loss: 2.0392 - val_accuracy: 0.2211\n",
            "Epoch 2751/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9043 - accuracy: 0.6306\n",
            "Epoch 02751: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9043 - accuracy: 0.6307 - val_loss: 2.0229 - val_accuracy: 0.2307\n",
            "Epoch 2752/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9076 - accuracy: 0.6290\n",
            "Epoch 02752: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9076 - accuracy: 0.6290 - val_loss: 2.0996 - val_accuracy: 0.1962\n",
            "Epoch 2753/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.6294\n",
            "Epoch 02753: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9081 - accuracy: 0.6294 - val_loss: 1.9877 - val_accuracy: 0.2414\n",
            "Epoch 2754/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9057 - accuracy: 0.6306\n",
            "Epoch 02754: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9057 - accuracy: 0.6307 - val_loss: 2.0421 - val_accuracy: 0.2163\n",
            "Epoch 2755/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9230 - accuracy: 0.6223\n",
            "Epoch 02755: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9230 - accuracy: 0.6223 - val_loss: 2.0899 - val_accuracy: 0.1975\n",
            "Epoch 2756/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9050 - accuracy: 0.6313\n",
            "Epoch 02756: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9050 - accuracy: 0.6313 - val_loss: 2.1231 - val_accuracy: 0.2077\n",
            "Epoch 2757/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9162 - accuracy: 0.6239\n",
            "Epoch 02757: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9158 - accuracy: 0.6242 - val_loss: 2.1164 - val_accuracy: 0.2060\n",
            "Epoch 2758/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.6273\n",
            "Epoch 02758: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9147 - accuracy: 0.6268 - val_loss: 2.0533 - val_accuracy: 0.2129\n",
            "Epoch 2759/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9078 - accuracy: 0.6294\n",
            "Epoch 02759: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9078 - accuracy: 0.6294 - val_loss: 2.0570 - val_accuracy: 0.2179\n",
            "Epoch 2760/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9066 - accuracy: 0.6291\n",
            "Epoch 02760: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9066 - accuracy: 0.6291 - val_loss: 2.1801 - val_accuracy: 0.1752\n",
            "Epoch 2761/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9034 - accuracy: 0.6309\n",
            "Epoch 02761: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9036 - accuracy: 0.6309 - val_loss: 2.0835 - val_accuracy: 0.2125\n",
            "Epoch 2762/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9047 - accuracy: 0.6301\n",
            "Epoch 02762: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9047 - accuracy: 0.6301 - val_loss: 2.0453 - val_accuracy: 0.2159\n",
            "Epoch 2763/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9181 - accuracy: 0.6238\n",
            "Epoch 02763: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9183 - accuracy: 0.6237 - val_loss: 2.0841 - val_accuracy: 0.2066\n",
            "Epoch 2764/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9088 - accuracy: 0.6293\n",
            "Epoch 02764: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9088 - accuracy: 0.6293 - val_loss: 1.9957 - val_accuracy: 0.2354\n",
            "Epoch 2765/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9098 - accuracy: 0.6298\n",
            "Epoch 02765: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9098 - accuracy: 0.6298 - val_loss: 1.9996 - val_accuracy: 0.2405\n",
            "Epoch 2766/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9097 - accuracy: 0.6302\n",
            "Epoch 02766: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9089 - accuracy: 0.6305 - val_loss: 2.0541 - val_accuracy: 0.2225\n",
            "Epoch 2767/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6323\n",
            "Epoch 02767: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9017 - accuracy: 0.6323 - val_loss: 1.9980 - val_accuracy: 0.2329\n",
            "Epoch 2768/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.6256\n",
            "Epoch 02768: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9122 - accuracy: 0.6256 - val_loss: 2.0781 - val_accuracy: 0.1977\n",
            "Epoch 2769/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9063 - accuracy: 0.6305\n",
            "Epoch 02769: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9063 - accuracy: 0.6305 - val_loss: 2.0699 - val_accuracy: 0.2164\n",
            "Epoch 2770/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9196 - accuracy: 0.6217\n",
            "Epoch 02770: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9196 - accuracy: 0.6217 - val_loss: 2.0479 - val_accuracy: 0.2125\n",
            "Epoch 2771/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9155 - accuracy: 0.6274\n",
            "Epoch 02771: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9154 - accuracy: 0.6273 - val_loss: 2.1028 - val_accuracy: 0.2092\n",
            "Epoch 2772/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9026 - accuracy: 0.6333\n",
            "Epoch 02772: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9026 - accuracy: 0.6333 - val_loss: 2.0925 - val_accuracy: 0.2038\n",
            "Epoch 2773/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9082 - accuracy: 0.6300\n",
            "Epoch 02773: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9082 - accuracy: 0.6300 - val_loss: 1.9977 - val_accuracy: 0.2382\n",
            "Epoch 2774/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9149 - accuracy: 0.6257\n",
            "Epoch 02774: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9154 - accuracy: 0.6255 - val_loss: 2.0706 - val_accuracy: 0.2091\n",
            "Epoch 2775/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9083 - accuracy: 0.6289\n",
            "Epoch 02775: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9083 - accuracy: 0.6289 - val_loss: 1.9856 - val_accuracy: 0.2421\n",
            "Epoch 2776/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9142 - accuracy: 0.6267\n",
            "Epoch 02776: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9141 - accuracy: 0.6266 - val_loss: 2.0483 - val_accuracy: 0.2185\n",
            "Epoch 2777/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6326\n",
            "Epoch 02777: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9017 - accuracy: 0.6326 - val_loss: 1.9688 - val_accuracy: 0.2398\n",
            "Epoch 2778/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9069 - accuracy: 0.6303\n",
            "Epoch 02778: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9075 - accuracy: 0.6299 - val_loss: 2.0187 - val_accuracy: 0.2353\n",
            "Epoch 2779/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9069 - accuracy: 0.6300\n",
            "Epoch 02779: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9067 - accuracy: 0.6301 - val_loss: 2.0842 - val_accuracy: 0.2099\n",
            "Epoch 2780/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6299\n",
            "Epoch 02780: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9084 - accuracy: 0.6299 - val_loss: 2.0325 - val_accuracy: 0.2355\n",
            "Epoch 2781/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9002 - accuracy: 0.6346\n",
            "Epoch 02781: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 45ms/step - loss: 0.8999 - accuracy: 0.6347 - val_loss: 2.0255 - val_accuracy: 0.2350\n",
            "Epoch 2782/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8992 - accuracy: 0.6337\n",
            "Epoch 02782: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8991 - accuracy: 0.6337 - val_loss: 2.0372 - val_accuracy: 0.2258\n",
            "Epoch 2783/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9059 - accuracy: 0.6298\n",
            "Epoch 02783: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6300 - val_loss: 2.0761 - val_accuracy: 0.2165\n",
            "Epoch 2784/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6313\n",
            "Epoch 02784: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9017 - accuracy: 0.6313 - val_loss: 2.0361 - val_accuracy: 0.2153\n",
            "Epoch 2785/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9028 - accuracy: 0.6326\n",
            "Epoch 02785: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9028 - accuracy: 0.6326 - val_loss: 2.0900 - val_accuracy: 0.2059\n",
            "Epoch 2786/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9110 - accuracy: 0.6282\n",
            "Epoch 02786: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9110 - accuracy: 0.6282 - val_loss: 2.0867 - val_accuracy: 0.2096\n",
            "Epoch 2787/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9019 - accuracy: 0.6324\n",
            "Epoch 02787: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9019 - accuracy: 0.6324 - val_loss: 2.1243 - val_accuracy: 0.1939\n",
            "Epoch 2788/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9038 - accuracy: 0.6325\n",
            "Epoch 02788: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9038 - accuracy: 0.6325 - val_loss: 2.0748 - val_accuracy: 0.2034\n",
            "Epoch 2789/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9089 - accuracy: 0.6299\n",
            "Epoch 02789: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9089 - accuracy: 0.6299 - val_loss: 2.0967 - val_accuracy: 0.2006\n",
            "Epoch 2790/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9121 - accuracy: 0.6265\n",
            "Epoch 02790: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9120 - accuracy: 0.6266 - val_loss: 2.0969 - val_accuracy: 0.1988\n",
            "Epoch 2791/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9181 - accuracy: 0.6250\n",
            "Epoch 02791: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9184 - accuracy: 0.6249 - val_loss: 2.0048 - val_accuracy: 0.2383\n",
            "Epoch 2792/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9017 - accuracy: 0.6343\n",
            "Epoch 02792: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9020 - accuracy: 0.6340 - val_loss: 2.1744 - val_accuracy: 0.1806\n",
            "Epoch 2793/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9116 - accuracy: 0.6276\n",
            "Epoch 02793: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9116 - accuracy: 0.6276 - val_loss: 2.0702 - val_accuracy: 0.2064\n",
            "Epoch 2794/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9064 - accuracy: 0.6289\n",
            "Epoch 02794: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9061 - accuracy: 0.6290 - val_loss: 2.0531 - val_accuracy: 0.2322\n",
            "Epoch 2795/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9065 - accuracy: 0.6303\n",
            "Epoch 02795: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9064 - accuracy: 0.6302 - val_loss: 2.0617 - val_accuracy: 0.2110\n",
            "Epoch 2796/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9143 - accuracy: 0.6276\n",
            "Epoch 02796: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9143 - accuracy: 0.6276 - val_loss: 2.0904 - val_accuracy: 0.2051\n",
            "Epoch 2797/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9049 - accuracy: 0.6302\n",
            "Epoch 02797: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9044 - accuracy: 0.6304 - val_loss: 2.0705 - val_accuracy: 0.2078\n",
            "Epoch 2798/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6268\n",
            "Epoch 02798: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9123 - accuracy: 0.6268 - val_loss: 2.1277 - val_accuracy: 0.1993\n",
            "Epoch 2799/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9068 - accuracy: 0.6307\n",
            "Epoch 02799: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9068 - accuracy: 0.6307 - val_loss: 2.0710 - val_accuracy: 0.2078\n",
            "Epoch 2800/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9064 - accuracy: 0.6281\n",
            "Epoch 02800: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9063 - accuracy: 0.6281 - val_loss: 2.0893 - val_accuracy: 0.2086\n",
            "Epoch 2801/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9073 - accuracy: 0.6309\n",
            "Epoch 02801: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9073 - accuracy: 0.6309 - val_loss: 2.0403 - val_accuracy: 0.2201\n",
            "Epoch 2802/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9139 - accuracy: 0.6271\n",
            "Epoch 02802: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9139 - accuracy: 0.6271 - val_loss: 2.0488 - val_accuracy: 0.2120\n",
            "Epoch 2803/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9192 - accuracy: 0.6228\n",
            "Epoch 02803: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9195 - accuracy: 0.6227 - val_loss: 1.9964 - val_accuracy: 0.2287\n",
            "Epoch 2804/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9324 - accuracy: 0.6162\n",
            "Epoch 02804: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9321 - accuracy: 0.6164 - val_loss: 2.0628 - val_accuracy: 0.2074\n",
            "Epoch 2805/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9089 - accuracy: 0.6279\n",
            "Epoch 02805: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9084 - accuracy: 0.6282 - val_loss: 2.0455 - val_accuracy: 0.2164\n",
            "Epoch 2806/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9006 - accuracy: 0.6337\n",
            "Epoch 02806: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9000 - accuracy: 0.6339 - val_loss: 2.1032 - val_accuracy: 0.2040\n",
            "Epoch 2807/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9202 - accuracy: 0.6234\n",
            "Epoch 02807: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9202 - accuracy: 0.6234 - val_loss: 2.0621 - val_accuracy: 0.2183\n",
            "Epoch 2808/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9076 - accuracy: 0.6275\n",
            "Epoch 02808: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9076 - accuracy: 0.6275 - val_loss: 2.0375 - val_accuracy: 0.2248\n",
            "Epoch 2809/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.6215\n",
            "Epoch 02809: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9214 - accuracy: 0.6215 - val_loss: 2.1010 - val_accuracy: 0.1947\n",
            "Epoch 2810/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.6281\n",
            "Epoch 02810: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9097 - accuracy: 0.6281 - val_loss: 2.0610 - val_accuracy: 0.2146\n",
            "Epoch 2811/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9113 - accuracy: 0.6273\n",
            "Epoch 02811: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9114 - accuracy: 0.6272 - val_loss: 2.0742 - val_accuracy: 0.2040\n",
            "Epoch 2812/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9059 - accuracy: 0.6307\n",
            "Epoch 02812: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6307 - val_loss: 2.0140 - val_accuracy: 0.2409\n",
            "Epoch 2813/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9053 - accuracy: 0.6297\n",
            "Epoch 02813: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9052 - accuracy: 0.6297 - val_loss: 2.0981 - val_accuracy: 0.2022\n",
            "Epoch 2814/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9022 - accuracy: 0.6324\n",
            "Epoch 02814: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9024 - accuracy: 0.6323 - val_loss: 2.0196 - val_accuracy: 0.2328\n",
            "Epoch 2815/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9100 - accuracy: 0.6268\n",
            "Epoch 02815: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9102 - accuracy: 0.6267 - val_loss: 1.9526 - val_accuracy: 0.2514\n",
            "Epoch 2816/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9073 - accuracy: 0.6295\n",
            "Epoch 02816: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9073 - accuracy: 0.6295 - val_loss: 2.1557 - val_accuracy: 0.1819\n",
            "Epoch 2817/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8990 - accuracy: 0.6347\n",
            "Epoch 02817: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8990 - accuracy: 0.6347 - val_loss: 2.0559 - val_accuracy: 0.2133\n",
            "Epoch 2818/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9132 - accuracy: 0.6262\n",
            "Epoch 02818: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9132 - accuracy: 0.6262 - val_loss: 2.1233 - val_accuracy: 0.1996\n",
            "Epoch 2819/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.6254\n",
            "Epoch 02819: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9142 - accuracy: 0.6254 - val_loss: 2.1098 - val_accuracy: 0.2084\n",
            "Epoch 2820/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9026 - accuracy: 0.6332\n",
            "Epoch 02820: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9024 - accuracy: 0.6334 - val_loss: 2.0676 - val_accuracy: 0.2146\n",
            "Epoch 2821/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9032 - accuracy: 0.6321\n",
            "Epoch 02821: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9032 - accuracy: 0.6321 - val_loss: 2.1122 - val_accuracy: 0.2014\n",
            "Epoch 2822/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.6333\n",
            "Epoch 02822: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9004 - accuracy: 0.6333 - val_loss: 2.1148 - val_accuracy: 0.1954\n",
            "Epoch 2823/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9062 - accuracy: 0.6310\n",
            "Epoch 02823: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9065 - accuracy: 0.6309 - val_loss: 2.1159 - val_accuracy: 0.1890\n",
            "Epoch 2824/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8993 - accuracy: 0.6351\n",
            "Epoch 02824: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.8993 - accuracy: 0.6351 - val_loss: 2.1217 - val_accuracy: 0.2044\n",
            "Epoch 2825/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9082 - accuracy: 0.6295\n",
            "Epoch 02825: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9080 - accuracy: 0.6295 - val_loss: 2.0463 - val_accuracy: 0.2296\n",
            "Epoch 2826/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9156 - accuracy: 0.6238\n",
            "Epoch 02826: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9160 - accuracy: 0.6235 - val_loss: 2.1267 - val_accuracy: 0.1891\n",
            "Epoch 2827/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9085 - accuracy: 0.6283\n",
            "Epoch 02827: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9086 - accuracy: 0.6283 - val_loss: 2.0669 - val_accuracy: 0.2058\n",
            "Epoch 2828/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9166 - accuracy: 0.6239\n",
            "Epoch 02828: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9163 - accuracy: 0.6240 - val_loss: 2.0690 - val_accuracy: 0.2065\n",
            "Epoch 2829/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6243\n",
            "Epoch 02829: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9174 - accuracy: 0.6243 - val_loss: 2.0008 - val_accuracy: 0.2285\n",
            "Epoch 2830/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9187 - accuracy: 0.6255\n",
            "Epoch 02830: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9186 - accuracy: 0.6255 - val_loss: 1.9898 - val_accuracy: 0.2417\n",
            "Epoch 2831/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9077 - accuracy: 0.6293\n",
            "Epoch 02831: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9077 - accuracy: 0.6293 - val_loss: 2.0443 - val_accuracy: 0.2211\n",
            "Epoch 2832/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9043 - accuracy: 0.6328\n",
            "Epoch 02832: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9041 - accuracy: 0.6329 - val_loss: 2.0433 - val_accuracy: 0.2171\n",
            "Epoch 2833/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6315\n",
            "Epoch 02833: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9017 - accuracy: 0.6315 - val_loss: 2.0358 - val_accuracy: 0.2221\n",
            "Epoch 2834/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9176 - accuracy: 0.6239\n",
            "Epoch 02834: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9176 - accuracy: 0.6239 - val_loss: 1.9735 - val_accuracy: 0.2387\n",
            "Epoch 2835/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9082 - accuracy: 0.6301\n",
            "Epoch 02835: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9082 - accuracy: 0.6301 - val_loss: 2.0172 - val_accuracy: 0.2353\n",
            "Epoch 2836/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9121 - accuracy: 0.6267\n",
            "Epoch 02836: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9122 - accuracy: 0.6269 - val_loss: 1.9978 - val_accuracy: 0.2414\n",
            "Epoch 2837/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9072 - accuracy: 0.6290\n",
            "Epoch 02837: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9072 - accuracy: 0.6290 - val_loss: 1.9733 - val_accuracy: 0.2348\n",
            "Epoch 2838/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9108 - accuracy: 0.6277\n",
            "Epoch 02838: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9103 - accuracy: 0.6280 - val_loss: 2.0529 - val_accuracy: 0.2153\n",
            "Epoch 2839/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8990 - accuracy: 0.6344\n",
            "Epoch 02839: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8990 - accuracy: 0.6344 - val_loss: 2.0655 - val_accuracy: 0.2141\n",
            "Epoch 2840/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9089 - accuracy: 0.6292\n",
            "Epoch 02840: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9093 - accuracy: 0.6289 - val_loss: 1.9994 - val_accuracy: 0.2381\n",
            "Epoch 2841/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9063 - accuracy: 0.6295\n",
            "Epoch 02841: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9066 - accuracy: 0.6292 - val_loss: 2.0478 - val_accuracy: 0.2152\n",
            "Epoch 2842/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9158 - accuracy: 0.6253\n",
            "Epoch 02842: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9171 - accuracy: 0.6246 - val_loss: 2.0163 - val_accuracy: 0.2288\n",
            "Epoch 2843/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9375 - accuracy: 0.6142\n",
            "Epoch 02843: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9381 - accuracy: 0.6142 - val_loss: 2.0353 - val_accuracy: 0.2253\n",
            "Epoch 2844/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.6237\n",
            "Epoch 02844: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9146 - accuracy: 0.6237 - val_loss: 2.1160 - val_accuracy: 0.1903\n",
            "Epoch 2845/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9199 - accuracy: 0.6211\n",
            "Epoch 02845: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9199 - accuracy: 0.6211 - val_loss: 2.1255 - val_accuracy: 0.1930\n",
            "Epoch 2846/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9085 - accuracy: 0.6287\n",
            "Epoch 02846: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9085 - accuracy: 0.6287 - val_loss: 2.0578 - val_accuracy: 0.2242\n",
            "Epoch 2847/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9037 - accuracy: 0.6317\n",
            "Epoch 02847: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9037 - accuracy: 0.6317 - val_loss: 2.0682 - val_accuracy: 0.2035\n",
            "Epoch 2848/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9037 - accuracy: 0.6304\n",
            "Epoch 02848: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9038 - accuracy: 0.6303 - val_loss: 2.0464 - val_accuracy: 0.2227\n",
            "Epoch 2849/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9001 - accuracy: 0.6337\n",
            "Epoch 02849: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9001 - accuracy: 0.6335 - val_loss: 2.1230 - val_accuracy: 0.1875\n",
            "Epoch 2850/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9022 - accuracy: 0.6323\n",
            "Epoch 02850: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9021 - accuracy: 0.6322 - val_loss: 2.1338 - val_accuracy: 0.1891\n",
            "Epoch 2851/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9078 - accuracy: 0.6304\n",
            "Epoch 02851: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9076 - accuracy: 0.6305 - val_loss: 2.1377 - val_accuracy: 0.1912\n",
            "Epoch 2852/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9089 - accuracy: 0.6294\n",
            "Epoch 02852: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9089 - accuracy: 0.6294 - val_loss: 2.0892 - val_accuracy: 0.2029\n",
            "Epoch 2853/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9057 - accuracy: 0.6296\n",
            "Epoch 02853: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9055 - accuracy: 0.6295 - val_loss: 2.1015 - val_accuracy: 0.2031\n",
            "Epoch 2854/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9126 - accuracy: 0.6266\n",
            "Epoch 02854: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9126 - accuracy: 0.6266 - val_loss: 2.0774 - val_accuracy: 0.2030\n",
            "Epoch 2855/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9105 - accuracy: 0.6274\n",
            "Epoch 02855: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9106 - accuracy: 0.6275 - val_loss: 2.0954 - val_accuracy: 0.2040\n",
            "Epoch 2856/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9001 - accuracy: 0.6336\n",
            "Epoch 02856: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9002 - accuracy: 0.6337 - val_loss: 2.0186 - val_accuracy: 0.2346\n",
            "Epoch 2857/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9021 - accuracy: 0.6311\n",
            "Epoch 02857: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9018 - accuracy: 0.6313 - val_loss: 2.0615 - val_accuracy: 0.2172\n",
            "Epoch 2858/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8972 - accuracy: 0.6358\n",
            "Epoch 02858: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.8972 - accuracy: 0.6358 - val_loss: 2.0665 - val_accuracy: 0.2132\n",
            "Epoch 2859/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9046 - accuracy: 0.6300\n",
            "Epoch 02859: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9053 - accuracy: 0.6298 - val_loss: 2.1918 - val_accuracy: 0.1707\n",
            "Epoch 2860/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9112 - accuracy: 0.6266\n",
            "Epoch 02860: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9112 - accuracy: 0.6266 - val_loss: 2.0506 - val_accuracy: 0.2268\n",
            "Epoch 2861/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9149 - accuracy: 0.6251\n",
            "Epoch 02861: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9144 - accuracy: 0.6256 - val_loss: 2.1306 - val_accuracy: 0.1978\n",
            "Epoch 2862/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9039 - accuracy: 0.6317\n",
            "Epoch 02862: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9039 - accuracy: 0.6317 - val_loss: 2.0589 - val_accuracy: 0.2188\n",
            "Epoch 2863/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9168 - accuracy: 0.6253\n",
            "Epoch 02863: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9164 - accuracy: 0.6253 - val_loss: 2.0142 - val_accuracy: 0.2321\n",
            "Epoch 2864/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9091 - accuracy: 0.6281\n",
            "Epoch 02864: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9095 - accuracy: 0.6278 - val_loss: 2.1200 - val_accuracy: 0.2028\n",
            "Epoch 2865/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9115 - accuracy: 0.6289\n",
            "Epoch 02865: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9115 - accuracy: 0.6289 - val_loss: 2.0482 - val_accuracy: 0.2188\n",
            "Epoch 2866/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6317\n",
            "Epoch 02866: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9017 - accuracy: 0.6317 - val_loss: 2.0633 - val_accuracy: 0.2234\n",
            "Epoch 2867/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6323\n",
            "Epoch 02867: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9017 - accuracy: 0.6323 - val_loss: 2.0330 - val_accuracy: 0.2184\n",
            "Epoch 2868/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9041 - accuracy: 0.6327\n",
            "Epoch 02868: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9040 - accuracy: 0.6326 - val_loss: 2.0740 - val_accuracy: 0.1999\n",
            "Epoch 2869/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9067 - accuracy: 0.6307\n",
            "Epoch 02869: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9064 - accuracy: 0.6309 - val_loss: 1.9881 - val_accuracy: 0.2413\n",
            "Epoch 2870/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9104 - accuracy: 0.6284\n",
            "Epoch 02870: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9104 - accuracy: 0.6284 - val_loss: 2.0585 - val_accuracy: 0.2144\n",
            "Epoch 2871/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6301\n",
            "Epoch 02871: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9084 - accuracy: 0.6301 - val_loss: 2.0703 - val_accuracy: 0.2142\n",
            "Epoch 2872/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9159 - accuracy: 0.6257\n",
            "Epoch 02872: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9155 - accuracy: 0.6257 - val_loss: 2.0395 - val_accuracy: 0.2147\n",
            "Epoch 2873/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9171 - accuracy: 0.6247\n",
            "Epoch 02873: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9171 - accuracy: 0.6247 - val_loss: 2.0459 - val_accuracy: 0.2248\n",
            "Epoch 2874/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9027 - accuracy: 0.6322\n",
            "Epoch 02874: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9027 - accuracy: 0.6322 - val_loss: 2.0852 - val_accuracy: 0.2101\n",
            "Epoch 2875/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9010 - accuracy: 0.6329\n",
            "Epoch 02875: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9010 - accuracy: 0.6329 - val_loss: 2.1486 - val_accuracy: 0.1802\n",
            "Epoch 2876/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9060 - accuracy: 0.6291\n",
            "Epoch 02876: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9056 - accuracy: 0.6292 - val_loss: 1.9939 - val_accuracy: 0.2369\n",
            "Epoch 2877/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9052 - accuracy: 0.6311\n",
            "Epoch 02877: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9052 - accuracy: 0.6311 - val_loss: 2.0046 - val_accuracy: 0.2379\n",
            "Epoch 2878/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9078 - accuracy: 0.6280\n",
            "Epoch 02878: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9078 - accuracy: 0.6280 - val_loss: 2.0674 - val_accuracy: 0.2220\n",
            "Epoch 2879/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9001 - accuracy: 0.6341\n",
            "Epoch 02879: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9005 - accuracy: 0.6338 - val_loss: 2.1570 - val_accuracy: 0.1833\n",
            "Epoch 2880/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9064 - accuracy: 0.6306\n",
            "Epoch 02880: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9063 - accuracy: 0.6307 - val_loss: 2.0336 - val_accuracy: 0.2182\n",
            "Epoch 2881/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8990 - accuracy: 0.6342\n",
            "Epoch 02881: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.8992 - accuracy: 0.6342 - val_loss: 2.0676 - val_accuracy: 0.2164\n",
            "Epoch 2882/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9245 - accuracy: 0.6201\n",
            "Epoch 02882: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9243 - accuracy: 0.6203 - val_loss: 2.0629 - val_accuracy: 0.2201\n",
            "Epoch 2883/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9038 - accuracy: 0.6305\n",
            "Epoch 02883: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9036 - accuracy: 0.6307 - val_loss: 2.0893 - val_accuracy: 0.2020\n",
            "Epoch 2884/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6251\n",
            "Epoch 02884: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9142 - accuracy: 0.6251 - val_loss: 2.0514 - val_accuracy: 0.2103\n",
            "Epoch 2885/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9049 - accuracy: 0.6299\n",
            "Epoch 02885: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9053 - accuracy: 0.6298 - val_loss: 2.0449 - val_accuracy: 0.2205\n",
            "Epoch 2886/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9083 - accuracy: 0.6288\n",
            "Epoch 02886: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9086 - accuracy: 0.6285 - val_loss: 2.1225 - val_accuracy: 0.1972\n",
            "Epoch 2887/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9118 - accuracy: 0.6275\n",
            "Epoch 02887: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9121 - accuracy: 0.6273 - val_loss: 2.0740 - val_accuracy: 0.2101\n",
            "Epoch 2888/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9039 - accuracy: 0.6315\n",
            "Epoch 02888: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9039 - accuracy: 0.6315 - val_loss: 2.0412 - val_accuracy: 0.2192\n",
            "Epoch 2889/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9142 - accuracy: 0.6258\n",
            "Epoch 02889: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9148 - accuracy: 0.6256 - val_loss: 2.0170 - val_accuracy: 0.2276\n",
            "Epoch 2890/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9014 - accuracy: 0.6332\n",
            "Epoch 02890: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9014 - accuracy: 0.6332 - val_loss: 2.0059 - val_accuracy: 0.2433\n",
            "Epoch 2891/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9022 - accuracy: 0.6321\n",
            "Epoch 02891: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9019 - accuracy: 0.6324 - val_loss: 2.0558 - val_accuracy: 0.2136\n",
            "Epoch 2892/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9019 - accuracy: 0.6330\n",
            "Epoch 02892: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9019 - accuracy: 0.6330 - val_loss: 2.1278 - val_accuracy: 0.1886\n",
            "Epoch 2893/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9099 - accuracy: 0.6292\n",
            "Epoch 02893: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9097 - accuracy: 0.6291 - val_loss: 2.1120 - val_accuracy: 0.1996\n",
            "Epoch 2894/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9193 - accuracy: 0.6232\n",
            "Epoch 02894: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9193 - accuracy: 0.6232 - val_loss: 2.0681 - val_accuracy: 0.2050\n",
            "Epoch 2895/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.6236\n",
            "Epoch 02895: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9182 - accuracy: 0.6236 - val_loss: 2.0487 - val_accuracy: 0.2030\n",
            "Epoch 2896/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9060 - accuracy: 0.6297\n",
            "Epoch 02896: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9060 - accuracy: 0.6297 - val_loss: 2.0734 - val_accuracy: 0.2119\n",
            "Epoch 2897/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9047 - accuracy: 0.6321\n",
            "Epoch 02897: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9047 - accuracy: 0.6321 - val_loss: 2.1529 - val_accuracy: 0.1842\n",
            "Epoch 2898/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.6282\n",
            "Epoch 02898: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9138 - accuracy: 0.6282 - val_loss: 2.1256 - val_accuracy: 0.1980\n",
            "Epoch 2899/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9038 - accuracy: 0.6314\n",
            "Epoch 02899: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9038 - accuracy: 0.6314 - val_loss: 2.1011 - val_accuracy: 0.1975\n",
            "Epoch 2900/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9015 - accuracy: 0.6321\n",
            "Epoch 02900: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9014 - accuracy: 0.6321 - val_loss: 2.0608 - val_accuracy: 0.2092\n",
            "Epoch 2901/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9099 - accuracy: 0.6289\n",
            "Epoch 02901: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9099 - accuracy: 0.6289 - val_loss: 2.0454 - val_accuracy: 0.2245\n",
            "Epoch 2902/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9093 - accuracy: 0.6282\n",
            "Epoch 02902: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9087 - accuracy: 0.6284 - val_loss: 1.9661 - val_accuracy: 0.2446\n",
            "Epoch 2903/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9130 - accuracy: 0.6257\n",
            "Epoch 02903: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9129 - accuracy: 0.6259 - val_loss: 2.0524 - val_accuracy: 0.2090\n",
            "Epoch 2904/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9247 - accuracy: 0.6212\n",
            "Epoch 02904: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9238 - accuracy: 0.6215 - val_loss: 2.0181 - val_accuracy: 0.2223\n",
            "Epoch 2905/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9100 - accuracy: 0.6298\n",
            "Epoch 02905: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9099 - accuracy: 0.6297 - val_loss: 2.0939 - val_accuracy: 0.2054\n",
            "Epoch 2906/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9050 - accuracy: 0.6300\n",
            "Epoch 02906: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9050 - accuracy: 0.6300 - val_loss: 2.0435 - val_accuracy: 0.2274\n",
            "Epoch 2907/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9026 - accuracy: 0.6319\n",
            "Epoch 02907: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9026 - accuracy: 0.6319 - val_loss: 2.0930 - val_accuracy: 0.2065\n",
            "Epoch 2908/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8987 - accuracy: 0.6341\n",
            "Epoch 02908: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.8994 - accuracy: 0.6338 - val_loss: 2.0515 - val_accuracy: 0.2190\n",
            "Epoch 2909/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9117 - accuracy: 0.6276\n",
            "Epoch 02909: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9112 - accuracy: 0.6278 - val_loss: 1.9862 - val_accuracy: 0.2450\n",
            "Epoch 2910/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9081 - accuracy: 0.6304\n",
            "Epoch 02910: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9085 - accuracy: 0.6302 - val_loss: 2.2570 - val_accuracy: 0.1554\n",
            "Epoch 2911/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9055 - accuracy: 0.6317\n",
            "Epoch 02911: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9052 - accuracy: 0.6317 - val_loss: 2.0853 - val_accuracy: 0.1997\n",
            "Epoch 2912/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.6303\n",
            "Epoch 02912: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9048 - accuracy: 0.6303 - val_loss: 2.0520 - val_accuracy: 0.2225\n",
            "Epoch 2913/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8992 - accuracy: 0.6345\n",
            "Epoch 02913: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.8991 - accuracy: 0.6345 - val_loss: 2.0423 - val_accuracy: 0.2131\n",
            "Epoch 2914/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9062 - accuracy: 0.6306\n",
            "Epoch 02914: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9062 - accuracy: 0.6306 - val_loss: 2.0920 - val_accuracy: 0.1990\n",
            "Epoch 2915/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9150 - accuracy: 0.6261\n",
            "Epoch 02915: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9156 - accuracy: 0.6258 - val_loss: 2.0026 - val_accuracy: 0.2530\n",
            "Epoch 2916/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9067 - accuracy: 0.6281\n",
            "Epoch 02916: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9059 - accuracy: 0.6283 - val_loss: 2.0430 - val_accuracy: 0.2216\n",
            "Epoch 2917/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.6284\n",
            "Epoch 02917: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9102 - accuracy: 0.6284 - val_loss: 2.0199 - val_accuracy: 0.2163\n",
            "Epoch 2918/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9081 - accuracy: 0.6293\n",
            "Epoch 02918: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9082 - accuracy: 0.6293 - val_loss: 2.0340 - val_accuracy: 0.2313\n",
            "Epoch 2919/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9039 - accuracy: 0.6299\n",
            "Epoch 02919: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9039 - accuracy: 0.6299 - val_loss: 2.1433 - val_accuracy: 0.1852\n",
            "Epoch 2920/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9062 - accuracy: 0.6298\n",
            "Epoch 02920: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9062 - accuracy: 0.6298 - val_loss: 2.1031 - val_accuracy: 0.2060\n",
            "Epoch 2921/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9011 - accuracy: 0.6328\n",
            "Epoch 02921: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9011 - accuracy: 0.6328 - val_loss: 2.1130 - val_accuracy: 0.1997\n",
            "Epoch 2922/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.6270\n",
            "Epoch 02922: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9134 - accuracy: 0.6270 - val_loss: 2.0548 - val_accuracy: 0.2124\n",
            "Epoch 2923/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.6263\n",
            "Epoch 02923: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9129 - accuracy: 0.6263 - val_loss: 2.1058 - val_accuracy: 0.2066\n",
            "Epoch 2924/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9086 - accuracy: 0.6279\n",
            "Epoch 02924: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9086 - accuracy: 0.6279 - val_loss: 2.1031 - val_accuracy: 0.2011\n",
            "Epoch 2925/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9142 - accuracy: 0.6266\n",
            "Epoch 02925: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9141 - accuracy: 0.6266 - val_loss: 2.1066 - val_accuracy: 0.2090\n",
            "Epoch 2926/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9055 - accuracy: 0.6307\n",
            "Epoch 02926: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9053 - accuracy: 0.6309 - val_loss: 2.0369 - val_accuracy: 0.2251\n",
            "Epoch 2927/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9094 - accuracy: 0.6283\n",
            "Epoch 02927: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9095 - accuracy: 0.6282 - val_loss: 2.0912 - val_accuracy: 0.2044\n",
            "Epoch 2928/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9010 - accuracy: 0.6301\n",
            "Epoch 02928: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9010 - accuracy: 0.6301 - val_loss: 2.0601 - val_accuracy: 0.2092\n",
            "Epoch 2929/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9106 - accuracy: 0.6266\n",
            "Epoch 02929: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9102 - accuracy: 0.6267 - val_loss: 2.0271 - val_accuracy: 0.2187\n",
            "Epoch 2930/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9077 - accuracy: 0.6282\n",
            "Epoch 02930: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9071 - accuracy: 0.6284 - val_loss: 2.0349 - val_accuracy: 0.2210\n",
            "Epoch 2931/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9033 - accuracy: 0.6310\n",
            "Epoch 02931: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9033 - accuracy: 0.6310 - val_loss: 2.1030 - val_accuracy: 0.2009\n",
            "Epoch 2932/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9172 - accuracy: 0.6236\n",
            "Epoch 02932: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9169 - accuracy: 0.6237 - val_loss: 2.0840 - val_accuracy: 0.2088\n",
            "Epoch 2933/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.6277\n",
            "Epoch 02933: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9098 - accuracy: 0.6279 - val_loss: 2.0737 - val_accuracy: 0.2062\n",
            "Epoch 2934/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9062 - accuracy: 0.6300\n",
            "Epoch 02934: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9062 - accuracy: 0.6300 - val_loss: 2.0130 - val_accuracy: 0.2244\n",
            "Epoch 2935/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9117 - accuracy: 0.6278\n",
            "Epoch 02935: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9119 - accuracy: 0.6278 - val_loss: 2.0120 - val_accuracy: 0.2243\n",
            "Epoch 2936/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9060 - accuracy: 0.6305\n",
            "Epoch 02936: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9060 - accuracy: 0.6304 - val_loss: 2.1105 - val_accuracy: 0.2129\n",
            "Epoch 2937/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9023 - accuracy: 0.6312\n",
            "Epoch 02937: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9023 - accuracy: 0.6312 - val_loss: 2.1241 - val_accuracy: 0.1874\n",
            "Epoch 2938/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9044 - accuracy: 0.6312\n",
            "Epoch 02938: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9044 - accuracy: 0.6312 - val_loss: 2.0839 - val_accuracy: 0.2011\n",
            "Epoch 2939/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9070 - accuracy: 0.6293\n",
            "Epoch 02939: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9070 - accuracy: 0.6293 - val_loss: 2.0964 - val_accuracy: 0.2023\n",
            "Epoch 2940/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9082 - accuracy: 0.6274\n",
            "Epoch 02940: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9085 - accuracy: 0.6271 - val_loss: 2.0884 - val_accuracy: 0.1998\n",
            "Epoch 2941/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9073 - accuracy: 0.6292\n",
            "Epoch 02941: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9071 - accuracy: 0.6294 - val_loss: 2.1102 - val_accuracy: 0.2049\n",
            "Epoch 2942/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9050 - accuracy: 0.6316\n",
            "Epoch 02942: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9048 - accuracy: 0.6317 - val_loss: 2.1082 - val_accuracy: 0.1990\n",
            "Epoch 2943/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9059 - accuracy: 0.6291\n",
            "Epoch 02943: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9055 - accuracy: 0.6291 - val_loss: 2.0079 - val_accuracy: 0.2330\n",
            "Epoch 2944/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9079 - accuracy: 0.6290\n",
            "Epoch 02944: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9081 - accuracy: 0.6291 - val_loss: 2.0167 - val_accuracy: 0.2192\n",
            "Epoch 2945/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9007 - accuracy: 0.6346\n",
            "Epoch 02945: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9007 - accuracy: 0.6346 - val_loss: 2.0532 - val_accuracy: 0.2166\n",
            "Epoch 2946/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9044 - accuracy: 0.6306\n",
            "Epoch 02946: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9044 - accuracy: 0.6306 - val_loss: 2.1199 - val_accuracy: 0.1925\n",
            "Epoch 2947/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9050 - accuracy: 0.6303\n",
            "Epoch 02947: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9050 - accuracy: 0.6303 - val_loss: 1.9490 - val_accuracy: 0.2426\n",
            "Epoch 2948/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9058 - accuracy: 0.6320\n",
            "Epoch 02948: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9058 - accuracy: 0.6320 - val_loss: 2.0372 - val_accuracy: 0.2216\n",
            "Epoch 2949/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9041 - accuracy: 0.6301\n",
            "Epoch 02949: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9037 - accuracy: 0.6302 - val_loss: 2.0526 - val_accuracy: 0.2232\n",
            "Epoch 2950/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9143 - accuracy: 0.6265\n",
            "Epoch 02950: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9142 - accuracy: 0.6265 - val_loss: 1.9672 - val_accuracy: 0.2449\n",
            "Epoch 2951/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9073 - accuracy: 0.6288\n",
            "Epoch 02951: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9079 - accuracy: 0.6286 - val_loss: 2.0016 - val_accuracy: 0.2388\n",
            "Epoch 2952/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9325 - accuracy: 0.6185\n",
            "Epoch 02952: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9325 - accuracy: 0.6185 - val_loss: 2.1433 - val_accuracy: 0.1842\n",
            "Epoch 2953/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9171 - accuracy: 0.6255\n",
            "Epoch 02953: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9171 - accuracy: 0.6255 - val_loss: 1.9001 - val_accuracy: 0.2715\n",
            "Epoch 2954/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9157 - accuracy: 0.6245\n",
            "Epoch 02954: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9154 - accuracy: 0.6247 - val_loss: 2.0169 - val_accuracy: 0.2294\n",
            "Epoch 2955/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9049 - accuracy: 0.6296\n",
            "Epoch 02955: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9049 - accuracy: 0.6296 - val_loss: 2.0414 - val_accuracy: 0.2226\n",
            "Epoch 2956/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9027 - accuracy: 0.6327\n",
            "Epoch 02956: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9027 - accuracy: 0.6327 - val_loss: 2.0803 - val_accuracy: 0.2044\n",
            "Epoch 2957/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9072 - accuracy: 0.6290\n",
            "Epoch 02957: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9072 - accuracy: 0.6290 - val_loss: 2.1558 - val_accuracy: 0.1850\n",
            "Epoch 2958/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6281\n",
            "Epoch 02958: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9121 - accuracy: 0.6281 - val_loss: 2.0055 - val_accuracy: 0.2356\n",
            "Epoch 2959/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9039 - accuracy: 0.6320\n",
            "Epoch 02959: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9037 - accuracy: 0.6321 - val_loss: 1.9871 - val_accuracy: 0.2422\n",
            "Epoch 2960/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9005 - accuracy: 0.6343\n",
            "Epoch 02960: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9005 - accuracy: 0.6343 - val_loss: 2.1205 - val_accuracy: 0.1921\n",
            "Epoch 2961/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9007 - accuracy: 0.6323\n",
            "Epoch 02961: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9007 - accuracy: 0.6323 - val_loss: 2.1068 - val_accuracy: 0.2025\n",
            "Epoch 2962/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9018 - accuracy: 0.6327\n",
            "Epoch 02962: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9018 - accuracy: 0.6327 - val_loss: 2.1690 - val_accuracy: 0.1830\n",
            "Epoch 2963/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9081 - accuracy: 0.6289\n",
            "Epoch 02963: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9078 - accuracy: 0.6291 - val_loss: 2.0491 - val_accuracy: 0.2263\n",
            "Epoch 2964/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.6300\n",
            "Epoch 02964: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9092 - accuracy: 0.6300 - val_loss: 2.1293 - val_accuracy: 0.1955\n",
            "Epoch 2965/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8995 - accuracy: 0.6340\n",
            "Epoch 02965: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.8995 - accuracy: 0.6340 - val_loss: 2.0029 - val_accuracy: 0.2347\n",
            "Epoch 2966/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.6267\n",
            "Epoch 02966: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9135 - accuracy: 0.6267 - val_loss: 1.9682 - val_accuracy: 0.2388\n",
            "Epoch 2967/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9061 - accuracy: 0.6293\n",
            "Epoch 02967: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9061 - accuracy: 0.6292 - val_loss: 2.0942 - val_accuracy: 0.2005\n",
            "Epoch 2968/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9076 - accuracy: 0.6269\n",
            "Epoch 02968: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9076 - accuracy: 0.6269 - val_loss: 2.0468 - val_accuracy: 0.2164\n",
            "Epoch 2969/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9098 - accuracy: 0.6294\n",
            "Epoch 02969: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9095 - accuracy: 0.6294 - val_loss: 2.0646 - val_accuracy: 0.2069\n",
            "Epoch 2970/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8994 - accuracy: 0.6339\n",
            "Epoch 02970: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8994 - accuracy: 0.6339 - val_loss: 2.0090 - val_accuracy: 0.2319\n",
            "Epoch 2971/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9125 - accuracy: 0.6271\n",
            "Epoch 02971: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.9125 - accuracy: 0.6271 - val_loss: 2.1456 - val_accuracy: 0.1835\n",
            "Epoch 2972/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9130 - accuracy: 0.6272\n",
            "Epoch 02972: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9130 - accuracy: 0.6272 - val_loss: 2.0916 - val_accuracy: 0.2057\n",
            "Epoch 2973/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9108 - accuracy: 0.6279\n",
            "Epoch 02973: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9108 - accuracy: 0.6279 - val_loss: 1.9965 - val_accuracy: 0.2441\n",
            "Epoch 2974/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9062 - accuracy: 0.6302\n",
            "Epoch 02974: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9061 - accuracy: 0.6302 - val_loss: 2.0117 - val_accuracy: 0.2139\n",
            "Epoch 2975/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9158 - accuracy: 0.6239\n",
            "Epoch 02975: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9158 - accuracy: 0.6239 - val_loss: 2.0525 - val_accuracy: 0.2200\n",
            "Epoch 2976/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9020 - accuracy: 0.6312\n",
            "Epoch 02976: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9016 - accuracy: 0.6316 - val_loss: 2.0654 - val_accuracy: 0.2161\n",
            "Epoch 2977/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9034 - accuracy: 0.6313\n",
            "Epoch 02977: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9034 - accuracy: 0.6313 - val_loss: 2.0928 - val_accuracy: 0.2003\n",
            "Epoch 2978/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9084 - accuracy: 0.6290\n",
            "Epoch 02978: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9082 - accuracy: 0.6292 - val_loss: 2.0281 - val_accuracy: 0.2381\n",
            "Epoch 2979/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9264 - accuracy: 0.6194\n",
            "Epoch 02979: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9264 - accuracy: 0.6194 - val_loss: 1.9755 - val_accuracy: 0.2283\n",
            "Epoch 2980/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9106 - accuracy: 0.6290\n",
            "Epoch 02980: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9106 - accuracy: 0.6290 - val_loss: 2.0877 - val_accuracy: 0.2084\n",
            "Epoch 2981/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9160 - accuracy: 0.6251\n",
            "Epoch 02981: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9160 - accuracy: 0.6251 - val_loss: 2.0063 - val_accuracy: 0.2389\n",
            "Epoch 2982/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8962 - accuracy: 0.6369\n",
            "Epoch 02982: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.8961 - accuracy: 0.6370 - val_loss: 2.0759 - val_accuracy: 0.2072\n",
            "Epoch 2983/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.8983 - accuracy: 0.6345\n",
            "Epoch 02983: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.8988 - accuracy: 0.6344 - val_loss: 2.1026 - val_accuracy: 0.1974\n",
            "Epoch 2984/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9068 - accuracy: 0.6282\n",
            "Epoch 02984: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9068 - accuracy: 0.6282 - val_loss: 2.1283 - val_accuracy: 0.1979\n",
            "Epoch 2985/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9038 - accuracy: 0.6308\n",
            "Epoch 02985: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.9045 - accuracy: 0.6304 - val_loss: 2.0596 - val_accuracy: 0.2055\n",
            "Epoch 2986/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9099 - accuracy: 0.6274\n",
            "Epoch 02986: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9099 - accuracy: 0.6274 - val_loss: 2.1244 - val_accuracy: 0.2066\n",
            "Epoch 2987/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9065 - accuracy: 0.6304\n",
            "Epoch 02987: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9062 - accuracy: 0.6305 - val_loss: 2.0937 - val_accuracy: 0.1992\n",
            "Epoch 2988/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.6273\n",
            "Epoch 02988: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9102 - accuracy: 0.6273 - val_loss: 2.0308 - val_accuracy: 0.2268\n",
            "Epoch 2989/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9023 - accuracy: 0.6321\n",
            "Epoch 02989: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9019 - accuracy: 0.6325 - val_loss: 2.1124 - val_accuracy: 0.1939\n",
            "Epoch 2990/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9003 - accuracy: 0.6323\n",
            "Epoch 02990: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.9003 - accuracy: 0.6323 - val_loss: 2.2552 - val_accuracy: 0.1405\n",
            "Epoch 2991/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9076 - accuracy: 0.6290\n",
            "Epoch 02991: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.9076 - accuracy: 0.6290 - val_loss: 2.0360 - val_accuracy: 0.2303\n",
            "Epoch 2992/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9032 - accuracy: 0.6308\n",
            "Epoch 02992: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9032 - accuracy: 0.6308 - val_loss: 2.1179 - val_accuracy: 0.1906\n",
            "Epoch 2993/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9031 - accuracy: 0.6319\n",
            "Epoch 02993: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9032 - accuracy: 0.6319 - val_loss: 2.2294 - val_accuracy: 0.1653\n",
            "Epoch 2994/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9063 - accuracy: 0.6282\n",
            "Epoch 02994: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9063 - accuracy: 0.6282 - val_loss: 2.0484 - val_accuracy: 0.2180\n",
            "Epoch 2995/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9021 - accuracy: 0.6331\n",
            "Epoch 02995: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9021 - accuracy: 0.6331 - val_loss: 2.0281 - val_accuracy: 0.2166\n",
            "Epoch 2996/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9076 - accuracy: 0.6278\n",
            "Epoch 02996: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9073 - accuracy: 0.6279 - val_loss: 2.1248 - val_accuracy: 0.1947\n",
            "Epoch 2997/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9034 - accuracy: 0.6309\n",
            "Epoch 02997: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9034 - accuracy: 0.6309 - val_loss: 2.2576 - val_accuracy: 0.1467\n",
            "Epoch 2998/3000\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.9095 - accuracy: 0.6271\n",
            "Epoch 02998: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9096 - accuracy: 0.6270 - val_loss: 2.1081 - val_accuracy: 0.1962\n",
            "Epoch 2999/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.9012 - accuracy: 0.6312\n",
            "Epoch 02999: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.9012 - accuracy: 0.6312 - val_loss: 2.0821 - val_accuracy: 0.2093\n",
            "Epoch 3000/3000\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.8994 - accuracy: 0.6352\n",
            "Epoch 03000: val_accuracy did not improve from 0.39172\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.8994 - accuracy: 0.6352 - val_loss: 2.0398 - val_accuracy: 0.2151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDSyKZrgXdaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d57bc2b-8163-445e-c868-161c2daa37ef"
      },
      "source": [
        "score = model.evaluate(x_test,y_test,verbose=1)\n",
        "print(\"\\n\")\n",
        "print(\"Test loss:\",score[0])\n",
        "print(\"Test accuracy:\",score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "461/461 [==============================] - 1s 2ms/step - loss: 2.0110 - accuracy: 0.2231\n",
            "\n",
            "\n",
            "Test loss: 2.01100754737854\n",
            "Test accuracy: 0.22305186092853546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhG1k3kVXkO9"
      },
      "source": [
        "def plot_history(history):\n",
        "    # print(history.history.keys())\n",
        "\n",
        "    # 精度の履歴をプロット\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['accuracy', 'val_accuracy'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # 損失の履歴をプロット\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['loss', 'val_loss'], loc='lower right')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gyxnkxyXlr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "f738cb3b-e090-4951-e20d-3cd4c4073dda"
      },
      "source": [
        "# 学習履歴をプロット\n",
        "plot_history(history)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3xUVfbAvyedUEIg9BZAeu8IKFhQFETURayrqNgWxbb2wqrrz7XrrrqyLpa1oKKryCKKCopSNPTeew0loYa0+/vjzWRKprwkM5kkc76fz2Teu+/e986bzNzz7j3nniPGGBRFUZToJSbSAiiKoiiRRRWBoihKlKOKQFEUJcpRRaAoihLlqCJQFEWJclQRKIqiRDmqCJSoQkTeFZGnbdbdKiLnhlsmRYk0qggURVGiHFUEilIJEZG4SMugVB1UESgVDseUzJ9FZLmIHBeRf4tIAxH5RkSOisj3IpLqVn+kiKwSkSwRmSMiHdyO9RCRxY52nwBJXtcaISJLHW3niUhXmzIOF5ElInJERHaIyESv44Mc58tyHL/eUV5NRF4UkW0iki0ivzjKhojITh+fw7mO7YkiMlVEPhCRI8D1ItJXROY7rrFHRP4hIglu7TuJyCwROSQi+0TkYRFpKCInRKSuW72eIpIpIvF27l2peqgiUCoqlwFDgbbARcA3wMNAPazv7Z0AItIW+Bi4y3FsBvC1iCQ4OsUvgf8AdYDPHOfF0bYHMBm4BagLvAVME5FEG/IdB/4I1AaGA7eJyCjHeVs45P27Q6buwFJHuxeAXsAAh0z3A4U2P5OLgamOa34IFAB3A2nA6cA5wO0OGWoC3wMzgcbAacAPxpi9wBzgcrfzXgtMMcbk2ZRDqWKoIlAqKn83xuwzxuwC5gILjTFLjDE5wH+BHo56Y4D/GWNmOTqyF4BqWB1tfyAeeMUYk2eMmQr87naNm4G3jDELjTEFxpj3gFOOdgExxswxxqwwxhQaY5ZjKaPBjsNXAd8bYz52XPegMWapiMQANwATjDG7HNecZ4w5ZfMzmW+M+dJxzZPGmEXGmAXGmHxjzFYsReaUYQSw1xjzojEmxxhz1Biz0HHsPeAaABGJBa7EUpZKlKKKQKmo7HPbPuljv4ZjuzGwzXnAGFMI7ACaOI7tMp6RFbe5bbcA7nVMrWSJSBbQzNEuICLST0RmO6ZUsoFbsZ7McZxjk49maVhTU76O2WGHlwxtRWS6iOx1TBc9Y0MGgK+AjiLSEmvUlW2M+a2UMilVAFUESmVnN1aHDoCICFYnuAvYAzRxlDlp7ra9A/irMaa22yvZGPOxjet+BEwDmhljUoB/As7r7ABa+2hzAMjxc+w4kOx2H7FY00rueIcKfhNYC7QxxtTCmjpzl6GVL8Edo6pPsUYF16KjgahHFYFS2fkUGC4i5ziMnfdiTe/MA+YD+cCdIhIvIpcCfd3a/gu41fF0LyJS3WEErmnjujWBQ8aYHBHpizUd5ORD4FwRuVxE4kSkroh0d4xWJgMviUhjEYkVkdMdNon1QJLj+vHAo0AwW0VN4AhwTETaA7e5HZsONBKRu0QkUURqikg/t+PvA9cDI1FFEPWoIlAqNcaYdVhPtn/HeuK+CLjIGJNrjMkFLsXq8A5h2RO+cGubAYwD/gEcBjY66trhduBJETkKPI6lkJzn3Q5ciKWUDmEZirs5Dt8HrMCyVRwC/gbEGGOyHed8G2s0cxzw8CLywX1YCugollL7xE2Go1jTPhcBe4ENwFlux3/FMlIvNsa4T5cpUYhoYhpFiU5E5EfgI2PM25GWRYksqggUJQoRkT7ALCwbx9FIy6NEFp0aUpQoQ0Tew1pjcJcqAQV0RKAoihL16IhAURQlyql0gavS0tJMenp6pMVQFEWpVCxatOiAMcZ7bQpQCRVBeno6GRkZkRZDURSlUiEift2EdWpIURQlylFFoCiKEuWoIlAURYlyVBEoiqJEOaoIFEVRohxVBIqiKFGOKgJFUZQoRxWBoihKBean9ZlsO3g8rNeodAvKFEVRAlFQaNh5+AQt6lZnd9ZJDh3PpXOTFAoLDSLgmbDO1SY2xlX+8W/bObNtPZrUrgbAiL/PZVinhvy0PpPftx7myz8NpFvTFN6bt5XOTVLo1qw28bExrNyVzX/mb6NL0xS6N6vNT+szef7bdTwwrD2FxvD1st2s3XuUNU8O4+5PlnLveW1pXLsaD/93BTcNakWXpinMXref/UdyWL37CCLCu/O2ApAYF8NfL+nCH3o1DflnVumCzvXu3dvoymJFKT+O5uQRFxNDtYRY5m08QPfmtUlOsPcMaYzx6Hhz8goASIqPpaDQ8OPa/WRsPcStg1uTWj2BZTuyAOjaNAURIb+gkFP5hSQnxHL0VD4fLNjGZT2bMnfDAbo3S+Hcl35m/FmncVmvpsxcuZc56/ZTLSGWOesymX3fEM56YQ4Ar13Zgzs/XkL9moncMKglz36zltNb1aV5nWQ+yXClgq5TPYGR3RoXdb6X927KpxnB8gOFjsS4GE7lF/o9/vKYblzSo3SKQEQWGWN6+zymikBRKgfGGJbuyKJNg5os35FFr/RUnpu5jlvObEWNpLiizvlUfgGb9h/HYGicUo3NB46TmhxPet3qxLg99X6asYOcvAJqJyfQvWltmtdNZuWubKYu2sk1/Ztz7ks/F9WtFh/Lh+P6cekb8+iTnsro3s3o2TyVnLwCpi3bzbX9W7ByVzYTpizlqn7NOZlbQF5hIV8s3gXA9DsG8Y8fNzJz1V4AmtdJZvuhE7bvvU71BA4dzw3Fx1ipeeKijowd2LJUbVURKEoYyMkr4J1ft3LTGS3JzS/kZF4BaTWsNMO/bz1Ep8a1iBHhSE4eqckJxMe6THJvzNnIJT2akJtfSObRU6TVSGTepoNsyjzGgxe0Z/ry3bw3bxtTbu7Pm3M28eoPG4LKs/XZ4XyasYP7py4P2z0rkeXNq3tyQZdGpWqrikCJCgoLDZnHTtGgVpLfOrn5hcQIxIhw4Pgp6tf0rHvg2Clmr93P6N7NAMgvKGRX1kkGPz+HBrUSmXx9HyZOW8Ujwzsyfdlu3v5lCy3qJpOXX8ju7Bz+fH47nv92XVjv0x/pdZPZetD+U7ZSuXjtyh5c1LWRTxuHHVQRKFWGE7n5/LrxIEM7NuDYqXwS42I4fiqf0f+cz4b9xwCYe/9Z7DuSw8e/7eDzxdb87h9Pb0Hd6om8/P36SIqvVCIa1Epk35FTPo/9785BDH/tF5/HFj82lAPHTnHey9bU2oMXtOfZb9YWHd/67HAWbD7IFZMWFJXFxQgbn7mQjxZup3+rOpz94k8AjO7VlM8W7WTcGS15ZHjHMt1PIEWgXkNKyDmRm0+MCAmxMR5z0gWFhl2HT7Jkx2Hq10ziyn8tYMrN/amRGMdrP2zgu9X7aFWvOved147uzWozb9NBTuTm8/hXqwB469pe3PKfRUGvf8Zzs4uVvT/fbwRepRy5bUhr3pyzyXb9To1rMf6s03j0y5U8MrwDPZun8tFv27luQDoDn/2xWP2r+jXno4XbgxpdAWbedQat0mqwcnc2l74xj14tUvnLyE5MnLaK3IJCnrmkCyP+7ruzb5lW3WN/67PDOZVfwI5DJ6lTPYGkeNc04K2DW3N572b0fGpWUVmf9DqM6t6Ym85oRXJCLDUS44rkd5JeN5nrBqTz2aKdnN2+QfAPqwzoiEApFfkFhew/eoqEuBjW7ztKraR48gsN3ZvVJv3B/xWrf077+vywdn8EJK0a3H1u2xKPZp4a1ZnHvlwJQI/mtVmyPYtXr+jOhClLg7b9evwgLvqH707QHxv/egEn8gpIiI3hrilLiwzDjVKS2JOdA1gd5tIdWRQUFpKanEBifCwNaiayZs9Rfli7j1e+t2whPZvX5pNbTvewq3hz/FQ+v2w84PFwkPHouXy1dDf1ayZyx8dLAHhgWHtW7s6meZ1k3pyziSa1q/Hrg2cHvZ8VO7P9fgabn7mQx6et5IMF24vuyx1jDC0fmsG1/Vvw1KjOZJ/Mo9tfvqNGYhwr/3J+0Gsv2naY9LrJ1HXYnEKBjgiUMrNsRxZr9hzhwS9WcHqrulRLiOVHHx37yG6NfbavSkqgW9MUTuQWFE1FlaaTLim3n9U64DV+f+Rc+vz1+6L9OtUT+EPPpkWKoH3DWizZnkVqcgLzHzqb+NgYhr0ylzPbpvHXUV148bt1bMo8xux1mQB0aZrCsifOY9vB43y/Zj8Hjp3io4Xbi85/79C2vDhrPYlxMXx715n8tuUQcbEx1HJ03Ff2a87MVXv55Ob+9GieSttHvylq271Z7WLyd2maQpemKWSfzGP+poN8cfvAoJ9J9cQ4zu/UkMWPDeWp6avJPplHWo1EbhzUkm9W7Cmqd9uQ1gD834w1AFzTv0XQc4M1GrllcCuuOz2dS9+Yx94jOcTFCPmFhpgY4elRXXh0eEdO5hYUaysirH/6AuIcI+IaiXG0b1iTu85ta+vavVqk2qoXKlQRRCFO3+78gkJiY4S+z/zAdae34LoB6WRsO8xp9Wpw+Vvzufe8dtw/dRn/vq4PY9/9vaj9/M0H/Z572rLd5XELESFGYPP/uZ78nCOf0+rXKCr74vYB3Px+BgeO5fLcH7rSok4yY9zmgv3RvE4y793Qt8jvHeC+89oWLWCKiwlsIKxVzfOnvPixoR77jw7vQMfGtTijTVqRsTHj0XNdx0dY889/+mgxuw6fBCClWjxdm9ama1Or406rnsBrP27kr5d0pnPjFF6ctZ6PxvUnPa066V5TJYPb1vN4Sm7XoGYxGX3xxEWdgtbxpk71BF4e092jrLFjIdi9Q10db36hNfsR7LN0EhMjPHRBB4+208YP4mReflGdpPhYkuJjfbZPiHONZmJjhJl3nWnrupEgrIpARIYBrwKxwNvGmGd91LkcmAgYYJkx5qpwyhRtLNp2mI8Wbqdj41q0SqtO9sk87vpkKfed15YXvnM9Yb7w3XqPfYD7PlsG4KEEypPrB6QXLewpCTPvOoNhr8y1Xf+ly7txz6fLipUPOi2NV6/oTnxcDC99t56HLmzvs72zX6mZFEfP5qnMunswq/ccYeBpaR71nNMzT1zUkUGnpTH0ZZef/l9GdiLWzRskMS6G8We34fqBLdl1+CQiwpd/GkiLOslkbDvMidx8jyme+JgYzmiTxtwNB/h4XP+i8qT4GIZ2bEj1xDiutfEk/PpVPf0eu+e8dtxzXruife/pkEB8e3f5doLdmtVm+h2D6NCoVlFZgaMzj7GpCNwpKLTsDQ1qJVK3Rq0gtSsfYVMEIhILvA4MBXYCv4vINGPMarc6bYCHgIHGmMMiUj9c8lQ1Vu8+wob9R7m4exMOHjvFku1ZdG6SwmcZO9idfZKpi3Zy2+DWvPbjRgA+X+zZ3rvTjxT+3C2XPX4eb/+yuWj/i9sHcOkb8zzqvDymG3d/UrwDb9egpsf+2qeG0f6xmR5lnRrXYtXuI6TXTebSnk25tGdTTuUX0O5Rq96XfxroMYUxcWTxJ1Vnx+t8wnZ2L6nVEzyUwJtX9+TzxTvJPGYtiOrerDZtGtQs6khz8gpIio/1iCfjfHqskRhHu4Y1i9oBDO1oGQ5HdmtMQaHhaE4+MTHC5Ov7sHDzIU5vXdft3i8oJne00LlJisf+6N5NeXfeVoZ2KLnhNTkhjsMnrBXWVZFwjgj6AhuNMZsBRGQKcDGw2q3OOOB1Y8xhAGNM1ZlIDhMfLtzGgNZpXPia9cT7zIw1fl3cnEog0lzYpSEzVuz1eezGQS2LKYKv/jSQlOT4ov3rB6TTtUkKV/VrTv9WdWnboAbV4mNpUbc6I7s14c9TlxWtYH3ioo7F/KwTYmNomVadLQeOF3Xej4/oyJhJCzzqJsa5hvi+5rG9ef4P3fj3L5vpkx54PveCLo24oEsjVu3O5m8z19GxsecTpXNqwfHASnrd5GJeKb4QEeJihdTqCQDEx8YwqE1akFbRS6fGKSUaxbjz4U39mLV6n8f3sioRTkXQBNjhtr8T6OdVpy2AiPyKNX000Rgz06sOInIzcDNA8+bNvQ9XOXZnnaRRShIiwh0fL6Fl3WQucsQ/+dDNYAf4VQLhJjkhlhM+jGSPj+jIk9Ndun7tU8NIjIshJ6+QRdsOs3J3Ni/NWk+uw7XP2yvkgxv70c2rE65TPYG42BieuaRLsevFxggvXd6di7s3oVPjWkUre92JiRHSaiSw5cBxxp91Gu+N7cuq3Uf83psz0FgwGqYk8YibsXBY54YB63dqnML7N/T1e9w5dx1KTxElNKSnVWfcma0iLUbYiLSxOA5oAwwBmgI/i0gXY0yWeyVjzCRgEljuo+UtZHny8/pM/jj5Nyac04bdWSf52mF8rShP907+dlnXIvc8d24Y1NJDETifdqslxDKoTRqD2qTxisP7Jb1uMrExwjvX9+HtXzbz6hU9PDpy53bdGglB5Rnctp7Pcu8nQBEhJkZwDgS8Z4tn3HkGjVL8r0z2RbWEWH57+JyiJ/PS0qxOMk+P6sx5ncLrM64o3oRTEewCmrntN3WUubMTWGiMyQO2iMh6LMUQGetkBPlhzT5ufM+1PsJObJlQ8ejwDny3eh+/bTkEWIbLJ6ZZi7iWTzyPrhO/Ayzf7vo1k5i5aq9HyN5vJpzBBa/aN846+dBh1DyrfX3Oal/cPHRN/xakVIv365IaiB/vHVy0SMeSPZXftx4mzVupeGkC72kbu9QPENaiJNh1bVSUUBJOy8fvQBsRaSkiCcAVwDSvOl9ijQYQkTSsqaLNRAnGGLJP5vH4Vys9lEBZ+eBG7xk431RPsJ7WR3ZvzL/+6FpnYozhqVGdeWdsH2olueZEv7h9IAWOBYixMcIAh1GyQ6NavHN9n6Lr/uOqHtROjueSHk18Xte5hjE2SMyU2BhhVI8mpfLyaFWvhkfn/Ofz2/HNhDNoVc9y9fQ3IlCUaCRsIwJjTL6IjAe+xZr/n2yMWSUiTwIZxphpjmPnichqoAD4szHGv5N6Jaag0HAiN587P17CqB5NbK3uLC2D2qSx9dnhZJ3I5a5PlnJl3+Y+QzMMPC2NSW4K4Nu7zmTU679yXqeGRX7YAK9e0Z3Wjg7U6YIXK8L7N/QtUgzuT/QjujZmRFf/T/FORVCeDhhxsTEeroTiUAGlDeClKFWJsNoIjDEzgBleZY+7bRvgHseryjL2nd+KVmwCHtsl5a1re/HT+kyPVZ73DG1L9cQ4npq+2qNu7eQE3h3raZx0Dx0Q49UJtmtYkzVPDSt2zYu7u57s/zKyE9UT4xjUJo242JhSfYEMLmUSKXREoCguqqZTbIQ5fiqf7BN5fLlkF+kP/q9MHb87U289nfM7NaSHl1dNbIxw4yB7ySq6NHX5VpfmibxZnWT+fmUPv6sp7VA0NVSKKZ9QowMCRYm811CVIr+gkH/M3lgUOKukPHxhe56ZYYWr/fSW03ly+ipW7jrC7PuG0LBWEtXc5vTX7zvKsVMFfPzb9qIn+1b1qrM503eS64TYGHILPKMxRmpapNCUfoVnqBEdEyiKKoJQsWjbIW75z2IOHAvs159SLZ7sk3nFyjc9cyECRYqgb8s6TL/jDJ/nSIyL5ZHhHfnr/6ypIKcr/v/uOINT+cV9+wGWPD60qAN+74a+XDf5N3o1L9/AVk6c/r+RnBpyjkp0RKAoqgjKTGGh4bUfN9geBVzUrRG1qyXwj9kbPWLplGaaxLnwqmMja7qnWkJs0ajBm+purpSD29bjx3sH21q9Gg4qwtSQoUovR1GUEqGKoIz8uumALSVw+5DW1KoWz7X9WzB1kZU1q0ntaqQmxxfrkM+wGSZgRNfGdG9Wm6apySWW2+lGGUm8jdXliWtEoEMCRVFFUAb+OPk3fl7v3xBcKymO3x89l73ZObSo6+rsr+rXnLyCQv54enqxZetLHhtKcqJ9Q2xplEBFoUIYiyMtgKJUAFQRlJKXvlsXUAkAPD+6G4lxsR5KAKz4Ojed4TtuSVnDFFQmIqkH1EagKC7UfbQUbDlw3Gfsnz7pqUW5Su8Z2pbzOwUOQhatfDyuP6N7NY3otIzTRqCKQFFUEZSYzZnHPLJIOXnmki58dusA+ra0wi64++srnpzeui7Pj+4WURmKRgQ6OaQoqghKwqHjuZz94k8+j13gCEFcWMJ0eEpkcPoM6YhAUdRGUCJues8zKOrc+89i3d6jdG2WUjS3XxQ+QRVBhcY4hgT6X1IUVQS22X80h8XbPdIk0KxOMs3qeHrt/O2yrvzzp030Ta9TnuIpJaRoFYEOCRRFFYFdxnmFiR7V3Xd0zaapyTw9qngmLaVi4bIRKIqiNgIbzN2QybKd2R5lL4/pHiFplNCgXkOK4kQVgQ2mL9vjsS+iK1IrOzoiUBQXOjVkA2fO3Nb1qnN1vxZc3qdZkBZKRcflNaSqQFFUEQTBGMMbczYB8M2EM0mI00FUVUBHBIriQhVBEHYcOlm0rUqg6tC9WW3O79SA+85rF2lRFCXiqCIIQqYjv8CEc9pEWBIllCTExfDWtb2DV1SUKEAfcYPwh3/OA2Bwu3oRlkRRFCU8qCIIgnMuuXVa5OP3K4qihANVBDZJSY6PtAiKoihhQRVBALJPWLmFrx+QHllBFEVRwogqggBc985vACTF288YpiiKUtlQRRCApTusIHNZJ3IjLImiKEr4UEVggx2HT0RaBEVRlLARVkUgIsNEZJ2IbBSRB30cv15EMkVkqeN1UzjlKS1/Pr99pEVQFEUJG2FTBCISC7wOXAB0BK4UkY4+qn5ijOnueL0dLnlKgzPrWPdmtSMsiaIoSvgI54igL7DRGLPZGJMLTAEuDuP1Qs6q3UdIVbdRRVGqOOFUBE2AHW77Ox1l3lwmIstFZKqI+AzrKSI3i0iGiGRkZmaGQ1afbD90gsMOF1JFUZSqSqSNxV8D6caYrsAs4D1flYwxk4wxvY0xvevV01APiqIooSScimAX4P6E39RRVoQx5qAx5pRj922gVxjlKRHO5OaX9vA1iFEURak6hFMR/A60EZGWIpIAXAFMc68gIo3cdkcCa8IoT4k4mVcAQNuGNSMsiaIoSngJWxhqY0y+iIwHvgVigcnGmFUi8iSQYYyZBtwpIiOBfOAQcH245Ckpx07lA1A9USN1K4pStQlrL2eMmQHM8Cp73G37IeChcMpQWo7lWIqgpioCRVGqOJE2FldYnN5CKdXUfVRRlKqNKgI/7M3OAaBR7aQIS6IoihJeVBH4YXeWlau4Ua1qEZZEURQlvKgi8MOurJPUSIyjVjW1ESiKUrVRReCHwydyqVsjARGJtCiKoihhRRWBH75auptTeYWRFkNRFCXsqCLwQX6BpQD2HsmJsCSKoijhRxWBD7JPaqA5RVGiB1UEPshyKIJR3RtHWBJFUZTwo4rAB1mOxWQXa8A5RVGiAFUEPsg+aSWrr62rihVFiQJUEfhg+rI9ANROToiwJIqiKOFHFYEPvlhipU1oWEvDSyiKUvVRRRCAagmxkRZBURQl7Kgi8OJkrpWQZkTXRkFqKoqiVA1UEXix9eBxAM7r1DDCkiiKopQPqgi82HrAUgQt61aPsCSKoijlgyoCL7Y4RgTpackRlkRRFKV8UEXgxdYDx0mrkUDNJF1DoChKdKCKwIutB06QrtNCiqJEEaoIvNhy8DjpaaoIFEWJHlQRuHHsVD6ZR0/RUhWBoihRhCoCN4o8hlQRKIoSRagicMO5hkBtBIqiRBOqCNxwjgjUdVRRlGjCliIQkS9EZLiIlEhxiMgwEVknIhtF5MEA9S4TESMivUty/lCz5cAJGtRKJDkhLpJiKIqilCt2O/Y3gKuADSLyrIi0C9ZARGKB14ELgI7AlSLS0Ue9msAEYKFtqcPEtoPHdVpIUZSow5YiMMZ8b4y5GugJbAW+F5F5IjJWRPytvOoLbDTGbDbG5AJTgIt91HsK+BsQ8UzxWw8eV0OxoihRh+2pHhGpC1wP3AQsAV7FUgyz/DRpAuxw29/pKHM/Z0+gmTHmf0GufbOIZIhIRmZmpl2RS8TRnDwOHMvVNQSKokQddm0E/wXmAsnARcaYkcaYT4wxdwA1SnNhh73hJeDeYHWNMZOMMb2NMb3r1atXmssFZefhkwA0S1VDsaIo0YVdq+hrxpjZvg4YY/wZeHcBzdz2mzrKnNQEOgNzRASgITBNREYaYzJsyhUyvl+9D4D6tRJDd1JjYMN3cNpQiFEHLUVRKiZ2e6eOIlLbuSMiqSJye5A2vwNtRKSliCQAVwDTnAeNMdnGmDRjTLoxJh1YAERECQDsPWKZKNrUL9UAxzcrP4ePLoff3w7dORVFUUKMXUUwzhiT5dwxxhwGxgVqYIzJB8YD3wJrgE+NMatE5EkRGVlagcNFbn4h9WsmhjZhffZOx/v20J1TURQlxNidGooVETHGGChyDQ3aYxpjZgAzvMoe91N3iE1ZwsLmAyEMNndoM6yZDhhHgYTmvIqiKGHA7ohgJvCJiJwjIucAHzvKqgTGGNbvPUq7BjVDc8KpN8KsxyDLMRKQIIrgyB44tCU011YURSkhdkcEDwC3ALc59mcBVWbie092DkdP5dO2YYgUQd4J6z33uL36L7W33idmh+b6iqIoJcCWIjDGFAJvOl5Vjk2ZxwA4rV6IDMXGeBXo1JCiKBUXW4pARNoA/4cVKiLJWW6MaRUmucqVTzMso27j2klBatrFoQicIwN3co7A8Uyo2zpE11IURSkbdm0E72CNBvKBs4D3gQ/CJVR58/Wy3QA0qBUiReAcEaz52nrf8hMUFlrbk4fB33uG5jqKoighwK4iqGaM+QEQY8w2Y8xEYHj4xCo/TuUXFG0nxceG6KxeU0O7l8CTqXD8AOxfFaJrKIqihAa7xuJTjpAQG0RkPNYK4RCuvIoc8zcdBODhC9uH7qTFbAQOsnf4LlcURYkgdkcEE7DiDN0J9AKuAa4Ll1DlyY9r9wNwZttQxjDyowgK8j33j+yBvStDeF1FUZSSE3RE4Fg8NsYYcx9wDBgbdqnKkd+2HAKgfcNaoTupvxHBb2951nm5E5gC33UVRVHKiaAjAmNMATCoHGQpd/ILClm792j5XfDIbjOnEAkAACAASURBVNd2YYEqAUVRKgR2bQRLRGQa8BlQtErKGPNFWKQqJ4a/9gsAV/VrHuIz+xkRuI8UCvN911EURSln7NoIkoCDwNnARY7XiHAJVV6s22eNBkZ0bVSyhruXwMks137uCVg2BSamwJ7lcHirn4ZuiiD/ZMmuqSiKEibsriyuUnYBb3q1SC1Zg0lDoGFXuHWutf9cS8h3ZNrMmOy/3fb5ru1pd/quc/wAnDoCdRxr9VZ/ZZX1udGzXmEh5GRBch3f5zlxCOKrWS9FUZQA2F1Z/A4+5juMMTeEXKJyYsVOK67P0I4NSIwrxfqBvctd2/lu6Zb3r7HXfs003+UvtrOmjZxxhz79o/XurQjmvgizn4Zbf7HCXbe7wPP4cy2hfke4fT6KoiiBsDs1NB34n+P1A1ALy4Oo0vLtqr0A9GheO0jNErJjQdnaB7MdnDoG8/7uUiT/PAM+vgJ+/KurzoGN1vv+1W7nLbSmrua+WDb5FEWpctidGvrcfV9EPgZ+CYtE5cS+IznUTo7ntsEljPnjzzU0FOTleO57X+voPpj9V1j8nnsl6+3n5+DsR6ztfw8tfu6CXOt9zrNwRtA00YqiRBGlTaTbBqgfSkHKm3X7jtK5cQoSLFeAN2FVBF5B6ha84drOyYYX23opAR8YAycPBahQASOh5p+Cn54rrggVRSkX7NoIjuJpI9iLlaOgcmEMiJCTV8DyndncWtLRwMnDIG668y+pcI9Nm4Adnmvpub/0I9f2szZcXCemFC/bsxyq1baMznbJ3mXdZ60SelOVlt8mWSOdmFgdrShKBLA7NRSijC0RZPMceP9iGL+If6+witJqlDA/8d/SIT7ZtW8KLeNuOFj2CewLQfiJt87wf2zqDdB+BHS+1LP85Y7Wu79EOTMfBgwM+7+yyweuBD75p0JzPkVRSoStqSERuUREUtz2a4vIqPCJFQZWOta+7crgwDGrwxndq1nJz+Mrx0A4+O/N4TmvczXzsUxY+TlMtekZvGUuvN4f8k7Cgtc9p63KLJMjRLeUdqYyTLxxOvzwVKSlUJSwY/eX94Qxpujx0BiTBTwRHpHCxIA7rHeJZXfWSVrXq05KcnxkZYoEhfnWuoTdSzzLl3wAa6b7b/feCMhcA4veLfk1Zz0BCycVX2ORe8Kazlo2xdr3VgTrvoE9y4qfr7AQvp4A+8Ic0nv/apj7QnivoSgVALshJnwpDLttKwhOI6lhy4HjtApVWsrKyPpv4avbXfur/gtf/cna9jUd5G4gd18zkZ8LIhAbRKH++opru7fb0pOje6z3rG2OAi9D9sdX+JYpa5ulkDbNhruWoyhK2bA7IsgQkZdEpLXj9RKwKJyChRyHd1BhYSFbD56gVVr1CAsUQdyVAMBn17u2131TvP6yj13b3090bT9dD17v51n31DFrxHEsM7gc3h5YgTy4crJhtWPthHPk4JxSUhSlTNh9qr8DeAz4BMt7aBbwp3AJFU4OHz9Fbn4hrepFsSIIhPMpHGDGn62V0u4Gcm8ObbI69O8ehV5jLYP8kZ2u49cEikvorQhirA6/sKB46IzPx8GGb+HOpa4RiCoCRQkJdr2GjgMPhlmW8OJ42tyTbU1tnFY/iqeG7PLbJHv1Dm2G+f+wXt584OWR5HDhZcdv8KOXIfboHvhbS8ug7T4dZIx1DbAWxsUlOsqDKIK8HMvlN/c4fPgHuPE7qBFg+cvJw7Djd2h7XuDzRgPGWCO7GqFM2KRUVOx6Dc0Skdpu+6ki8m34xAof2w8dJy5G6NjIh8/9qi9h+WflL1Rl5+897dd1dt6Th8GWnz2P/TbJ5dW0bZ6rfN5rcHCDtS2xFNkSvBXBoS1WsD0nn1wNL7WH+X+Hw1tgzdeBZfvkWvhoNBw/6L/OvlWBjzs5sgdWTA1er6Iy/3V44TQ4uCnSkijlgF0bQZrDUwgAY8xhbKwsFpFhIrJORDaKSLERhYjcKiIrRGSpiPwiIh3ti15SrM7j4NEcmqRWo1qCV6C5rO3w2XXwxU3hE0FxxVIK5ir6jlsQvVmPu7bd7QiFBZb30MQU66n/te7wjz6u4xu/d9UDa8GaP/JzYasjmmxBgPUMbw6wXsH4zyj4/EY45Zb4yBjL/baisvVXa1QEsMLxQHR4S+TkUcoNu4qgUESKlraKSDp+s68U1YkFXgcuADoCV/ro6D8yxnQxxnQHngNesilPyXF0IIdP5NI01Udo5le6hO3Siht2FYE/JAY2/Whtnzjgcmd1Zn87ccAaFcx5tvg1nUrD1xP9hu9c294jjbUzrBGK8wn/2N7gcjrlMYXWKOKVLtbq6b82tGJGOTm23zK4+81hUUK2L4Tlnwavt3cl5Bxx7eefgncvhA9HW/t7ljrkD41YSsXGrrH4EeAXEfkJ69H6DCDYiqe+wEZjzGYAEZkCXAwUhcQ0xrh9E6lOWL92liI4cSqfutUTA1f9/CYYdDc06FT2y8YlebpcRjuFBVZHHeipOxASU9zryXleJ96hOpwB95w83wr+9JtlmP73ULhrhedI48AGl00CYO10WPqhf5mWfwrN+kFqi+LH3EOD/Py89Z61DWo2cLXNXAsL3wrNSu3JDvtG18sD1/vnQGjaF26aZe07P7+9XqvZ1SAfFdh6LDPGzAR6A+uAj4F7gWBj3CbADrf9nY4yD0TkTyKyCWtE4DNbi4jcLCIZIpKRmWnDLdH3SQDILyigWnyQ/AMrPrM3/LfDjd8FrxNNLHijeEddEvy5mDqngXzhK3TFiqmu0cRm5/ONg/+Mgvcucr+o/3PvzIAvxsGrXV1lBXmBgxMW5BUvW/AGLPin/zbhYOdvwetUBkUwMQWm3RFpKSo1do3FN2HlIbgXuA/4DzAxFAIYY143xrTGCmL3qJ86k4wxvY0xvevVK60Xg1MRFJIUX4ppic/Hle6yddsULzvrUWg3vHTnq+zMKeNT7+xnfJd/94j/Nt4jArA6uE2zre2Y2MBrGPatsH/u4wfhqTTIPeq7PlhTVUf3OaZm3BTGzAfsR7f9+XlY4jZKWfIBHLUxZeUXx3XzT3qe5/e3oaAS5Nde/H6kJajU2O0RJwB9gG3GmLOAHkBW4CbsAtyD+TR1lPljChC++EWOH3peQSFJ3oZifzjnkn99FVbYmHf1RYyP2bfBf4YrP4LOl1n7A/Rpxjbui9vs4mtEsHY6HHXM4895Fn7/t//2vsJcOEl0i8c46wn4eExweQrzrJDi/+hTvOMP5tnk5MenrSmy3BOQtcNaGW43AKIvZeNe5n6ejbPg93/ZO69SabGrCHKMMTkAIpJojFkLBPvW/Q60EZGWIpIAXAF45GcUEffH5eHABpvylAJLERQUFpJkNzXl8468we5eKyXFlyJw8ofJcN8GGPJQ6c+vBGfLT8XLMte6trO2WR1eic75M/z8gmXsdfLrK7Dz9+BtnfPxvozOi96FGffbl2Pyeb5HPE62/urpAlpY4Kd+gJFIjp8otEqVwa6xeKdjHcGXwCwROQxsC9TAGJMvIuOBb4FYYLIxZpWIPAlkGGOmAeNF5FwgDzgMXFfaGwmKY0QgmOKuo4EoSyKaRt0gJoiurVHfFYa5JJzzBPzwl9LJpZQdDxtCCTnpPpj2+n5t+sF6Xfic47CxPJASa0L2DiuER3O3sB57A0xbgeUJBK4Fev+5xLdiDGQLCEdU2Pxca21IKBwylDJjd2XxJY7NiSIyG0gBZtpoNwOY4VX2uNv2BPuilhUp+psUV4IvtveiJ7s06Q3X/tdm5RJmDbtnrZU0ZuBd8GRq8eOXvq3rISoyyz9xbfvLwZC1w7IlLHwLFr7peeymHz33fXXi+afg3RHFy30pAX/ncHL8gGWQHfVP6H6l/3olYcZ9Vra9e9ZArcahOWdpWfwf6DACqvn4LUUJJVb1xpifjDHTjDEBxqMVkNKOCN4fWbrr1WkFSbXs1S1JusyrPnVlDvM32mhSgpW+Svmz6QfX9pL/+K7zSmdrgZy3EgB4+2zP/d1Li9fZvsDTK+jQZusp3B+BMtgd3Gi9+zLIGmM5Usx90X97X+xYaL07p50mplhxqsqbPcth2nj4slKGTgsZFSwTSDhxKYKkYO6jIaEkU0olUAStz7Fxaa9rt/fxZKhUDLK2l/0cvkZ/3iuYX+sB0+/2L8PXAQbngR5UdmZYjhQ/PBlczmBsnlP2c5QU5+d0fH/gelWcqFEEu7Ndi7rKRxG40WccjA6QdN7XD+2xA77rxtqYzaue5v/8Dd383QfdE/xcSuXE11SPv47W7qp6X99T52gBij+ATL8HfnnZ97n82d7cFwaC5RXla91FIGY+XIqYYSWcnq1iRI0ieOUHyyGp3EYE7l/04S9AJ4dnbIuBPir7+BJ6G+j63gz3rg9+3Qe2Wcnq2w4LLldJpqSUysW6GcXL3MODO/k2wPoLJ84QHc7v6Zxnramc1V/Bl7e66nl37hn/tvJX5Odar7yTsGux9SrC6zv4s1dGuGcaWau/S8KC112jpFVfembBqywserdcR0iVLMtY6UmIs25VMMFXFocEH088dyyGmg3tNXcPkHbxG9DjanvtqjmCxF71ifUDADx+bB5PiqVUBEkpni6FSbWtJ7lAi6iU8sVucDtfocO9cXZIOxZYob0XOlZA/+a9vsDPU/7MB62V31lujoZpTu9zrzbeKVT9lfli7QxPD7ycI1YgSbAW3HV1rPHweAAyPsoqAM6pOl8ZA8NA1IwIWjtSUwoUX1m88YfiDYIx0u0HVLMRtDnPchcNRN3WkOAjIY6/L2GHkdD3FvtKwJuWg633Zg53w943QM9rg1/XF3e6/Rgb9/A89udN8IBGqaxQhCM0RGE+TL/L/zW2/uK73Z6lnkoA/H/3nOVH9wXOoe2LKVd62kvcp5SMseI+vdHfs03RKKaCKYJyJmpGBGMHtYJ5fkYE3slT7NDzWsvbAOBex+KkvJOWd8abAyy7gG38fAnH+PEo8cdAL4Pf1Z9B3gnrib3t+ZDWxnpimlmKHEN1WsHp460nSO8pAKfdovcNxRPUK5EhXDGCln0M1RzZ4wq9Qk+8PxJum2+Fsk53mwL1JYv7gj53nFOi742AA15TobknrACOzux162a6VocHwxTCqSOQeSR43SgkakYE7gS0EZxxb/ATXPSq9d7Ta/1bfDVrgczEbGhxun2ByjIsvX2B9T78JRjq5bkRl2j5RotYSsC6mPuF4faF1o/XnYnZvmMhBVv84x5uQYkwYQzke9KR/CdrR/Fjb55uLWJ7221eP9iizElnubbXTremNL2VAMBbZ3oGLPx4jH9PKG9CpRh3L7GmoKoYUacIBEPduY+55s9zvJ4Qev7R6gj/FCAyY6/rrfeRr4VoDq8MiqB+B2tqpvcNNi8lntv120NKsaCwcP5ffbR1fl38/bAD3EeKWzjm623+kFqeaa+eL85+rPRtqwKB1gWEikBP4+5rGAJ1wkd2we7F/o+748xSZ0xx7yJf/PQ3tx1/39kSKsxJQ6wpqCpG9CiCogVlkLzkbats1uPF3dsSnE+1fjq18vDJr9moZPWrp9kfVXh4I4nXOy7FVsdHqGhnW7thNy5+HS5/37Iv3O0WCqFJr+J1fSnUcybau44vEm0u5lPCT6DvyweXlfx8/xkFT9YJXs89UOQBP2HMdjgU1hE3pfb5OHixg5VdbsrVnomEwsWJQ3CslCH2Q0D0KAJHZ/fgBW6x8hb80zNoGED1utZ7nZbFn0iv+QKuCJCgpNSilaehymtEAG7KIZgcQY5730ePa6DjxZZ9waNekK9di4GWvaOpm8I4fXwQ2bwodBgKmw+AVkNc5SP/XrLzKGUn1PYKu26V7gropFsu691LrZmAQ1vg+yessmzHwr4ThywFcnQ3LP3Ymqr6+bnSy7rgTdgQIFeGk+daWjmiI0T0KAJHJ5UQ4+5KWeBKlu5NbDxc9zWMz3CVpbUNq2zlgnjZCEpy/aD1QnSesTOK2zsCRXH1Jj7ZZchs0hP++BWkOkY4Pf9YvH5MvP1zKyUnUm7F/hTQpMHwbDMrhIc37jYIZxa99T6SS835m72R8cwH4cNSjHrKmehRBEWdlNs/zxQGn2tMawPxTpfPqpDA1Vcn7EchdP6DV7UgXxfbCq0Eim/MB9Z7QxurXy94Hh47CI/sAXE4BMQ50pLeOhfuXuVHHDd5RvhZCetvgZ4SnFCE0ShNchxvr6ZgeCf2+c6RJyt7O2Su85w+mvOMK69zWdm+wHPfXcGcOmqt3Qgz0aMInD9246UI7CSccYYErl7a7GgVCG9jsXeZO5dO8txvez6cdi6c6y/8dWnsFEHocBHc+gt0+UPwujGxnq6s/W5zhdFIrAkpTYOfo/1F0PUKz7KHd8MVH9mXWQk9T9UteZuSTkn98or/YycOwUsdPMsmDXFtH9xk9S0bv7emnLxZ940r0VVhoaV0MtdbTiuTz/esm+GWJOn/msLrfUp0G6UhatYR+BwR2KXHNdarvCipsbgk+DIW+7MRxMRC7eauJ7qE6nDN59aPwue5He2HPAxDHgggg6Ne7RaeC40u/Ze17sGbQKOBVmfBZreUk04SkuGCZ/23c5J+hmcyGYmh2HfE1yLAYLQ+Bxp19R9rp9OlsOqLkp9XsU9+CZ+kT/iJ7wXwjp8R4YlDvnNwP7DNCt/t5GPHw8XEbJj7Asz+qytDoTf/83Jhz9pu2TTsRjMuBdE9IqiIXPq2FR4iXHiMCIptFOfWX638B6FgfIZ1LhEY8yHc4JXSouvlLtdcu8Qnu7ZTmvmv5w9TSNH9t3fEpPcZD6qEtD0f2pzv/3i7C8p+DSW0rChpoDp8KwGAv7WARe8ULz+4CTY58kms/Nz+df4R3lFB9CgC54/dGQe9otJ1tJW1rFywYSxOquXKf1DUzE/91HTrvbafDjmtjetcHUa4EpI06W1LWq76zNN4D9C4u5Uw5arP4DQbIbq9qdUEWjlCcfxhspXjoecfrYQpgbjTT/wb986/xemeRu9717kUly/j94Tl9uVWKj6+ssdNHgbb5xcvD4avtKYhJHqmhqQSKIL65Zy2z7tDdxpWgzf0Xdz9akfHOsS+DHcstq/42p7n57qlXOAz+l04bag1HZS903X/IoGzZj20CxJr+D5WNC/t+IzcnRFqNoQBd1gLneKSPNuVU3AxpRzxlQ0uWN6DQM4rJ7NcQSVDTPSNCMIVg6Ws3DwHxv6vnC/q+Exi4+GsR+Emm8H33BWIe3gKEWh9VsncYeu2Lltoig5lyB3c6RKrQ09Ihno+XINbn13ccAz+lQAUf9Lv5qWkznrY6vRjShkB93IfWcKUqoN7fgdvPrThMFFKom9EUFFtBN4RPcsD9w578J/tt3N+hom1oEHH0MpUEsL9FG0757Qb3uE6vKfVnPj7HsYn+zaYA9w4C5r1tS9Lp0tgVSnuQYkcc1/yf8zdqSHERN+IIJDX0FWlMBZVRgbcWcYTRDh075WfwEWvRebagajtFk8p6KjIz/ew3YX+m5TE7Rb8+9F7r/RWKg7LI5NAJ3oUgZ0Rga/4OlWSMnbkzs8wUiHc2w2DXtcFrxcJ7I447dhjuoz23C+pG6u/UeadS+CJLHvnaBp+H3Yl8kSPIsCGIihJGIOqQJlDW0R3Mo/ilODzaHWW7/LznrLer/ncsxMe84EVabYkDAiSkL5akMBtae3g3Iklu6YSXnLCMx0aPYqgaEQQwCofGyUxZ8pqJ0msBXXbVN4Abpf+y4ojVVJ632BjWsXHZ9vmfGgxyLPMnxKu1diyfZx2rpWn2klpjOKxXg8213zuFWspyPegUVdP5wrnKm0lciy3EQmhFETRI7DTnS9A/BEpj1zGFYlSPtHHxsEdGcHrVVS6Xl66dv7iEAXjaj8/3pRmkO0juYsTEbjlZ8g/FfwacdUg/yT0uBaW+Mlsd9q51stJMA+6LqM9HxrOeRx+CWDMLCnpZ8DWuaE7XzQQpuRP0aMIxIb7aEVLYB0uBtxppQp0z1+slB07sYx8cclb/o8Fy4PtpFoqHD0JnS+1jMr+FvW5U6Oh/6mGm36Apr09g72F8vdx3wY4vBX+PTRo1WIMnGBlCTvoJ8dAVaY04U5sENapIREZJiLrRGSjiBRLlCsi94jIahFZLiI/iEiLcMoDBB4RxCaE/fIVgpoNrGmCaqmRlqRq4e7VY6fTdMavau8jLWhJGfGytRak5RArc96ZDnfgQJn2/vil5/6FL0BN50I65xqTMj4rXvo2/HmzZ1lSirWI0D08SEnoekXlHpGWhTCFTA+bIhCRWOB14AKgI3CliHg7nS8BehtjugJTgTJkgAgqUODjV3/uSoqtKCWh06XWu0jJ7C+DH4DHDoRmuF8t1VoLEuP1k67Xznd9cK2ebjEIxn4DfW5ypSit62YLqeUjlak3FzxvTWN506yvlezpvKdd6UPrOYze3jmw+91WvL2v0OHRYsvzRWkXIgYhnFNDfYGNxpjNACIyBbgYWO2sYIyZ7VZ/AVCOIT69aHNu8DqK4oue1zoiifpI+hMIkdB1ak1txmvy5t51kFQb4h0hLzpfar3cuekH2L/as+zhPfCMY7FcoIV9qY5B/oA7rPf6HaF5f2tbxDr3244YUdXTPNveOMuabhvxCsx9Ecb8B1Z+AXUdmbyqpcLJw/bvtSoQpunrcE4NNQHcLWE7HWX+uBH4xtcBEblZRDJEJCMzsyx5Pd0+xGiZBlLCj9PtOCFA6IlQ0mIgNHLLrvXw7tI/KdZs6FIC/qjVqHhAv4RSTuu0v9Bz5N20N1zwHDTrZ4X08LiG4/PsPRbuXmmtizjvKVdneKONFJD+uPiN0reNJGFyaKkQ7qMicg3QG3je13FjzCRjTG9jTO969cqQHMYjBHOFuHWlKtBiEAx5yOFOWw4hTMbOgFt+sqZzbpkbNgNiudHvFrjxOyut6MRs19RRMNJKkeN3+Ivw+OHioUACUa89JKYUL+84yrKpuCuwMWHIae5OmPqtcPaGuwB314WmjjIPRORc4BFgpDHGhp9cWXBXBG6a9eqp4b2sUnWpXt+alx/yoDUX3tIR0tqut09ZaDHA8vWPFLV8eEld+IL1dD9+EVw/o3TndY4+wmGz63yZ9f/yZ8tp4yPCbUy8tZrdnetnWNFr711jxaRy5sSu38GV1jRQuJDSUgmnhn4H2ohISxFJAK4AprlXEJEewFtYSiBIfNYQ4P4hug+l25TChU1R7loJ470CgXW+FO7fUvo5+4rO9f+zQocDjPuxeGffd5z1dJ92GqSXMsHPuX+Bu1ZY01alZZiP7HQTs/17yjltGL4QYOQ/PA3XLQZ4dcrOsCviUgDJpUivGZTwKIKwGYuNMfkiMh74FogFJhtjVonIk0CGMWYa1lRQDeAzsT7U7caYkeGSyeNDjImFYX8L2wINJQrw56tf0bzPmvSCXmNDc650txXSNRtYr1ATG+cZwK809L8NZhbzWHfDbUSQUBMadLa2/bmXxyV4rhPxfjIvdKxPkhjXWiX3aZzb5sOJg/DeiMByN+tnJbTxF4E2TCOCsC4oM8bMAGZ4lT3utl2+rjriNTXU/9ZyvbyiRIRxP0ZagvBzySRY+E9ris5OuGZ/C0uT0ywjde4xt0Ibna/xpQjc2jnDtdc9zTPnQLP+sGOBa/+6r62AhBPdbBLx1SHvuH1ZSkH0rCwGPEcEUXbrilKV6TbGeoGVLxrgjHstt9OgGDz6hjsWw7sX+k4S07gn7F7s5xxYisAZj6pBZzh9vKeLsLex98Zv4fe3rYT1vca6otJOzLZyGxsD+Tkw5SpHe1UEZUfEMvwU5sGguyMtjaIo4eSsR12KwDvHdDFbsVtBzQbW4rhnHAvu3Dvf66bBMR/mTPcRQeuzrPYNuxbvuJ2KYNSbLocCYzyPOXF6I21yH9FVPmNxBURcNoH2YbDoK4pSvvQZB8P9BMKLibFCqaS19e3h5IuijtvP4sDEmlZ6VW+c0WGd/Uujbr6f3p2dfcOurpXVxs3Q7FsoH/KFlugbETjDUOs6AkWp/Ax/IfDx086F8T5Mkanprm3vBECl4fz/gzPvD+584nRb9wiH72dEUNQm/MEwo0sRIFDoVATRFnJaUZQi0k6De9db3kCJtWCFd5pat6kiOx1xbBzUsLHYdfS7MP/vLi8lcDNc2+nwdURQdsRdEeiIQFGimkCur3HVwnPNtNPgolc9y4JNDZU0qm0piLLeUFx+wmGK4qcoShUgJsYKiAeEPyVrsBziJQxmWAqiSxGojUBRFF+0uxCan26tQ3BSr72V1/mcx8J7bV8L0PyhxuJQICX70BVFiQ6SasENMz3LEmvAA1vCf21fC9DcKQdjcfT2hqoIFEWpCDjXC7T3F34i/FND0TUicP8M1UagKEpFoGGXwMl9PELjhEeE6FIE/sJQK4pSavLy8ti5cyc5OTmRFqVqkp8M539qbR+KgyNrAlZPSkqiadOmxMfbz34XXYogJ8u1rVNDihISdu7cSc2aNUlPT0fKYT476jh1DA46PIsadAyY3tQYw8GDB9m5cyctW7a0fYno7Q11akhRQkJOTg5169ZVJRBu4pOD5rgWEerWrVvi0Vn0KgIdEShKyFAlEEacnb/N3Cml+V9E19SQO/rFVRSlMhCXCPU7QmxC+C4RtjMriqIoocGZpyBM6PyIoiiKTfLz/aSyrOToiEBRlJDxl69XsXr3kZCes2PjWjxxUaeg9UaNGsWOHTvIyclhwoQJ3HzzzcycOZOHH36YgoIC0tLS+OGHHzh27Bh33HEHGRkZiAhPPPEEl112GTVq1ODYMStF5dSpU5k+fTrvvvsu119/PUlJSSxZsoSBAwdyxRVXMGHCBHJycqhWrRrvvPMO7dq1o6CggAceeICZM2cSExPDuHHj6NSpE6+99hpffvklALNmzeKNN97gv//9b0g/o7KiikBRlCrB5MmTqVOnDidPnqRPnz5c75bKAgAADnFJREFUfPHFjBs3jp9//pmWLVty6NAhAJ566ilSUlJYsWIFAIcPHw567p07dzJv3jxiY2M5cuQIc+fOJS4uju+//56HH36Yzz//nEmTJrF161aWLl1KXFwchw4dIjU1ldtvv53MzEzq1avHO++8ww033BDWz6E0qCJQFCVk2HlyDxevvfZa0ZP2jh07mDRpEmeeeWaRP32dOnUA+P7775kyZUpRu9TU1KDnHj16NLGxlst5dnY21113HRs2bEBEyMvLKzrvrbfeSlxcnMf1rr32Wj744APGjh3L/Pnzef/990N0x6FDFYGiKJWeOXPm8P333zN//nySk5MZMmQI3bt3Z+3atbbP4e526e2HX7169aLtxx57jLPOOov//ve/bN26lSFDhgQ879ixY7noootISkpi9OjRRYqiIqHGYkVRKj3Z2dmkpqaSnJzM2rVrWbBgATk5Ofz8889s2WJFEHVODQ0dOpTXX3+9qK1zaqhBgwasWbOGwsLCgHP42dnZNGnSBIB33323qHzo0KG89dZbRQZl5/UaN25M48aNefrppxk7dmzobjqEqCJQFKXSM2zYMPLz8+nQoQMPPvgg/fv3p169ekyaNIlLL72Ubt26MWbMGAAeffRRDh8+TOfOnenWrRuzZ88G4Nlnn2XEiBEMGDCARo0a+b3W/fffz0MPPUSPHj08vIhuuukmmjdvTteuXenWrRsfffRR0bGrr76aZs2a0aFDhzB9AmVDjDHBa1UgevfubTIyMkrXeGKK23aAaH+KothmzZo1FbaDqyiMHz+eHj16cOONN5bL9Xz9T0RkkTGmt6/6YR0RiMgwEVknIhtF5EEfx88UkcUiki8ifwinLIqiKJGgV69eLF++nGuuuSbSovglbFYLEYkFXgeGAjuB30VkmjFmtVu17cD1wH3hkkNRFCWSLFq0KNIiBCWc5uu+wEZjzGYAEZkCXAwUKQJjzFbHscIwyqEoiqIEIJxTQ02AHW77Ox1liqIoSgWiUngNicjNIpIhIhmZmZmRFkdRFKVKEU5FsAto5rbf1FFWYowxk4wxvY0xvevVqxcS4RRFURSLcCqC34E2ItJSRBKAK4BpYbyeoiiKUgrCpgiMMfnAeOBbYA3wqTFmlYg8KSIjAUSkj4jsBEYDb4nIqnDJoyiKAlCjRo1Ii1DhCGvQC2PMDGCGV9njbtu/Y00ZlQ93LoHXepTb5RQl6vjmQdi7IrTnbNgFLng2tOesAOTn51eYuEOVwlgcMkQT1itKVePBBx/0iB00ceJEnn76ac455xx69uxJly5d+Oqrr2yd69ixY37bvf/++0XhI6699loA9u3bxyWXXEK3bt3o1q0b8+bNY+vWrXTu3Lmo3QsvvMDEiRMBGDJkCHfddRe9e/fm1Vdf5euvv6Zfv3706NGDc889l3379hXJMXbsWLp06ULXrl35/PPPmTx5MnfddVfRef/1r39x9913l/pz88AYU6levXr1MqUma4cxT9SyXoqihITVq1dH9PqLFy82Z555ZtF+hw4dzPbt2012drYxxpjMzEzTunVrU1hYaIwxpnr16n7PlZeX57PdypUrTZs2bUxmZqYxxpiDBw8aY4y5/PLLzcsvv2yMMSY/P99kZWWZLVu2mE6dOhWd8/nnnzdPPPGEMcaYwYMHm9tuu63o2KFDh4rk+te//mXuueceY4wx999/v5kwYYJHvaNHj5pWrVqZ3NxcY4wxp59+ulm+fLnP+/D1PwEyjJ9+tWKMS8oLia4BkKJEAz169GD//v3s3r2bzMxMUlNTadiwIXfffTc///wzMTEx7Nq1i3379tGwYcOA5zLG8PDDDxdr9+OPPzJ69GjS0tIAV66BH3/8sSi/QGxsLCkpKUET3TiD34GV8GbMmDHs2bOH3NzcotwJ/nImnH322UyfPp0OHTqQl5dHly5dSvhp+UYVgaIolZ7Ro0czdepU9u7dy5gxY/jwww/JzMxk0aJFxMfHk56eXizHgC9K286duLg4CgtdwRIC5Ta44447uOeeexg5ciRz5swpmkLyx0033cQzzzxD+/btQxrSOrp6RrURKEqVZMyYMUyZMoWpU6cyevRosrOzqV+/PvHx8cyePZtt27bZOo+/dmeffTafffYZBw8eBFy5Bs455xzefPNNAAoKCsjOzqZBgwbs37+fgwcPcurUKaZPnx7wes7cBu+9915Rub+cCf369WPHjh189NFHXHnllXY/nqBElyKIcSiCWuXnqKQoSvjp1KkTR48epUmTJjRq1Iirr76ajIwMunTpwvvvv0/79u1tncdfu06dOvHII48wePBgunXrxj333APAq6++yuzZs+nSpQu9evVi9erVxMfH8/jjj9O3b1+GDh0a8NoTJ05k9OjR9OrVq2jaCfznTAC4/PLLGThwoK0Um3aJrnwEAHNfhI6joG7r0AmlKFGM5iMoX0aMGMHdd9/NOeec47dOhcpHUCE5415VAoqiVDqysrJo27Yt1apVC6gESkN0GYsVRVGAFStWFK0FcJKYmMjChQsjJFFwateuzfr168NyblUEiqKUGWMMIhJpMWzTpUsXli5dGmkxwkJppvujb2pIUZSQkpSUxMGDB0vVASmhxRjDwYMHSUpKKlE7HREoilImmjZtys6dO9FcIRWDpKQkmjYtmWekKgJFUcpEfHx80YpYpXKiU0OKoihRjioCRVGUKEcVgaIoSpRT6VYWi0gmYC9wSHHSgAMhFCeS6L1UTKrKvVSV+wC9FyctjDE+k75XOkVQFkQkw98S68qG3kvFpKrcS1W5D9B7sYNODSmKokQ5qggURVGinGhTBJMiLUAI0XupmFSVe6kq9wF6L0GJKhuBoiiKUpxoGxEoiqIoXqgiUBRFiXKiRhGIyDARWSciG0XkwUjLEwwR2SoiK0RkqYhkOMrqiMgsEdngeE91lIuIvOa4t+Ui0jPCsk8Wkf0istKtrMSyi8h1jvobROS6CnQvE0Vkl+N/s1RELnQ79pDjXtaJyPlu5RH9/olIMxGZLSKrRWSViExwlFe6/0uAe6mM/5ckEflNRJY57uUvjvKWIrLQIdcnIpLgKE907G90HE8Pdo+2MMZU+RcQC2wCWgEJwDKgY6TlCiLzViDNq+w54EHH9oPA3xzbFwLfAAL0BxZGWPYzgZ7AytLKDtQBNjveUx3bqRXkXiYC9/mo29Hx3UoEWjq+c7EV4fsHNAJ6OrZrAusd8la6/0uAe6mM/xcBaji244GFjs/7U+AKR/k/gdsc27cD/3RsXwF8Euge7coRLSOCvsBGY8xmY0wuMAW4OMIylYaLgfcc2+8Bo9zK3zcWC4DaItIoEgICGGN+Bg55FZdU9vOBWcaYQ8aYw8AsYFj4pffEz73442JgijHmlDFmC7AR67sX8e+fMWaPMWaxY/sosAZoQiX8vwS4F39U5P+LMcYcc+zGO14GOBuY6ij3/r84/19TgXNERPB/j7aIFkXQBNjhtr+T/2/v3kKsqqM4jn9/3cyc0AqTyKjGhCKw6UJUWgTRgD0VTBSVhQW92INvEXaB3qsnKYkCqyHCckh6KZxkwIfQLpPZfepJMQciJwyKGlcP/3VmTuNcNeeczf594DD7/PeezVr8zzlr7//e53+mf+G0gwA+kvSZpMezbVlEHMrlX4BluVyF/OYae7vn9EQOmbzeGE6hIrnkcMK1lKPPSvfLhFyggv0i6XRJg8AwpbD+BByJiH8miWss5lw/AlzASeZSl0JQRWsi4jpgLbBB0m3NK6OcD1by3t8qx55eBlYAXcAh4IXWhjN7kjqA94CNEfF787qq9cskuVSyXyJiNCK6gOWUo/gr5zuGuhSCg8AlTc+XZ1vbioiD+XcY6KO8QA43hnzy73BuXoX85hp72+YUEYfzzXsMeJXxU/C2zkXSmZQPzt6I2J7NleyXyXKpar80RMQRYBdwM2UorvHDYc1xjcWc6xcDv3KSudSlEOwFVuaV+LMoF1l2tDimKUlaJOncxjLQDeynxNy4S+MR4P1c3gE8nHd63ASMNJ3ut4u5xv4h0C3pvDzF7862lptw/eUeSt9AyeX+vLPjcmAlsIc2eP3lOPJrwLcR8WLTqsr1y1S5VLRflkpakssLgTsp1zx2AT252cR+afRXD/BxnslNlePszOcV8lY+KHdB/EAZf9vU6nhmiLWTcgfAl8DXjXgpY4H9wI/ATuD8GL/zYHPm9hVwQ4vjf5tyav43ZazysROJHXiUctFrCFjfRrm8mbHuyzfgRU3bb8pcvgfWtsvrD1hDGfbZBwzm464q9ss0uVSxX1YBX2TM+4Fns72T8kE+BGwDFmT72fl8KNd3zpTjbB6eYsLMrObqMjRkZmZTcCEwM6s5FwIzs5pzITAzqzkXAjOzmnMhMJtHkm6X9EGr4zBr5kJgZlZzLgRmk5D0UM4TPyhpS04MdlTSSzlvfL+kpbltl6RPcrKzPo3P6X+FpJ051/znklbk7jskvSvpO0m9+U1Zs5ZxITCbQNJVwH3A6iiTgY0CDwKLgE8j4mpgAHgu/+UN4MmIWEX5ZmujvRfYHBHXALdQvqEMZbbMjZQ55DuB1ac8KbNpnDHzJma1cwdwPbA3D9YXUiZjOwa8k9u8BWyXtBhYEhED2b4V2JZzRV0cEX0AEfEnQO5vT0QcyOeDwGXA7lOfltnkXAjMjidga0Q89Z9G6ZkJ253o/Cx/NS2P4vehtZiHhsyO1w/0SLoQxn7X91LK+6UxI+QDwO6IGAF+k3Rrtq8DBqL8ctYBSXfnPhZIOmdeszCbJR+JmE0QEd9IepryC3GnUWYe3QD8AdyY64Yp1xGgTAv8Sn7Q/wysz/Z1wBZJz+c+7p3HNMxmzbOPms2SpKMR0dHqOMz+bx4aMjOrOZ8RmJnVnM8IzMxqzoXAzKzmXAjMzGrOhcDMrOZcCMzMau5fgIGsPmh7Qc8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}