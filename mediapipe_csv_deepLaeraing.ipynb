{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mediapipe_csv_deepLaeraing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWZiM6IjK+tJDSyvJpDv3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikunemu/colab_study/blob/main/mediapipe_csv_deepLaeraing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12_PCa7i1Bzr"
      },
      "source": [
        "#  mediapipeで取った特徴点を分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rptBUegwvb1"
      },
      "source": [
        "90分対策  \n",
        "F12を開いて以下のスクリプトをコンソールで実行  \n",
        "function KeepClicking(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(KeepClicking,600000)\n",
        "\n",
        "  \n",
        "10分ごとに接続"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQzvztFP513g",
        "outputId": "932d8991-4ff5-42a2-f851-2720116daf91"
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import os\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, SimpleRNN,GRU,Activation,LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import utils as np_utils\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3KcXQxqltwZ"
      },
      "source": [
        "## csvデータ読み込み\n",
        "train,test,validそれぞれ7感情の特徴点を記録"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYz-88dFlO4e"
      },
      "source": [
        "fp_train='/content/drive/MyDrive/data分析/Mediapipe/csvtraintotal.csv'\n",
        "df_train=pd.read_csv(fp_train)\n",
        "fp_test='/content/drive/MyDrive/data分析/Mediapipe/csvtesttotal.csv'\n",
        "df_test=pd.read_csv(fp_test)\n",
        "fp_valid='/content/drive/MyDrive/data分析/Mediapipe/csvvalidtotal.csv'\n",
        "df_valid=pd.read_csv(fp_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "OnuDyefiX3F1",
        "outputId": "4943f11f-32ad-454a-c3fc-4f081c3ddff9"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "      <td>8108</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "      <td>8109</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "      <td>8110</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>8111</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "      <td>8112</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.470799  0.682127 -0.080930  ... -0.045183           0        0\n",
              "1      0.469567  0.799870 -0.017644  ... -0.206077           3        0\n",
              "2      0.711111  0.717369 -0.049849  ... -0.146556           4        0\n",
              "3      0.483527  0.809326 -0.011760  ... -0.195280           5        0\n",
              "4      0.061688  0.692855 -0.050553  ... -0.058228           6        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "82190  0.436367  0.787778 -0.124726  ... -0.051585        8108        6\n",
              "82191  0.456891  0.704518 -0.093469  ... -0.061855        8109        6\n",
              "82192  0.502264  0.729350 -0.124422  ... -0.034920        8110        6\n",
              "82193  0.539922  0.733853 -0.116346  ... -0.023272        8111        6\n",
              "82194  0.508446  0.805487 -0.070251  ... -0.153780        8112        6\n",
              "\n",
              "[82195 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "IwOLXOUD1NKy",
        "outputId": "f89cee7d-52c3-46a3-b62b-7b913be9ec67"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.516601</td>\n",
              "      <td>0.585682</td>\n",
              "      <td>-0.128171</td>\n",
              "      <td>0.354741</td>\n",
              "      <td>0.407085</td>\n",
              "      <td>-0.017539</td>\n",
              "      <td>0.303413</td>\n",
              "      <td>0.437223</td>\n",
              "      <td>-0.022323</td>\n",
              "      <td>0.385631</td>\n",
              "      <td>0.474988</td>\n",
              "      <td>-0.077427</td>\n",
              "      <td>0.192280</td>\n",
              "      <td>0.117430</td>\n",
              "      <td>0.133606</td>\n",
              "      <td>0.227670</td>\n",
              "      <td>0.153081</td>\n",
              "      <td>0.075319</td>\n",
              "      <td>0.261869</td>\n",
              "      <td>0.196746</td>\n",
              "      <td>0.025099</td>\n",
              "      <td>0.384131</td>\n",
              "      <td>0.734853</td>\n",
              "      <td>-0.092528</td>\n",
              "      <td>0.409821</td>\n",
              "      <td>0.185824</td>\n",
              "      <td>-0.000665</td>\n",
              "      <td>0.388524</td>\n",
              "      <td>0.114111</td>\n",
              "      <td>0.043742</td>\n",
              "      <td>0.366814</td>\n",
              "      <td>0.051379</td>\n",
              "      <td>0.084901</td>\n",
              "      <td>0.466509</td>\n",
              "      <td>0.041154</td>\n",
              "      <td>0.091456</td>\n",
              "      <td>0.247168</td>\n",
              "      <td>0.348231</td>\n",
              "      <td>0.038491</td>\n",
              "      <td>0.162550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.673695</td>\n",
              "      <td>-0.099631</td>\n",
              "      <td>0.486085</td>\n",
              "      <td>0.227604</td>\n",
              "      <td>-0.000800</td>\n",
              "      <td>0.411432</td>\n",
              "      <td>0.685160</td>\n",
              "      <td>-0.107264</td>\n",
              "      <td>0.403327</td>\n",
              "      <td>0.699680</td>\n",
              "      <td>-0.105119</td>\n",
              "      <td>0.365605</td>\n",
              "      <td>0.599879</td>\n",
              "      <td>-0.087972</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>0.566646</td>\n",
              "      <td>0.268614</td>\n",
              "      <td>0.517140</td>\n",
              "      <td>0.487549</td>\n",
              "      <td>-0.123952</td>\n",
              "      <td>0.407326</td>\n",
              "      <td>0.672411</td>\n",
              "      <td>-0.080824</td>\n",
              "      <td>0.400786</td>\n",
              "      <td>0.675086</td>\n",
              "      <td>-0.085923</td>\n",
              "      <td>0.467047</td>\n",
              "      <td>0.510292</td>\n",
              "      <td>-0.103923</td>\n",
              "      <td>0.406921</td>\n",
              "      <td>0.514333</td>\n",
              "      <td>-0.071427</td>\n",
              "      <td>0.460763</td>\n",
              "      <td>0.503372</td>\n",
              "      <td>-0.100683</td>\n",
              "      <td>0.483841</td>\n",
              "      <td>0.187230</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.350142</td>\n",
              "      <td>0.704083</td>\n",
              "      <td>-0.051000</td>\n",
              "      <td>0.258494</td>\n",
              "      <td>0.467294</td>\n",
              "      <td>0.006417</td>\n",
              "      <td>0.210762</td>\n",
              "      <td>0.484544</td>\n",
              "      <td>0.028842</td>\n",
              "      <td>0.256672</td>\n",
              "      <td>0.560486</td>\n",
              "      <td>-0.029805</td>\n",
              "      <td>0.163514</td>\n",
              "      <td>0.115912</td>\n",
              "      <td>0.038936</td>\n",
              "      <td>0.169306</td>\n",
              "      <td>0.169693</td>\n",
              "      <td>-0.000869</td>\n",
              "      <td>0.178214</td>\n",
              "      <td>0.229622</td>\n",
              "      <td>-0.030477</td>\n",
              "      <td>0.262613</td>\n",
              "      <td>0.771681</td>\n",
              "      <td>0.078652</td>\n",
              "      <td>0.294888</td>\n",
              "      <td>0.263658</td>\n",
              "      <td>-0.103125</td>\n",
              "      <td>0.291660</td>\n",
              "      <td>0.172433</td>\n",
              "      <td>-0.100819</td>\n",
              "      <td>0.290796</td>\n",
              "      <td>0.086691</td>\n",
              "      <td>-0.094484</td>\n",
              "      <td>0.382622</td>\n",
              "      <td>0.088537</td>\n",
              "      <td>-0.127045</td>\n",
              "      <td>0.190766</td>\n",
              "      <td>0.370796</td>\n",
              "      <td>0.059996</td>\n",
              "      <td>0.127881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.731658</td>\n",
              "      <td>0.042865</td>\n",
              "      <td>0.366543</td>\n",
              "      <td>0.318007</td>\n",
              "      <td>-0.107573</td>\n",
              "      <td>0.279443</td>\n",
              "      <td>0.738324</td>\n",
              "      <td>0.041889</td>\n",
              "      <td>0.274230</td>\n",
              "      <td>0.747920</td>\n",
              "      <td>0.050578</td>\n",
              "      <td>0.236568</td>\n",
              "      <td>0.660978</td>\n",
              "      <td>0.032212</td>\n",
              "      <td>0.120189</td>\n",
              "      <td>0.495042</td>\n",
              "      <td>0.403236</td>\n",
              "      <td>0.337132</td>\n",
              "      <td>0.624860</td>\n",
              "      <td>-0.094728</td>\n",
              "      <td>0.281208</td>\n",
              "      <td>0.722307</td>\n",
              "      <td>0.068394</td>\n",
              "      <td>0.274025</td>\n",
              "      <td>0.721999</td>\n",
              "      <td>0.067390</td>\n",
              "      <td>0.308012</td>\n",
              "      <td>0.624537</td>\n",
              "      <td>-0.054114</td>\n",
              "      <td>0.268657</td>\n",
              "      <td>0.601183</td>\n",
              "      <td>-0.016952</td>\n",
              "      <td>0.302620</td>\n",
              "      <td>0.616359</td>\n",
              "      <td>-0.057269</td>\n",
              "      <td>0.366551</td>\n",
              "      <td>0.274136</td>\n",
              "      <td>-0.123009</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.478097</td>\n",
              "      <td>0.829416</td>\n",
              "      <td>-0.035864</td>\n",
              "      <td>0.338219</td>\n",
              "      <td>0.586676</td>\n",
              "      <td>-0.044701</td>\n",
              "      <td>0.281508</td>\n",
              "      <td>0.608822</td>\n",
              "      <td>-0.030072</td>\n",
              "      <td>0.362336</td>\n",
              "      <td>0.690208</td>\n",
              "      <td>-0.055267</td>\n",
              "      <td>0.158889</td>\n",
              "      <td>0.208705</td>\n",
              "      <td>-0.127339</td>\n",
              "      <td>0.195300</td>\n",
              "      <td>0.286799</td>\n",
              "      <td>-0.150213</td>\n",
              "      <td>0.229269</td>\n",
              "      <td>0.368802</td>\n",
              "      <td>-0.164157</td>\n",
              "      <td>0.357517</td>\n",
              "      <td>0.888281</td>\n",
              "      <td>0.091781</td>\n",
              "      <td>0.391913</td>\n",
              "      <td>0.394345</td>\n",
              "      <td>-0.200909</td>\n",
              "      <td>0.374054</td>\n",
              "      <td>0.287385</td>\n",
              "      <td>-0.220581</td>\n",
              "      <td>0.355048</td>\n",
              "      <td>0.184166</td>\n",
              "      <td>-0.231421</td>\n",
              "      <td>0.476613</td>\n",
              "      <td>0.184120</td>\n",
              "      <td>-0.238496</td>\n",
              "      <td>0.228681</td>\n",
              "      <td>0.488865</td>\n",
              "      <td>-0.035819</td>\n",
              "      <td>0.134504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.857773</td>\n",
              "      <td>0.046278</td>\n",
              "      <td>0.478034</td>\n",
              "      <td>0.444558</td>\n",
              "      <td>-0.172615</td>\n",
              "      <td>0.386257</td>\n",
              "      <td>0.865752</td>\n",
              "      <td>0.048123</td>\n",
              "      <td>0.378545</td>\n",
              "      <td>0.874142</td>\n",
              "      <td>0.057897</td>\n",
              "      <td>0.340892</td>\n",
              "      <td>0.793487</td>\n",
              "      <td>0.010045</td>\n",
              "      <td>0.054897</td>\n",
              "      <td>0.550742</td>\n",
              "      <td>0.351424</td>\n",
              "      <td>0.478415</td>\n",
              "      <td>0.751673</td>\n",
              "      <td>-0.092016</td>\n",
              "      <td>0.385269</td>\n",
              "      <td>0.849144</td>\n",
              "      <td>0.059494</td>\n",
              "      <td>0.379350</td>\n",
              "      <td>0.852295</td>\n",
              "      <td>0.059011</td>\n",
              "      <td>0.434535</td>\n",
              "      <td>0.750683</td>\n",
              "      <td>-0.052339</td>\n",
              "      <td>0.380534</td>\n",
              "      <td>0.728616</td>\n",
              "      <td>-0.024325</td>\n",
              "      <td>0.428548</td>\n",
              "      <td>0.743129</td>\n",
              "      <td>-0.058275</td>\n",
              "      <td>0.478435</td>\n",
              "      <td>0.401297</td>\n",
              "      <td>-0.201465</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.440407</td>\n",
              "      <td>0.633268</td>\n",
              "      <td>-0.130409</td>\n",
              "      <td>0.313094</td>\n",
              "      <td>0.456206</td>\n",
              "      <td>0.001129</td>\n",
              "      <td>0.255751</td>\n",
              "      <td>0.477937</td>\n",
              "      <td>0.012412</td>\n",
              "      <td>0.327564</td>\n",
              "      <td>0.527424</td>\n",
              "      <td>-0.067629</td>\n",
              "      <td>0.187209</td>\n",
              "      <td>0.126621</td>\n",
              "      <td>0.163992</td>\n",
              "      <td>0.209704</td>\n",
              "      <td>0.178016</td>\n",
              "      <td>0.096623</td>\n",
              "      <td>0.230991</td>\n",
              "      <td>0.237480</td>\n",
              "      <td>0.042394</td>\n",
              "      <td>0.312069</td>\n",
              "      <td>0.801668</td>\n",
              "      <td>0.008883</td>\n",
              "      <td>0.372335</td>\n",
              "      <td>0.248090</td>\n",
              "      <td>-0.028703</td>\n",
              "      <td>0.360934</td>\n",
              "      <td>0.151648</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.349558</td>\n",
              "      <td>0.068262</td>\n",
              "      <td>0.044676</td>\n",
              "      <td>0.458189</td>\n",
              "      <td>0.061373</td>\n",
              "      <td>0.022868</td>\n",
              "      <td>0.221603</td>\n",
              "      <td>0.385956</td>\n",
              "      <td>0.085039</td>\n",
              "      <td>0.143525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.774976</td>\n",
              "      <td>-0.017430</td>\n",
              "      <td>0.452601</td>\n",
              "      <td>0.292870</td>\n",
              "      <td>-0.042988</td>\n",
              "      <td>0.331663</td>\n",
              "      <td>0.783315</td>\n",
              "      <td>-0.020361</td>\n",
              "      <td>0.322092</td>\n",
              "      <td>0.792106</td>\n",
              "      <td>-0.013262</td>\n",
              "      <td>0.298874</td>\n",
              "      <td>0.636628</td>\n",
              "      <td>-0.034253</td>\n",
              "      <td>0.112172</td>\n",
              "      <td>0.549183</td>\n",
              "      <td>0.409472</td>\n",
              "      <td>0.433293</td>\n",
              "      <td>0.558691</td>\n",
              "      <td>-0.145084</td>\n",
              "      <td>0.341583</td>\n",
              "      <td>0.752286</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.752438</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.395023</td>\n",
              "      <td>0.569501</td>\n",
              "      <td>-0.106214</td>\n",
              "      <td>0.345461</td>\n",
              "      <td>0.563145</td>\n",
              "      <td>-0.061551</td>\n",
              "      <td>0.390786</td>\n",
              "      <td>0.563328</td>\n",
              "      <td>-0.106485</td>\n",
              "      <td>0.452236</td>\n",
              "      <td>0.249019</td>\n",
              "      <td>-0.042549</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.508843</td>\n",
              "      <td>0.783509</td>\n",
              "      <td>-0.075715</td>\n",
              "      <td>0.342278</td>\n",
              "      <td>0.582771</td>\n",
              "      <td>-0.031950</td>\n",
              "      <td>0.289867</td>\n",
              "      <td>0.611026</td>\n",
              "      <td>-0.020946</td>\n",
              "      <td>0.375631</td>\n",
              "      <td>0.676300</td>\n",
              "      <td>-0.067914</td>\n",
              "      <td>0.148420</td>\n",
              "      <td>0.218699</td>\n",
              "      <td>-0.009825</td>\n",
              "      <td>0.183308</td>\n",
              "      <td>0.281258</td>\n",
              "      <td>-0.048717</td>\n",
              "      <td>0.216023</td>\n",
              "      <td>0.349559</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>0.396198</td>\n",
              "      <td>0.916336</td>\n",
              "      <td>0.045910</td>\n",
              "      <td>0.370082</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>-0.122842</td>\n",
              "      <td>0.345563</td>\n",
              "      <td>0.249492</td>\n",
              "      <td>-0.119133</td>\n",
              "      <td>0.317737</td>\n",
              "      <td>0.152755</td>\n",
              "      <td>-0.109723</td>\n",
              "      <td>0.428981</td>\n",
              "      <td>0.135849</td>\n",
              "      <td>-0.117880</td>\n",
              "      <td>0.226637</td>\n",
              "      <td>0.504523</td>\n",
              "      <td>0.004935</td>\n",
              "      <td>0.154398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.875826</td>\n",
              "      <td>0.011859</td>\n",
              "      <td>0.461890</td>\n",
              "      <td>0.394244</td>\n",
              "      <td>-0.109819</td>\n",
              "      <td>0.415073</td>\n",
              "      <td>0.887762</td>\n",
              "      <td>0.011303</td>\n",
              "      <td>0.407975</td>\n",
              "      <td>0.899817</td>\n",
              "      <td>0.019094</td>\n",
              "      <td>0.362383</td>\n",
              "      <td>0.781520</td>\n",
              "      <td>-0.017575</td>\n",
              "      <td>0.112559</td>\n",
              "      <td>0.630753</td>\n",
              "      <td>0.341884</td>\n",
              "      <td>0.502403</td>\n",
              "      <td>0.719198</td>\n",
              "      <td>-0.114545</td>\n",
              "      <td>0.415689</td>\n",
              "      <td>0.858130</td>\n",
              "      <td>0.028379</td>\n",
              "      <td>0.405933</td>\n",
              "      <td>0.861435</td>\n",
              "      <td>0.026839</td>\n",
              "      <td>0.456500</td>\n",
              "      <td>0.724646</td>\n",
              "      <td>-0.078316</td>\n",
              "      <td>0.398571</td>\n",
              "      <td>0.712659</td>\n",
              "      <td>-0.046950</td>\n",
              "      <td>0.450282</td>\n",
              "      <td>0.718590</td>\n",
              "      <td>-0.081530</td>\n",
              "      <td>0.455816</td>\n",
              "      <td>0.346828</td>\n",
              "      <td>-0.125290</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14727</th>\n",
              "      <td>0.560496</td>\n",
              "      <td>0.769144</td>\n",
              "      <td>-0.111749</td>\n",
              "      <td>0.395681</td>\n",
              "      <td>0.565257</td>\n",
              "      <td>-0.017007</td>\n",
              "      <td>0.342978</td>\n",
              "      <td>0.594650</td>\n",
              "      <td>-0.008701</td>\n",
              "      <td>0.431437</td>\n",
              "      <td>0.644474</td>\n",
              "      <td>-0.072280</td>\n",
              "      <td>0.212711</td>\n",
              "      <td>0.199529</td>\n",
              "      <td>0.095772</td>\n",
              "      <td>0.235907</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>0.043887</td>\n",
              "      <td>0.261247</td>\n",
              "      <td>0.286265</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>0.444982</td>\n",
              "      <td>0.899928</td>\n",
              "      <td>-0.032473</td>\n",
              "      <td>0.421535</td>\n",
              "      <td>0.284107</td>\n",
              "      <td>-0.043228</td>\n",
              "      <td>0.409811</td>\n",
              "      <td>0.212278</td>\n",
              "      <td>-0.018695</td>\n",
              "      <td>0.392661</td>\n",
              "      <td>0.132065</td>\n",
              "      <td>0.010029</td>\n",
              "      <td>0.507709</td>\n",
              "      <td>0.118564</td>\n",
              "      <td>0.007106</td>\n",
              "      <td>0.279881</td>\n",
              "      <td>0.494782</td>\n",
              "      <td>0.037404</td>\n",
              "      <td>0.207185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.865063</td>\n",
              "      <td>-0.052868</td>\n",
              "      <td>0.521913</td>\n",
              "      <td>0.335642</td>\n",
              "      <td>-0.041206</td>\n",
              "      <td>0.468909</td>\n",
              "      <td>0.876704</td>\n",
              "      <td>-0.059213</td>\n",
              "      <td>0.459812</td>\n",
              "      <td>0.887839</td>\n",
              "      <td>-0.054344</td>\n",
              "      <td>0.422744</td>\n",
              "      <td>0.770018</td>\n",
              "      <td>-0.057373</td>\n",
              "      <td>0.168215</td>\n",
              "      <td>0.636903</td>\n",
              "      <td>0.338159</td>\n",
              "      <td>0.546422</td>\n",
              "      <td>0.660709</td>\n",
              "      <td>-0.123501</td>\n",
              "      <td>0.473307</td>\n",
              "      <td>0.854591</td>\n",
              "      <td>-0.037045</td>\n",
              "      <td>0.464889</td>\n",
              "      <td>0.859350</td>\n",
              "      <td>-0.040605</td>\n",
              "      <td>0.506149</td>\n",
              "      <td>0.682234</td>\n",
              "      <td>-0.094430</td>\n",
              "      <td>0.455463</td>\n",
              "      <td>0.684035</td>\n",
              "      <td>-0.061109</td>\n",
              "      <td>0.500817</td>\n",
              "      <td>0.675388</td>\n",
              "      <td>-0.092881</td>\n",
              "      <td>0.517969</td>\n",
              "      <td>0.281741</td>\n",
              "      <td>-0.043033</td>\n",
              "      <td>1470</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14728</th>\n",
              "      <td>0.491013</td>\n",
              "      <td>0.712008</td>\n",
              "      <td>-0.122762</td>\n",
              "      <td>0.370838</td>\n",
              "      <td>0.521274</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.321248</td>\n",
              "      <td>0.549112</td>\n",
              "      <td>0.029226</td>\n",
              "      <td>0.397335</td>\n",
              "      <td>0.598340</td>\n",
              "      <td>-0.050178</td>\n",
              "      <td>0.231024</td>\n",
              "      <td>0.155950</td>\n",
              "      <td>0.198852</td>\n",
              "      <td>0.244556</td>\n",
              "      <td>0.193912</td>\n",
              "      <td>0.136056</td>\n",
              "      <td>0.262082</td>\n",
              "      <td>0.232480</td>\n",
              "      <td>0.084471</td>\n",
              "      <td>0.410388</td>\n",
              "      <td>0.861402</td>\n",
              "      <td>-0.035873</td>\n",
              "      <td>0.396322</td>\n",
              "      <td>0.229427</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.388710</td>\n",
              "      <td>0.161868</td>\n",
              "      <td>0.052182</td>\n",
              "      <td>0.378511</td>\n",
              "      <td>0.085723</td>\n",
              "      <td>0.091270</td>\n",
              "      <td>0.480310</td>\n",
              "      <td>0.072318</td>\n",
              "      <td>0.071249</td>\n",
              "      <td>0.277920</td>\n",
              "      <td>0.448965</td>\n",
              "      <td>0.099246</td>\n",
              "      <td>0.215149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820451</td>\n",
              "      <td>-0.054481</td>\n",
              "      <td>0.485971</td>\n",
              "      <td>0.284793</td>\n",
              "      <td>-0.002835</td>\n",
              "      <td>0.426605</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>-0.060726</td>\n",
              "      <td>0.418768</td>\n",
              "      <td>0.843263</td>\n",
              "      <td>-0.056251</td>\n",
              "      <td>0.383449</td>\n",
              "      <td>0.718743</td>\n",
              "      <td>-0.044822</td>\n",
              "      <td>0.213713</td>\n",
              "      <td>0.614015</td>\n",
              "      <td>0.380890</td>\n",
              "      <td>0.485615</td>\n",
              "      <td>0.622031</td>\n",
              "      <td>-0.120505</td>\n",
              "      <td>0.436568</td>\n",
              "      <td>0.807114</td>\n",
              "      <td>-0.036760</td>\n",
              "      <td>0.426990</td>\n",
              "      <td>0.810267</td>\n",
              "      <td>-0.040235</td>\n",
              "      <td>0.456969</td>\n",
              "      <td>0.639337</td>\n",
              "      <td>-0.088925</td>\n",
              "      <td>0.418537</td>\n",
              "      <td>0.638220</td>\n",
              "      <td>-0.049196</td>\n",
              "      <td>0.452996</td>\n",
              "      <td>0.632927</td>\n",
              "      <td>-0.086052</td>\n",
              "      <td>0.482375</td>\n",
              "      <td>0.228888</td>\n",
              "      <td>0.002328</td>\n",
              "      <td>1471</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14729</th>\n",
              "      <td>0.527434</td>\n",
              "      <td>0.738273</td>\n",
              "      <td>-0.079975</td>\n",
              "      <td>0.328769</td>\n",
              "      <td>0.457341</td>\n",
              "      <td>-0.067171</td>\n",
              "      <td>0.263649</td>\n",
              "      <td>0.484076</td>\n",
              "      <td>-0.066484</td>\n",
              "      <td>0.365410</td>\n",
              "      <td>0.582288</td>\n",
              "      <td>-0.097822</td>\n",
              "      <td>0.136217</td>\n",
              "      <td>0.021140</td>\n",
              "      <td>-0.074558</td>\n",
              "      <td>0.175402</td>\n",
              "      <td>0.085695</td>\n",
              "      <td>-0.108161</td>\n",
              "      <td>0.210071</td>\n",
              "      <td>0.146272</td>\n",
              "      <td>-0.132795</td>\n",
              "      <td>0.343199</td>\n",
              "      <td>0.830920</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>0.406371</td>\n",
              "      <td>0.175288</td>\n",
              "      <td>-0.141420</td>\n",
              "      <td>0.396257</td>\n",
              "      <td>0.092930</td>\n",
              "      <td>-0.141256</td>\n",
              "      <td>0.377956</td>\n",
              "      <td>-0.004383</td>\n",
              "      <td>-0.133272</td>\n",
              "      <td>0.510257</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>-0.116076</td>\n",
              "      <td>0.188901</td>\n",
              "      <td>0.338566</td>\n",
              "      <td>-0.055147</td>\n",
              "      <td>0.084274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798353</td>\n",
              "      <td>-0.025366</td>\n",
              "      <td>0.508404</td>\n",
              "      <td>0.246948</td>\n",
              "      <td>-0.110513</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>0.809397</td>\n",
              "      <td>-0.028493</td>\n",
              "      <td>0.373280</td>\n",
              "      <td>0.819410</td>\n",
              "      <td>-0.022557</td>\n",
              "      <td>0.337109</td>\n",
              "      <td>0.707447</td>\n",
              "      <td>-0.056202</td>\n",
              "      <td>-0.054723</td>\n",
              "      <td>0.439372</td>\n",
              "      <td>0.278810</td>\n",
              "      <td>0.532050</td>\n",
              "      <td>0.647020</td>\n",
              "      <td>-0.122507</td>\n",
              "      <td>0.381318</td>\n",
              "      <td>0.785802</td>\n",
              "      <td>-0.011244</td>\n",
              "      <td>0.373739</td>\n",
              "      <td>0.790772</td>\n",
              "      <td>-0.014106</td>\n",
              "      <td>0.463954</td>\n",
              "      <td>0.650501</td>\n",
              "      <td>-0.093005</td>\n",
              "      <td>0.385598</td>\n",
              "      <td>0.631218</td>\n",
              "      <td>-0.068217</td>\n",
              "      <td>0.455586</td>\n",
              "      <td>0.642425</td>\n",
              "      <td>-0.095598</td>\n",
              "      <td>0.511577</td>\n",
              "      <td>0.192825</td>\n",
              "      <td>-0.125415</td>\n",
              "      <td>1472</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14730</th>\n",
              "      <td>0.528845</td>\n",
              "      <td>0.735577</td>\n",
              "      <td>-0.041305</td>\n",
              "      <td>0.342966</td>\n",
              "      <td>0.537490</td>\n",
              "      <td>-0.021240</td>\n",
              "      <td>0.291597</td>\n",
              "      <td>0.566791</td>\n",
              "      <td>-0.001343</td>\n",
              "      <td>0.376911</td>\n",
              "      <td>0.633230</td>\n",
              "      <td>-0.048700</td>\n",
              "      <td>0.118212</td>\n",
              "      <td>0.205293</td>\n",
              "      <td>-0.040242</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.265181</td>\n",
              "      <td>-0.071281</td>\n",
              "      <td>0.188816</td>\n",
              "      <td>0.329133</td>\n",
              "      <td>-0.094329</td>\n",
              "      <td>0.402845</td>\n",
              "      <td>0.801184</td>\n",
              "      <td>0.070091</td>\n",
              "      <td>0.351299</td>\n",
              "      <td>0.319882</td>\n",
              "      <td>-0.148940</td>\n",
              "      <td>0.321788</td>\n",
              "      <td>0.228375</td>\n",
              "      <td>-0.157065</td>\n",
              "      <td>0.290454</td>\n",
              "      <td>0.137655</td>\n",
              "      <td>-0.158898</td>\n",
              "      <td>0.406340</td>\n",
              "      <td>0.110980</td>\n",
              "      <td>-0.176145</td>\n",
              "      <td>0.219970</td>\n",
              "      <td>0.473707</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>0.150090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.768059</td>\n",
              "      <td>0.035476</td>\n",
              "      <td>0.450491</td>\n",
              "      <td>0.353091</td>\n",
              "      <td>-0.136861</td>\n",
              "      <td>0.419335</td>\n",
              "      <td>0.775359</td>\n",
              "      <td>0.032669</td>\n",
              "      <td>0.414294</td>\n",
              "      <td>0.783746</td>\n",
              "      <td>0.040324</td>\n",
              "      <td>0.367322</td>\n",
              "      <td>0.733595</td>\n",
              "      <td>0.017130</td>\n",
              "      <td>0.119499</td>\n",
              "      <td>0.563274</td>\n",
              "      <td>0.379588</td>\n",
              "      <td>0.510960</td>\n",
              "      <td>0.668780</td>\n",
              "      <td>-0.098167</td>\n",
              "      <td>0.413863</td>\n",
              "      <td>0.767714</td>\n",
              "      <td>0.057815</td>\n",
              "      <td>0.407349</td>\n",
              "      <td>0.770133</td>\n",
              "      <td>0.055871</td>\n",
              "      <td>0.462658</td>\n",
              "      <td>0.677776</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.400108</td>\n",
              "      <td>0.668224</td>\n",
              "      <td>-0.026347</td>\n",
              "      <td>0.454312</td>\n",
              "      <td>0.672526</td>\n",
              "      <td>-0.062448</td>\n",
              "      <td>0.442451</td>\n",
              "      <td>0.308193</td>\n",
              "      <td>-0.157336</td>\n",
              "      <td>1473</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14731</th>\n",
              "      <td>0.452533</td>\n",
              "      <td>0.791973</td>\n",
              "      <td>-0.099548</td>\n",
              "      <td>0.335543</td>\n",
              "      <td>0.535332</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.284853</td>\n",
              "      <td>0.564165</td>\n",
              "      <td>0.029366</td>\n",
              "      <td>0.351197</td>\n",
              "      <td>0.637508</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>0.184646</td>\n",
              "      <td>0.126609</td>\n",
              "      <td>0.138878</td>\n",
              "      <td>0.201092</td>\n",
              "      <td>0.182896</td>\n",
              "      <td>0.081410</td>\n",
              "      <td>0.219815</td>\n",
              "      <td>0.240867</td>\n",
              "      <td>0.035379</td>\n",
              "      <td>0.358867</td>\n",
              "      <td>0.892018</td>\n",
              "      <td>-0.009873</td>\n",
              "      <td>0.362378</td>\n",
              "      <td>0.247935</td>\n",
              "      <td>-0.036624</td>\n",
              "      <td>0.353144</td>\n",
              "      <td>0.162810</td>\n",
              "      <td>-0.012710</td>\n",
              "      <td>0.343602</td>\n",
              "      <td>0.071618</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.451449</td>\n",
              "      <td>0.063975</td>\n",
              "      <td>-0.008453</td>\n",
              "      <td>0.239168</td>\n",
              "      <td>0.439385</td>\n",
              "      <td>0.086585</td>\n",
              "      <td>0.170206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.848754</td>\n",
              "      <td>-0.034963</td>\n",
              "      <td>0.452250</td>\n",
              "      <td>0.307410</td>\n",
              "      <td>-0.048951</td>\n",
              "      <td>0.375028</td>\n",
              "      <td>0.858712</td>\n",
              "      <td>-0.041104</td>\n",
              "      <td>0.368408</td>\n",
              "      <td>0.870020</td>\n",
              "      <td>-0.035212</td>\n",
              "      <td>0.335122</td>\n",
              "      <td>0.770662</td>\n",
              "      <td>-0.023500</td>\n",
              "      <td>0.156868</td>\n",
              "      <td>0.588514</td>\n",
              "      <td>0.422205</td>\n",
              "      <td>0.446972</td>\n",
              "      <td>0.681988</td>\n",
              "      <td>-0.115878</td>\n",
              "      <td>0.378910</td>\n",
              "      <td>0.843526</td>\n",
              "      <td>-0.012812</td>\n",
              "      <td>0.371462</td>\n",
              "      <td>0.847001</td>\n",
              "      <td>-0.016520</td>\n",
              "      <td>0.412317</td>\n",
              "      <td>0.695334</td>\n",
              "      <td>-0.079299</td>\n",
              "      <td>0.370427</td>\n",
              "      <td>0.685399</td>\n",
              "      <td>-0.037217</td>\n",
              "      <td>0.407154</td>\n",
              "      <td>0.687007</td>\n",
              "      <td>-0.078226</td>\n",
              "      <td>0.449482</td>\n",
              "      <td>0.251166</td>\n",
              "      <td>-0.053193</td>\n",
              "      <td>1474</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14732 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.516601  0.585682 -0.128171  ...  0.006673           0        0\n",
              "1      0.350142  0.704083 -0.051000  ... -0.123009           1        0\n",
              "2      0.478097  0.829416 -0.035864  ... -0.201465           2        0\n",
              "3      0.440407  0.633268 -0.130409  ... -0.042549           3        0\n",
              "4      0.508843  0.783509 -0.075715  ... -0.125290           4        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "14727  0.560496  0.769144 -0.111749  ... -0.043033        1470        6\n",
              "14728  0.491013  0.712008 -0.122762  ...  0.002328        1471        6\n",
              "14729  0.527434  0.738273 -0.079975  ... -0.125415        1472        6\n",
              "14730  0.528845  0.735577 -0.041305  ... -0.157336        1473        6\n",
              "14731  0.452533  0.791973 -0.099548  ... -0.053193        1474        6\n",
              "\n",
              "[14732 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "SGSiuoOG1Peq",
        "outputId": "65e339ef-e658-4653-ebe9-8d8702b77245"
      },
      "source": [
        "df_valid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.646184</td>\n",
              "      <td>0.771045</td>\n",
              "      <td>-0.064509</td>\n",
              "      <td>0.479793</td>\n",
              "      <td>0.566351</td>\n",
              "      <td>-0.088064</td>\n",
              "      <td>0.426735</td>\n",
              "      <td>0.590875</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>0.520683</td>\n",
              "      <td>0.649389</td>\n",
              "      <td>-0.101141</td>\n",
              "      <td>0.297111</td>\n",
              "      <td>0.243108</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>0.351343</td>\n",
              "      <td>0.308081</td>\n",
              "      <td>-0.161240</td>\n",
              "      <td>0.396239</td>\n",
              "      <td>0.377762</td>\n",
              "      <td>-0.175256</td>\n",
              "      <td>0.463307</td>\n",
              "      <td>0.887630</td>\n",
              "      <td>-0.003552</td>\n",
              "      <td>0.560809</td>\n",
              "      <td>0.397581</td>\n",
              "      <td>-0.148304</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.302792</td>\n",
              "      <td>-0.148388</td>\n",
              "      <td>0.513932</td>\n",
              "      <td>0.218670</td>\n",
              "      <td>-0.145369</td>\n",
              "      <td>0.618115</td>\n",
              "      <td>0.221551</td>\n",
              "      <td>-0.109952</td>\n",
              "      <td>0.356944</td>\n",
              "      <td>0.489348</td>\n",
              "      <td>-0.107089</td>\n",
              "      <td>0.258764</td>\n",
              "      <td>...</td>\n",
              "      <td>0.861163</td>\n",
              "      <td>-0.021057</td>\n",
              "      <td>0.630754</td>\n",
              "      <td>0.437828</td>\n",
              "      <td>-0.104779</td>\n",
              "      <td>0.503365</td>\n",
              "      <td>0.870509</td>\n",
              "      <td>-0.021248</td>\n",
              "      <td>0.491106</td>\n",
              "      <td>0.878412</td>\n",
              "      <td>-0.018093</td>\n",
              "      <td>0.482451</td>\n",
              "      <td>0.760005</td>\n",
              "      <td>-0.072059</td>\n",
              "      <td>0.072792</td>\n",
              "      <td>0.583212</td>\n",
              "      <td>0.110986</td>\n",
              "      <td>0.669708</td>\n",
              "      <td>0.690351</td>\n",
              "      <td>-0.098678</td>\n",
              "      <td>0.497744</td>\n",
              "      <td>0.844740</td>\n",
              "      <td>-0.020899</td>\n",
              "      <td>0.490786</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>-0.021804</td>\n",
              "      <td>0.605350</td>\n",
              "      <td>0.697076</td>\n",
              "      <td>-0.082129</td>\n",
              "      <td>0.535840</td>\n",
              "      <td>0.685600</td>\n",
              "      <td>-0.071414</td>\n",
              "      <td>0.600393</td>\n",
              "      <td>0.690829</td>\n",
              "      <td>-0.085879</td>\n",
              "      <td>0.633086</td>\n",
              "      <td>0.403363</td>\n",
              "      <td>-0.117815</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.505669</td>\n",
              "      <td>0.783729</td>\n",
              "      <td>-0.018900</td>\n",
              "      <td>0.383348</td>\n",
              "      <td>0.551676</td>\n",
              "      <td>-0.021686</td>\n",
              "      <td>0.335575</td>\n",
              "      <td>0.573097</td>\n",
              "      <td>-0.002283</td>\n",
              "      <td>0.399952</td>\n",
              "      <td>0.650358</td>\n",
              "      <td>-0.031835</td>\n",
              "      <td>0.229131</td>\n",
              "      <td>0.181487</td>\n",
              "      <td>-0.092331</td>\n",
              "      <td>0.258003</td>\n",
              "      <td>0.257158</td>\n",
              "      <td>-0.111276</td>\n",
              "      <td>0.284237</td>\n",
              "      <td>0.338158</td>\n",
              "      <td>-0.123996</td>\n",
              "      <td>0.408235</td>\n",
              "      <td>0.862809</td>\n",
              "      <td>0.118722</td>\n",
              "      <td>0.414858</td>\n",
              "      <td>0.365437</td>\n",
              "      <td>-0.173962</td>\n",
              "      <td>0.397574</td>\n",
              "      <td>0.252688</td>\n",
              "      <td>-0.191834</td>\n",
              "      <td>0.380021</td>\n",
              "      <td>0.148873</td>\n",
              "      <td>-0.204391</td>\n",
              "      <td>0.480591</td>\n",
              "      <td>0.143603</td>\n",
              "      <td>-0.224993</td>\n",
              "      <td>0.294881</td>\n",
              "      <td>0.460291</td>\n",
              "      <td>-0.004822</td>\n",
              "      <td>0.217984</td>\n",
              "      <td>...</td>\n",
              "      <td>0.831755</td>\n",
              "      <td>0.075494</td>\n",
              "      <td>0.490290</td>\n",
              "      <td>0.410842</td>\n",
              "      <td>-0.158440</td>\n",
              "      <td>0.426757</td>\n",
              "      <td>0.840886</td>\n",
              "      <td>0.078719</td>\n",
              "      <td>0.419972</td>\n",
              "      <td>0.850027</td>\n",
              "      <td>0.088572</td>\n",
              "      <td>0.383190</td>\n",
              "      <td>0.750469</td>\n",
              "      <td>0.039978</td>\n",
              "      <td>0.169034</td>\n",
              "      <td>0.535456</td>\n",
              "      <td>0.333599</td>\n",
              "      <td>0.499905</td>\n",
              "      <td>0.714165</td>\n",
              "      <td>-0.075163</td>\n",
              "      <td>0.430074</td>\n",
              "      <td>0.816941</td>\n",
              "      <td>0.089510</td>\n",
              "      <td>0.422110</td>\n",
              "      <td>0.818853</td>\n",
              "      <td>0.090431</td>\n",
              "      <td>0.462675</td>\n",
              "      <td>0.711085</td>\n",
              "      <td>-0.036148</td>\n",
              "      <td>0.415931</td>\n",
              "      <td>0.687106</td>\n",
              "      <td>-0.008565</td>\n",
              "      <td>0.457287</td>\n",
              "      <td>0.704161</td>\n",
              "      <td>-0.042259</td>\n",
              "      <td>0.487358</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>-0.183734</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.570398</td>\n",
              "      <td>0.522947</td>\n",
              "      <td>-0.131259</td>\n",
              "      <td>0.385422</td>\n",
              "      <td>0.250885</td>\n",
              "      <td>-0.050247</td>\n",
              "      <td>0.313107</td>\n",
              "      <td>0.278246</td>\n",
              "      <td>-0.048979</td>\n",
              "      <td>0.409144</td>\n",
              "      <td>0.370157</td>\n",
              "      <td>-0.113957</td>\n",
              "      <td>0.206225</td>\n",
              "      <td>-0.195378</td>\n",
              "      <td>0.061084</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>-0.130779</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.276074</td>\n",
              "      <td>-0.063120</td>\n",
              "      <td>-0.046078</td>\n",
              "      <td>0.358516</td>\n",
              "      <td>0.730264</td>\n",
              "      <td>0.017391</td>\n",
              "      <td>0.468482</td>\n",
              "      <td>-0.048374</td>\n",
              "      <td>-0.084189</td>\n",
              "      <td>0.454622</td>\n",
              "      <td>-0.150500</td>\n",
              "      <td>-0.053712</td>\n",
              "      <td>0.437219</td>\n",
              "      <td>-0.252034</td>\n",
              "      <td>-0.025488</td>\n",
              "      <td>0.571085</td>\n",
              "      <td>-0.254265</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.253504</td>\n",
              "      <td>0.142910</td>\n",
              "      <td>0.007577</td>\n",
              "      <td>0.149256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.706014</td>\n",
              "      <td>-0.006454</td>\n",
              "      <td>0.569965</td>\n",
              "      <td>0.023101</td>\n",
              "      <td>-0.076176</td>\n",
              "      <td>0.390708</td>\n",
              "      <td>0.716519</td>\n",
              "      <td>-0.009776</td>\n",
              "      <td>0.376529</td>\n",
              "      <td>0.725704</td>\n",
              "      <td>-0.003093</td>\n",
              "      <td>0.352764</td>\n",
              "      <td>0.512117</td>\n",
              "      <td>-0.059380</td>\n",
              "      <td>0.041792</td>\n",
              "      <td>0.330219</td>\n",
              "      <td>0.345695</td>\n",
              "      <td>0.581562</td>\n",
              "      <td>0.438015</td>\n",
              "      <td>-0.170266</td>\n",
              "      <td>0.391589</td>\n",
              "      <td>0.667979</td>\n",
              "      <td>0.011425</td>\n",
              "      <td>0.375232</td>\n",
              "      <td>0.668794</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.507817</td>\n",
              "      <td>0.442177</td>\n",
              "      <td>-0.133319</td>\n",
              "      <td>0.426135</td>\n",
              "      <td>0.421102</td>\n",
              "      <td>-0.094905</td>\n",
              "      <td>0.501055</td>\n",
              "      <td>0.433366</td>\n",
              "      <td>-0.135095</td>\n",
              "      <td>0.571534</td>\n",
              "      <td>-0.036601</td>\n",
              "      <td>-0.077871</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.550640</td>\n",
              "      <td>0.693204</td>\n",
              "      <td>-0.039426</td>\n",
              "      <td>0.377895</td>\n",
              "      <td>0.420303</td>\n",
              "      <td>-0.049635</td>\n",
              "      <td>0.310557</td>\n",
              "      <td>0.444947</td>\n",
              "      <td>-0.030445</td>\n",
              "      <td>0.395424</td>\n",
              "      <td>0.535625</td>\n",
              "      <td>-0.071252</td>\n",
              "      <td>0.196327</td>\n",
              "      <td>0.037854</td>\n",
              "      <td>-0.122527</td>\n",
              "      <td>0.226640</td>\n",
              "      <td>0.102505</td>\n",
              "      <td>-0.149672</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.168133</td>\n",
              "      <td>-0.169691</td>\n",
              "      <td>0.385633</td>\n",
              "      <td>0.777333</td>\n",
              "      <td>0.130624</td>\n",
              "      <td>0.440921</td>\n",
              "      <td>0.182286</td>\n",
              "      <td>-0.220092</td>\n",
              "      <td>0.426143</td>\n",
              "      <td>0.093054</td>\n",
              "      <td>-0.238092</td>\n",
              "      <td>0.408764</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.251741</td>\n",
              "      <td>0.543859</td>\n",
              "      <td>-0.005355</td>\n",
              "      <td>-0.265706</td>\n",
              "      <td>0.259817</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>-0.029762</td>\n",
              "      <td>0.159518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.744009</td>\n",
              "      <td>0.082338</td>\n",
              "      <td>0.545075</td>\n",
              "      <td>0.237159</td>\n",
              "      <td>-0.195135</td>\n",
              "      <td>0.414130</td>\n",
              "      <td>0.752458</td>\n",
              "      <td>0.083467</td>\n",
              "      <td>0.404759</td>\n",
              "      <td>0.761788</td>\n",
              "      <td>0.094698</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>0.646520</td>\n",
              "      <td>0.029753</td>\n",
              "      <td>0.087381</td>\n",
              "      <td>0.425480</td>\n",
              "      <td>0.414335</td>\n",
              "      <td>0.550101</td>\n",
              "      <td>0.611782</td>\n",
              "      <td>-0.120470</td>\n",
              "      <td>0.411218</td>\n",
              "      <td>0.722901</td>\n",
              "      <td>0.103741</td>\n",
              "      <td>0.399978</td>\n",
              "      <td>0.723879</td>\n",
              "      <td>0.104076</td>\n",
              "      <td>0.488015</td>\n",
              "      <td>0.608906</td>\n",
              "      <td>-0.072477</td>\n",
              "      <td>0.414568</td>\n",
              "      <td>0.579445</td>\n",
              "      <td>-0.037483</td>\n",
              "      <td>0.479306</td>\n",
              "      <td>0.600204</td>\n",
              "      <td>-0.079331</td>\n",
              "      <td>0.544766</td>\n",
              "      <td>0.188114</td>\n",
              "      <td>-0.223748</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.524232</td>\n",
              "      <td>0.722622</td>\n",
              "      <td>-0.049771</td>\n",
              "      <td>0.378299</td>\n",
              "      <td>0.537701</td>\n",
              "      <td>-0.022929</td>\n",
              "      <td>0.332367</td>\n",
              "      <td>0.561998</td>\n",
              "      <td>-0.011429</td>\n",
              "      <td>0.401680</td>\n",
              "      <td>0.617495</td>\n",
              "      <td>-0.051351</td>\n",
              "      <td>0.205039</td>\n",
              "      <td>0.227481</td>\n",
              "      <td>-0.018862</td>\n",
              "      <td>0.235042</td>\n",
              "      <td>0.282814</td>\n",
              "      <td>-0.050678</td>\n",
              "      <td>0.265167</td>\n",
              "      <td>0.346141</td>\n",
              "      <td>-0.075391</td>\n",
              "      <td>0.425222</td>\n",
              "      <td>0.818088</td>\n",
              "      <td>0.049098</td>\n",
              "      <td>0.399753</td>\n",
              "      <td>0.352436</td>\n",
              "      <td>-0.117018</td>\n",
              "      <td>0.375707</td>\n",
              "      <td>0.255840</td>\n",
              "      <td>-0.116756</td>\n",
              "      <td>0.351857</td>\n",
              "      <td>0.170662</td>\n",
              "      <td>-0.111585</td>\n",
              "      <td>0.448775</td>\n",
              "      <td>0.154980</td>\n",
              "      <td>-0.122691</td>\n",
              "      <td>0.283837</td>\n",
              "      <td>0.472371</td>\n",
              "      <td>0.007503</td>\n",
              "      <td>0.215384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.779411</td>\n",
              "      <td>0.023573</td>\n",
              "      <td>0.478230</td>\n",
              "      <td>0.386732</td>\n",
              "      <td>-0.106638</td>\n",
              "      <td>0.441388</td>\n",
              "      <td>0.787471</td>\n",
              "      <td>0.022020</td>\n",
              "      <td>0.435998</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.027950</td>\n",
              "      <td>0.390656</td>\n",
              "      <td>0.715117</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.174290</td>\n",
              "      <td>0.598808</td>\n",
              "      <td>0.309484</td>\n",
              "      <td>0.511831</td>\n",
              "      <td>0.657377</td>\n",
              "      <td>-0.094378</td>\n",
              "      <td>0.437287</td>\n",
              "      <td>0.769950</td>\n",
              "      <td>0.040689</td>\n",
              "      <td>0.429929</td>\n",
              "      <td>0.771605</td>\n",
              "      <td>0.039537</td>\n",
              "      <td>0.471688</td>\n",
              "      <td>0.662292</td>\n",
              "      <td>-0.062175</td>\n",
              "      <td>0.420203</td>\n",
              "      <td>0.650105</td>\n",
              "      <td>-0.033778</td>\n",
              "      <td>0.465479</td>\n",
              "      <td>0.656399</td>\n",
              "      <td>-0.065202</td>\n",
              "      <td>0.473144</td>\n",
              "      <td>0.346925</td>\n",
              "      <td>-0.121656</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14631</th>\n",
              "      <td>0.529442</td>\n",
              "      <td>0.667210</td>\n",
              "      <td>-0.066573</td>\n",
              "      <td>0.386126</td>\n",
              "      <td>0.486287</td>\n",
              "      <td>-0.033634</td>\n",
              "      <td>0.341563</td>\n",
              "      <td>0.510485</td>\n",
              "      <td>-0.029726</td>\n",
              "      <td>0.418692</td>\n",
              "      <td>0.560215</td>\n",
              "      <td>-0.061615</td>\n",
              "      <td>0.219367</td>\n",
              "      <td>0.183714</td>\n",
              "      <td>-0.007092</td>\n",
              "      <td>0.248392</td>\n",
              "      <td>0.228448</td>\n",
              "      <td>-0.037222</td>\n",
              "      <td>0.276280</td>\n",
              "      <td>0.271904</td>\n",
              "      <td>-0.061118</td>\n",
              "      <td>0.423037</td>\n",
              "      <td>0.759102</td>\n",
              "      <td>-0.001626</td>\n",
              "      <td>0.413478</td>\n",
              "      <td>0.273705</td>\n",
              "      <td>-0.080519</td>\n",
              "      <td>0.398730</td>\n",
              "      <td>0.209388</td>\n",
              "      <td>-0.074626</td>\n",
              "      <td>0.379055</td>\n",
              "      <td>0.137959</td>\n",
              "      <td>-0.063146</td>\n",
              "      <td>0.475969</td>\n",
              "      <td>0.127867</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>0.284961</td>\n",
              "      <td>0.422919</td>\n",
              "      <td>-0.011928</td>\n",
              "      <td>0.216206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.732706</td>\n",
              "      <td>-0.021548</td>\n",
              "      <td>0.494808</td>\n",
              "      <td>0.313197</td>\n",
              "      <td>-0.065520</td>\n",
              "      <td>0.445838</td>\n",
              "      <td>0.741607</td>\n",
              "      <td>-0.024574</td>\n",
              "      <td>0.437814</td>\n",
              "      <td>0.749662</td>\n",
              "      <td>-0.020488</td>\n",
              "      <td>0.407561</td>\n",
              "      <td>0.660633</td>\n",
              "      <td>-0.038206</td>\n",
              "      <td>0.153465</td>\n",
              "      <td>0.517849</td>\n",
              "      <td>0.231584</td>\n",
              "      <td>0.524237</td>\n",
              "      <td>0.586032</td>\n",
              "      <td>-0.089157</td>\n",
              "      <td>0.447015</td>\n",
              "      <td>0.723673</td>\n",
              "      <td>-0.012398</td>\n",
              "      <td>0.440423</td>\n",
              "      <td>0.727365</td>\n",
              "      <td>-0.014400</td>\n",
              "      <td>0.484241</td>\n",
              "      <td>0.597897</td>\n",
              "      <td>-0.066358</td>\n",
              "      <td>0.436844</td>\n",
              "      <td>0.592494</td>\n",
              "      <td>-0.044895</td>\n",
              "      <td>0.479310</td>\n",
              "      <td>0.592026</td>\n",
              "      <td>-0.067038</td>\n",
              "      <td>0.491907</td>\n",
              "      <td>0.271605</td>\n",
              "      <td>-0.073554</td>\n",
              "      <td>1469</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14632</th>\n",
              "      <td>0.500539</td>\n",
              "      <td>0.734860</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>0.327387</td>\n",
              "      <td>0.516931</td>\n",
              "      <td>-0.010588</td>\n",
              "      <td>0.265571</td>\n",
              "      <td>0.545812</td>\n",
              "      <td>-0.007016</td>\n",
              "      <td>0.361814</td>\n",
              "      <td>0.604152</td>\n",
              "      <td>-0.083395</td>\n",
              "      <td>0.160265</td>\n",
              "      <td>0.151897</td>\n",
              "      <td>0.173618</td>\n",
              "      <td>0.184835</td>\n",
              "      <td>0.188428</td>\n",
              "      <td>0.104391</td>\n",
              "      <td>0.210830</td>\n",
              "      <td>0.227435</td>\n",
              "      <td>0.047180</td>\n",
              "      <td>0.350158</td>\n",
              "      <td>0.870057</td>\n",
              "      <td>-0.090012</td>\n",
              "      <td>0.386060</td>\n",
              "      <td>0.230485</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.377281</td>\n",
              "      <td>0.166759</td>\n",
              "      <td>0.043994</td>\n",
              "      <td>0.364579</td>\n",
              "      <td>0.095391</td>\n",
              "      <td>0.089954</td>\n",
              "      <td>0.487561</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.089601</td>\n",
              "      <td>0.203078</td>\n",
              "      <td>0.435127</td>\n",
              "      <td>0.063342</td>\n",
              "      <td>0.114195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820860</td>\n",
              "      <td>-0.104007</td>\n",
              "      <td>0.487804</td>\n",
              "      <td>0.291259</td>\n",
              "      <td>-0.005642</td>\n",
              "      <td>0.382895</td>\n",
              "      <td>0.832251</td>\n",
              "      <td>-0.113706</td>\n",
              "      <td>0.372782</td>\n",
              "      <td>0.845191</td>\n",
              "      <td>-0.110114</td>\n",
              "      <td>0.337598</td>\n",
              "      <td>0.735444</td>\n",
              "      <td>-0.089631</td>\n",
              "      <td>0.034739</td>\n",
              "      <td>0.615029</td>\n",
              "      <td>0.373997</td>\n",
              "      <td>0.498705</td>\n",
              "      <td>0.626003</td>\n",
              "      <td>-0.145978</td>\n",
              "      <td>0.384501</td>\n",
              "      <td>0.817192</td>\n",
              "      <td>-0.083325</td>\n",
              "      <td>0.376356</td>\n",
              "      <td>0.821481</td>\n",
              "      <td>-0.089008</td>\n",
              "      <td>0.447718</td>\n",
              "      <td>0.647504</td>\n",
              "      <td>-0.116903</td>\n",
              "      <td>0.385622</td>\n",
              "      <td>0.649125</td>\n",
              "      <td>-0.076227</td>\n",
              "      <td>0.441510</td>\n",
              "      <td>0.639893</td>\n",
              "      <td>-0.113286</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.238567</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>1470</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14633</th>\n",
              "      <td>0.494283</td>\n",
              "      <td>0.710024</td>\n",
              "      <td>-0.116081</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.497803</td>\n",
              "      <td>-0.009445</td>\n",
              "      <td>0.295208</td>\n",
              "      <td>0.520917</td>\n",
              "      <td>-0.001318</td>\n",
              "      <td>0.381529</td>\n",
              "      <td>0.580622</td>\n",
              "      <td>-0.068246</td>\n",
              "      <td>0.204759</td>\n",
              "      <td>0.123697</td>\n",
              "      <td>0.127479</td>\n",
              "      <td>0.225363</td>\n",
              "      <td>0.164128</td>\n",
              "      <td>0.072507</td>\n",
              "      <td>0.249508</td>\n",
              "      <td>0.206157</td>\n",
              "      <td>0.027697</td>\n",
              "      <td>0.370819</td>\n",
              "      <td>0.833998</td>\n",
              "      <td>-0.037612</td>\n",
              "      <td>0.400362</td>\n",
              "      <td>0.218823</td>\n",
              "      <td>-0.020845</td>\n",
              "      <td>0.388722</td>\n",
              "      <td>0.150536</td>\n",
              "      <td>0.007938</td>\n",
              "      <td>0.374952</td>\n",
              "      <td>0.073443</td>\n",
              "      <td>0.040432</td>\n",
              "      <td>0.483528</td>\n",
              "      <td>0.071126</td>\n",
              "      <td>0.034102</td>\n",
              "      <td>0.244181</td>\n",
              "      <td>0.414586</td>\n",
              "      <td>0.053564</td>\n",
              "      <td>0.170244</td>\n",
              "      <td>...</td>\n",
              "      <td>0.797283</td>\n",
              "      <td>-0.057481</td>\n",
              "      <td>0.491774</td>\n",
              "      <td>0.282192</td>\n",
              "      <td>-0.024813</td>\n",
              "      <td>0.390480</td>\n",
              "      <td>0.806401</td>\n",
              "      <td>-0.063569</td>\n",
              "      <td>0.381867</td>\n",
              "      <td>0.816910</td>\n",
              "      <td>-0.058509</td>\n",
              "      <td>0.356374</td>\n",
              "      <td>0.697895</td>\n",
              "      <td>-0.055486</td>\n",
              "      <td>0.130981</td>\n",
              "      <td>0.558962</td>\n",
              "      <td>0.341036</td>\n",
              "      <td>0.493862</td>\n",
              "      <td>0.611363</td>\n",
              "      <td>-0.122746</td>\n",
              "      <td>0.395654</td>\n",
              "      <td>0.785397</td>\n",
              "      <td>-0.038678</td>\n",
              "      <td>0.386269</td>\n",
              "      <td>0.787321</td>\n",
              "      <td>-0.042230</td>\n",
              "      <td>0.452422</td>\n",
              "      <td>0.626979</td>\n",
              "      <td>-0.094577</td>\n",
              "      <td>0.401670</td>\n",
              "      <td>0.622043</td>\n",
              "      <td>-0.060001</td>\n",
              "      <td>0.447939</td>\n",
              "      <td>0.619601</td>\n",
              "      <td>-0.092344</td>\n",
              "      <td>0.490952</td>\n",
              "      <td>0.228598</td>\n",
              "      <td>-0.023898</td>\n",
              "      <td>1471</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14634</th>\n",
              "      <td>0.480344</td>\n",
              "      <td>0.614472</td>\n",
              "      <td>-0.073619</td>\n",
              "      <td>0.351348</td>\n",
              "      <td>0.359926</td>\n",
              "      <td>-0.029388</td>\n",
              "      <td>0.296622</td>\n",
              "      <td>0.382071</td>\n",
              "      <td>-0.013036</td>\n",
              "      <td>0.371280</td>\n",
              "      <td>0.466668</td>\n",
              "      <td>-0.062456</td>\n",
              "      <td>0.200838</td>\n",
              "      <td>-0.066494</td>\n",
              "      <td>-0.011770</td>\n",
              "      <td>0.220040</td>\n",
              "      <td>-0.001829</td>\n",
              "      <td>-0.048701</td>\n",
              "      <td>0.240770</td>\n",
              "      <td>0.061476</td>\n",
              "      <td>-0.075901</td>\n",
              "      <td>0.363895</td>\n",
              "      <td>0.734271</td>\n",
              "      <td>0.071702</td>\n",
              "      <td>0.399486</td>\n",
              "      <td>0.096403</td>\n",
              "      <td>-0.128011</td>\n",
              "      <td>0.393551</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.130443</td>\n",
              "      <td>0.384674</td>\n",
              "      <td>-0.095847</td>\n",
              "      <td>-0.123506</td>\n",
              "      <td>0.504907</td>\n",
              "      <td>-0.092631</td>\n",
              "      <td>-0.136478</td>\n",
              "      <td>0.245395</td>\n",
              "      <td>0.251886</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.169527</td>\n",
              "      <td>...</td>\n",
              "      <td>0.721825</td>\n",
              "      <td>0.031216</td>\n",
              "      <td>0.494581</td>\n",
              "      <td>0.161664</td>\n",
              "      <td>-0.117363</td>\n",
              "      <td>0.387729</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.031917</td>\n",
              "      <td>0.376631</td>\n",
              "      <td>0.738281</td>\n",
              "      <td>0.040350</td>\n",
              "      <td>0.353397</td>\n",
              "      <td>0.587014</td>\n",
              "      <td>-0.009165</td>\n",
              "      <td>0.138784</td>\n",
              "      <td>0.330477</td>\n",
              "      <td>0.364592</td>\n",
              "      <td>0.478526</td>\n",
              "      <td>0.523328</td>\n",
              "      <td>-0.111924</td>\n",
              "      <td>0.401928</td>\n",
              "      <td>0.697973</td>\n",
              "      <td>0.040930</td>\n",
              "      <td>0.390769</td>\n",
              "      <td>0.701493</td>\n",
              "      <td>0.040458</td>\n",
              "      <td>0.437818</td>\n",
              "      <td>0.528118</td>\n",
              "      <td>-0.072447</td>\n",
              "      <td>0.389704</td>\n",
              "      <td>0.510692</td>\n",
              "      <td>-0.040840</td>\n",
              "      <td>0.432847</td>\n",
              "      <td>0.520244</td>\n",
              "      <td>-0.076076</td>\n",
              "      <td>0.496153</td>\n",
              "      <td>0.107584</td>\n",
              "      <td>-0.134429</td>\n",
              "      <td>1472</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>0.451848</td>\n",
              "      <td>0.622114</td>\n",
              "      <td>-0.181673</td>\n",
              "      <td>0.305574</td>\n",
              "      <td>0.401656</td>\n",
              "      <td>-0.015953</td>\n",
              "      <td>0.233015</td>\n",
              "      <td>0.421863</td>\n",
              "      <td>-0.015226</td>\n",
              "      <td>0.335668</td>\n",
              "      <td>0.477531</td>\n",
              "      <td>-0.099770</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>0.018560</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.221613</td>\n",
              "      <td>0.058618</td>\n",
              "      <td>0.131506</td>\n",
              "      <td>0.242395</td>\n",
              "      <td>0.106519</td>\n",
              "      <td>0.064972</td>\n",
              "      <td>0.257232</td>\n",
              "      <td>0.817205</td>\n",
              "      <td>-0.085517</td>\n",
              "      <td>0.429682</td>\n",
              "      <td>0.147278</td>\n",
              "      <td>0.012527</td>\n",
              "      <td>0.433578</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.067867</td>\n",
              "      <td>0.434073</td>\n",
              "      <td>-0.013070</td>\n",
              "      <td>0.124950</td>\n",
              "      <td>0.568935</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.123650</td>\n",
              "      <td>0.182558</td>\n",
              "      <td>0.309983</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>0.081712</td>\n",
              "      <td>...</td>\n",
              "      <td>0.787673</td>\n",
              "      <td>-0.104376</td>\n",
              "      <td>0.523025</td>\n",
              "      <td>0.221324</td>\n",
              "      <td>0.002962</td>\n",
              "      <td>0.306585</td>\n",
              "      <td>0.796358</td>\n",
              "      <td>-0.111493</td>\n",
              "      <td>0.289647</td>\n",
              "      <td>0.805510</td>\n",
              "      <td>-0.107110</td>\n",
              "      <td>0.283806</td>\n",
              "      <td>0.617631</td>\n",
              "      <td>-0.111347</td>\n",
              "      <td>-0.033763</td>\n",
              "      <td>0.493091</td>\n",
              "      <td>0.385915</td>\n",
              "      <td>0.465378</td>\n",
              "      <td>0.510059</td>\n",
              "      <td>-0.169199</td>\n",
              "      <td>0.322039</td>\n",
              "      <td>0.765549</td>\n",
              "      <td>-0.091213</td>\n",
              "      <td>0.309973</td>\n",
              "      <td>0.766603</td>\n",
              "      <td>-0.096392</td>\n",
              "      <td>0.413244</td>\n",
              "      <td>0.528470</td>\n",
              "      <td>-0.136841</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>0.525035</td>\n",
              "      <td>-0.093184</td>\n",
              "      <td>0.409769</td>\n",
              "      <td>0.520118</td>\n",
              "      <td>-0.133804</td>\n",
              "      <td>0.533610</td>\n",
              "      <td>0.171482</td>\n",
              "      <td>0.013007</td>\n",
              "      <td>1473</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14636 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.646184  0.771045 -0.064509  ... -0.117815           0        0\n",
              "1      0.505669  0.783729 -0.018900  ... -0.183734           1        0\n",
              "2      0.570398  0.522947 -0.131259  ... -0.077871           3        0\n",
              "3      0.550640  0.693204 -0.039426  ... -0.223748           4        0\n",
              "4      0.524232  0.722622 -0.049771  ... -0.121656           5        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "14631  0.529442  0.667210 -0.066573  ... -0.073554        1469        6\n",
              "14632  0.500539  0.734860 -0.148886  ...  0.000434        1470        6\n",
              "14633  0.494283  0.710024 -0.116081  ... -0.023898        1471        6\n",
              "14634  0.480344  0.614472 -0.073619  ... -0.134429        1472        6\n",
              "14635  0.451848  0.622114 -0.181673  ...  0.013007        1473        6\n",
              "\n",
              "[14636 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT4rg5Ugl6ll"
      },
      "source": [
        "Unnameはいらないので削除"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvzA8Gy1EUhN"
      },
      "source": [
        "df_train=df_train.drop(columns=df_train.columns[[-2]],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OqO9I_q6S1z"
      },
      "source": [
        "967,5,17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrF0BgW-1UTy"
      },
      "source": [
        "df_test=df_test.drop(columns=df_test.columns[[-2]],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDKn2CRL1V_6"
      },
      "source": [
        "df_valid=df_valid.drop(columns=df_valid.columns[[-2]],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4gkpPe6eh6W"
      },
      "source": [
        "3659,4  \n",
        "30,487"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBmlmqc1mNui"
      },
      "source": [
        "それぞれのデータを目的変数と説明変数に分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xf5l4ylWS_V"
      },
      "source": [
        "x_train=DataFrame(df_train.drop(\"correct\",axis=1))\n",
        "y_train=DataFrame(df_train[\"correct\"])\n",
        "\n",
        "x_test=DataFrame(df_test.drop(\"correct\",axis=1))\n",
        "y_test=DataFrame(df_test[\"correct\"])\n",
        "\n",
        "x_valid=DataFrame(df_valid.drop(\"correct\",axis=1))\n",
        "y_valid=DataFrame(df_valid[\"correct\"])\n",
        "\n",
        "#データの整形\n",
        "x_train = x_train.astype(np.float)\n",
        "x_test = x_test.astype(np.float)\n",
        "x_valid = x_valid.astype(np.float)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,7)\n",
        "y_test = np_utils.to_categorical(y_test,7)\n",
        "y_valid = np_utils.to_categorical(y_valid,7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "J1MVLX0luYdl",
        "outputId": "6c187439-7b5c-47b1-b8e5-ce8724fc8c48"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>88_z</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020444</td>\n",
              "      <td>0.405580</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064467</td>\n",
              "      <td>0.362563</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031648</td>\n",
              "      <td>0.629462</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072879</td>\n",
              "      <td>0.406199</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.058224</td>\n",
              "      <td>0.077165</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025769</td>\n",
              "      <td>0.366158</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025465</td>\n",
              "      <td>0.392929</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.047505</td>\n",
              "      <td>0.429142</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.055350</td>\n",
              "      <td>0.469884</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026445</td>\n",
              "      <td>0.405460</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1404 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_x       9_y       9_z\n",
              "0      0.470799  0.682127 -0.080930  ...  0.428503  0.269363 -0.045183\n",
              "1      0.469567  0.799870 -0.017644  ...  0.513777  0.319253 -0.206077\n",
              "2      0.711111  0.717369 -0.049849  ...  0.565757  0.328598 -0.146556\n",
              "3      0.483527  0.809326 -0.011760  ...  0.464442  0.411362 -0.195280\n",
              "4      0.061688  0.692855 -0.050553  ...  0.126254  0.306380 -0.058228\n",
              "...         ...       ...       ...  ...       ...       ...       ...\n",
              "82190  0.436367  0.787778 -0.124726  ...  0.507884  0.236569 -0.051585\n",
              "82191  0.456891  0.704518 -0.093469  ...  0.485513  0.282853 -0.061855\n",
              "82192  0.502264  0.729350 -0.124422  ...  0.488749  0.268140 -0.034920\n",
              "82193  0.539922  0.733853 -0.116346  ...  0.486013  0.276309 -0.023272\n",
              "82194  0.508446  0.805487 -0.070251  ...  0.487336  0.245632 -0.153780\n",
              "\n",
              "[82195 rows x 1404 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZIebnHV0MAz"
      },
      "source": [
        "batch_size=256 #バッチサイズ\n",
        "n_rnn = 1  # 時系列の数\n",
        "n_sample = len(x_train)-n_rnn  # サンプル数\n",
        "n_in=len(df_train.columns)-1 #入力層のニューロン数\n",
        "n_mid=128 #中間層のニューロン数\n",
        "n_out=7 #出力層のニューロン数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsc2iW2weMba",
        "outputId": "45a8759a-c569-4624-fa3d-02d7bffb14e4"
      },
      "source": [
        "n_in"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1404"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUtFTKJkmdSl"
      },
      "source": [
        "モデル構築  \n",
        "SimpleRNN:全結合の中間層が再起的になる。  \n",
        "LSTM:RNNの発展版であるLSTMを活用できる。複雑な時系列データを扱えるが学習に時間がかかる  \n",
        "GRU: LSTMの簡易版。パラメータが少ないので学習に時間がかからない  \n",
        "参考サイト:https://wagtail.cds.tohoku.ac.jp/coda/python/machine-learning/recurrent-net-1.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sL9ng-uKFK5"
      },
      "source": [
        "#ニューラルネットワークの実装①\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_in,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_in,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_in,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4rTm8KqW0jK",
        "outputId": "221f4601-9349-4d2e-c139-91376f967a7b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 50)                70250     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 7)                 357       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75,707\n",
            "Trainable params: 75,707\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdrsd9iXABvZ"
      },
      "source": [
        "adam = Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZDO6Y1hfTbP"
      },
      "source": [
        "model.save('/content/drive/MyDrive/data分析/Mediapipe/NN_Mediapipe_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy-s8F2KiZpk"
      },
      "source": [
        "LSTM:44%  \n",
        "NN: 54%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGnhC5IwkpQu",
        "outputId": "5a3969b6-0f6e-4c70-9b53-f0a34498e6af"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "modelCheckpoint = ModelCheckpoint(filepath ='/content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5',\n",
        "                                  monitor='val_accuracy',\n",
        "                                  verbose=1,\n",
        "                                  save_best_only=True,\n",
        "                                  save_weights_only=False,\n",
        "                                  mode='max',\n",
        "                                  period=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tqXuJjsmhTy"
      },
      "source": [
        "##  学習  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "704KOTdfW_7y",
        "outputId": "26bc59a3-2645-410d-dea5-fccdce827ac9"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=500,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    validation_batch_size=64,\n",
        "                    callbacks=[modelCheckpoint]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.7180 - accuracy: 0.3243\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.37790, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.7175 - accuracy: 0.3244 - val_loss: 1.6257 - val_accuracy: 0.3779\n",
            "Epoch 2/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.6345 - accuracy: 0.3788\n",
            "Epoch 00002: val_accuracy improved from 0.37790 to 0.39984, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.6343 - accuracy: 0.3790 - val_loss: 1.5889 - val_accuracy: 0.3998\n",
            "Epoch 3/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.5479 - accuracy: 0.4404\n",
            "Epoch 00003: val_accuracy improved from 0.39984 to 0.42484, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.5473 - accuracy: 0.4408 - val_loss: 1.5443 - val_accuracy: 0.4248\n",
            "Epoch 4/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.4582 - accuracy: 0.4921\n",
            "Epoch 00004: val_accuracy improved from 0.42484 to 0.45477, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.4582 - accuracy: 0.4921 - val_loss: 1.5170 - val_accuracy: 0.4548\n",
            "Epoch 5/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.4126 - accuracy: 0.5124\n",
            "Epoch 00005: val_accuracy did not improve from 0.45477\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.4128 - accuracy: 0.5123 - val_loss: 1.5339 - val_accuracy: 0.4475\n",
            "Epoch 6/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.3865 - accuracy: 0.5191\n",
            "Epoch 00006: val_accuracy improved from 0.45477 to 0.46912, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3866 - accuracy: 0.5189 - val_loss: 1.4788 - val_accuracy: 0.4691\n",
            "Epoch 7/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.3694 - accuracy: 0.5267\n",
            "Epoch 00007: val_accuracy improved from 0.46912 to 0.47001, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3694 - accuracy: 0.5265 - val_loss: 1.4990 - val_accuracy: 0.4700\n",
            "Epoch 8/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.3527 - accuracy: 0.5309\n",
            "Epoch 00008: val_accuracy improved from 0.47001 to 0.47356, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3528 - accuracy: 0.5309 - val_loss: 1.4649 - val_accuracy: 0.4736\n",
            "Epoch 9/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.3400 - accuracy: 0.5381\n",
            "Epoch 00009: val_accuracy did not improve from 0.47356\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.3403 - accuracy: 0.5383 - val_loss: 1.4548 - val_accuracy: 0.4716\n",
            "Epoch 10/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.3308 - accuracy: 0.5455\n",
            "Epoch 00010: val_accuracy improved from 0.47356 to 0.49945, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.3308 - accuracy: 0.5455 - val_loss: 1.4333 - val_accuracy: 0.4995\n",
            "Epoch 11/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.3212 - accuracy: 0.5519\n",
            "Epoch 00011: val_accuracy improved from 0.49945 to 0.50225, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.3212 - accuracy: 0.5519 - val_loss: 1.4241 - val_accuracy: 0.5023\n",
            "Epoch 12/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.3138 - accuracy: 0.5553\n",
            "Epoch 00012: val_accuracy improved from 0.50225 to 0.50478, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.3138 - accuracy: 0.5553 - val_loss: 1.4149 - val_accuracy: 0.5048\n",
            "Epoch 13/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.3084 - accuracy: 0.5586\n",
            "Epoch 00013: val_accuracy improved from 0.50478 to 0.50779, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3085 - accuracy: 0.5587 - val_loss: 1.4025 - val_accuracy: 0.5078\n",
            "Epoch 14/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.3030 - accuracy: 0.5598\n",
            "Epoch 00014: val_accuracy did not improve from 0.50779\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3027 - accuracy: 0.5600 - val_loss: 1.4092 - val_accuracy: 0.5044\n",
            "Epoch 15/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2946 - accuracy: 0.5644\n",
            "Epoch 00015: val_accuracy did not improve from 0.50779\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2949 - accuracy: 0.5643 - val_loss: 1.4522 - val_accuracy: 0.4982\n",
            "Epoch 16/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2944 - accuracy: 0.5640\n",
            "Epoch 00016: val_accuracy did not improve from 0.50779\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2946 - accuracy: 0.5641 - val_loss: 1.4283 - val_accuracy: 0.5013\n",
            "Epoch 17/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2873 - accuracy: 0.5656\n",
            "Epoch 00017: val_accuracy improved from 0.50779 to 0.51244, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2873 - accuracy: 0.5656 - val_loss: 1.4088 - val_accuracy: 0.5124\n",
            "Epoch 18/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2861 - accuracy: 0.5654\n",
            "Epoch 00018: val_accuracy did not improve from 0.51244\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2861 - accuracy: 0.5654 - val_loss: 1.4025 - val_accuracy: 0.5082\n",
            "Epoch 19/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2836 - accuracy: 0.5669\n",
            "Epoch 00019: val_accuracy improved from 0.51244 to 0.51489, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2830 - accuracy: 0.5672 - val_loss: 1.3836 - val_accuracy: 0.5149\n",
            "Epoch 20/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2777 - accuracy: 0.5694\n",
            "Epoch 00020: val_accuracy improved from 0.51489 to 0.51742, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2777 - accuracy: 0.5694 - val_loss: 1.3990 - val_accuracy: 0.5174\n",
            "Epoch 21/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2761 - accuracy: 0.5686\n",
            "Epoch 00021: val_accuracy improved from 0.51742 to 0.52289, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2759 - accuracy: 0.5688 - val_loss: 1.3781 - val_accuracy: 0.5229\n",
            "Epoch 22/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2726 - accuracy: 0.5714\n",
            "Epoch 00022: val_accuracy did not improve from 0.52289\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2723 - accuracy: 0.5716 - val_loss: 1.3949 - val_accuracy: 0.5098\n",
            "Epoch 23/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2726 - accuracy: 0.5698\n",
            "Epoch 00023: val_accuracy did not improve from 0.52289\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2725 - accuracy: 0.5697 - val_loss: 1.3749 - val_accuracy: 0.5219\n",
            "Epoch 24/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2698 - accuracy: 0.5708\n",
            "Epoch 00024: val_accuracy did not improve from 0.52289\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2692 - accuracy: 0.5710 - val_loss: 1.4659 - val_accuracy: 0.4916\n",
            "Epoch 25/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2652 - accuracy: 0.5718\n",
            "Epoch 00025: val_accuracy did not improve from 0.52289\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2647 - accuracy: 0.5720 - val_loss: 1.4146 - val_accuracy: 0.5124\n",
            "Epoch 26/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2666 - accuracy: 0.5724\n",
            "Epoch 00026: val_accuracy did not improve from 0.52289\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2663 - accuracy: 0.5724 - val_loss: 1.3841 - val_accuracy: 0.5122\n",
            "Epoch 27/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2621 - accuracy: 0.5747\n",
            "Epoch 00027: val_accuracy did not improve from 0.52289\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2614 - accuracy: 0.5748 - val_loss: 1.3932 - val_accuracy: 0.5154\n",
            "Epoch 28/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2603 - accuracy: 0.5731\n",
            "Epoch 00028: val_accuracy did not improve from 0.52289\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2603 - accuracy: 0.5732 - val_loss: 1.3783 - val_accuracy: 0.5187\n",
            "Epoch 29/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2592 - accuracy: 0.5749\n",
            "Epoch 00029: val_accuracy improved from 0.52289 to 0.52514, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2597 - accuracy: 0.5748 - val_loss: 1.3587 - val_accuracy: 0.5251\n",
            "Epoch 30/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2601 - accuracy: 0.5754\n",
            "Epoch 00030: val_accuracy did not improve from 0.52514\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2601 - accuracy: 0.5755 - val_loss: 1.3799 - val_accuracy: 0.5226\n",
            "Epoch 31/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2549 - accuracy: 0.5769\n",
            "Epoch 00031: val_accuracy did not improve from 0.52514\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2547 - accuracy: 0.5770 - val_loss: 1.3753 - val_accuracy: 0.5221\n",
            "Epoch 32/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2552 - accuracy: 0.5757\n",
            "Epoch 00032: val_accuracy did not improve from 0.52514\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2552 - accuracy: 0.5757 - val_loss: 1.3850 - val_accuracy: 0.5159\n",
            "Epoch 33/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2523 - accuracy: 0.5762\n",
            "Epoch 00033: val_accuracy improved from 0.52514 to 0.52699, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2524 - accuracy: 0.5762 - val_loss: 1.3617 - val_accuracy: 0.5270\n",
            "Epoch 34/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2517 - accuracy: 0.5771\n",
            "Epoch 00034: val_accuracy improved from 0.52699 to 0.52781, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2518 - accuracy: 0.5771 - val_loss: 1.3561 - val_accuracy: 0.5278\n",
            "Epoch 35/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2532 - accuracy: 0.5771\n",
            "Epoch 00035: val_accuracy did not improve from 0.52781\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2531 - accuracy: 0.5772 - val_loss: 1.3830 - val_accuracy: 0.5122\n",
            "Epoch 36/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2486 - accuracy: 0.5776\n",
            "Epoch 00036: val_accuracy did not improve from 0.52781\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2485 - accuracy: 0.5776 - val_loss: 1.4243 - val_accuracy: 0.5038\n",
            "Epoch 37/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2508 - accuracy: 0.5758\n",
            "Epoch 00037: val_accuracy did not improve from 0.52781\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2504 - accuracy: 0.5759 - val_loss: 1.3864 - val_accuracy: 0.5174\n",
            "Epoch 38/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2490 - accuracy: 0.5781\n",
            "Epoch 00038: val_accuracy did not improve from 0.52781\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2489 - accuracy: 0.5779 - val_loss: 1.4039 - val_accuracy: 0.5070\n",
            "Epoch 39/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2448 - accuracy: 0.5785\n",
            "Epoch 00039: val_accuracy improved from 0.52781 to 0.53225, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2446 - accuracy: 0.5786 - val_loss: 1.3459 - val_accuracy: 0.5322\n",
            "Epoch 40/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2446 - accuracy: 0.5789\n",
            "Epoch 00040: val_accuracy did not improve from 0.53225\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2443 - accuracy: 0.5791 - val_loss: 1.3569 - val_accuracy: 0.5245\n",
            "Epoch 41/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2459 - accuracy: 0.5775\n",
            "Epoch 00041: val_accuracy did not improve from 0.53225\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2459 - accuracy: 0.5774 - val_loss: 1.3671 - val_accuracy: 0.5269\n",
            "Epoch 42/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2430 - accuracy: 0.5788\n",
            "Epoch 00042: val_accuracy did not improve from 0.53225\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2423 - accuracy: 0.5790 - val_loss: 1.3755 - val_accuracy: 0.5234\n",
            "Epoch 43/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2415 - accuracy: 0.5806\n",
            "Epoch 00043: val_accuracy did not improve from 0.53225\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2407 - accuracy: 0.5808 - val_loss: 1.3880 - val_accuracy: 0.5144\n",
            "Epoch 44/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2399 - accuracy: 0.5805\n",
            "Epoch 00044: val_accuracy improved from 0.53225 to 0.53573, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2404 - accuracy: 0.5803 - val_loss: 1.3472 - val_accuracy: 0.5357\n",
            "Epoch 45/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2384 - accuracy: 0.5814\n",
            "Epoch 00045: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2383 - accuracy: 0.5815 - val_loss: 1.3804 - val_accuracy: 0.5151\n",
            "Epoch 46/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2406 - accuracy: 0.5788\n",
            "Epoch 00046: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2406 - accuracy: 0.5788 - val_loss: 1.3612 - val_accuracy: 0.5267\n",
            "Epoch 47/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2400 - accuracy: 0.5800\n",
            "Epoch 00047: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2402 - accuracy: 0.5798 - val_loss: 1.4316 - val_accuracy: 0.5077\n",
            "Epoch 48/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2374 - accuracy: 0.5802\n",
            "Epoch 00048: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2376 - accuracy: 0.5800 - val_loss: 1.3571 - val_accuracy: 0.5284\n",
            "Epoch 49/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2349 - accuracy: 0.5825\n",
            "Epoch 00049: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2342 - accuracy: 0.5827 - val_loss: 1.3464 - val_accuracy: 0.5328\n",
            "Epoch 50/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2375 - accuracy: 0.5800\n",
            "Epoch 00050: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2375 - accuracy: 0.5800 - val_loss: 1.3698 - val_accuracy: 0.5197\n",
            "Epoch 51/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2364 - accuracy: 0.5814\n",
            "Epoch 00051: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2363 - accuracy: 0.5815 - val_loss: 1.4138 - val_accuracy: 0.5089\n",
            "Epoch 52/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2338 - accuracy: 0.5810\n",
            "Epoch 00052: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2339 - accuracy: 0.5809 - val_loss: 1.3536 - val_accuracy: 0.5273\n",
            "Epoch 53/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2343 - accuracy: 0.5814\n",
            "Epoch 00053: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2343 - accuracy: 0.5814 - val_loss: 1.3840 - val_accuracy: 0.5144\n",
            "Epoch 54/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2336 - accuracy: 0.5814\n",
            "Epoch 00054: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2338 - accuracy: 0.5811 - val_loss: 1.3602 - val_accuracy: 0.5270\n",
            "Epoch 55/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2341 - accuracy: 0.5804\n",
            "Epoch 00055: val_accuracy did not improve from 0.53573\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2338 - accuracy: 0.5806 - val_loss: 1.3592 - val_accuracy: 0.5317\n",
            "Epoch 56/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2324 - accuracy: 0.5828\n",
            "Epoch 00056: val_accuracy improved from 0.53573 to 0.53703, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2327 - accuracy: 0.5827 - val_loss: 1.3296 - val_accuracy: 0.5370\n",
            "Epoch 57/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2334 - accuracy: 0.5813\n",
            "Epoch 00057: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2336 - accuracy: 0.5813 - val_loss: 1.3910 - val_accuracy: 0.5174\n",
            "Epoch 58/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2317 - accuracy: 0.5821\n",
            "Epoch 00058: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2325 - accuracy: 0.5819 - val_loss: 1.3807 - val_accuracy: 0.5156\n",
            "Epoch 59/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2325 - accuracy: 0.5811\n",
            "Epoch 00059: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2318 - accuracy: 0.5815 - val_loss: 1.3910 - val_accuracy: 0.5117\n",
            "Epoch 60/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2309 - accuracy: 0.5839\n",
            "Epoch 00060: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2305 - accuracy: 0.5840 - val_loss: 1.3605 - val_accuracy: 0.5222\n",
            "Epoch 61/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2334 - accuracy: 0.5804\n",
            "Epoch 00061: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2336 - accuracy: 0.5805 - val_loss: 1.3963 - val_accuracy: 0.5175\n",
            "Epoch 62/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2369 - accuracy: 0.5796\n",
            "Epoch 00062: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2368 - accuracy: 0.5795 - val_loss: 1.3961 - val_accuracy: 0.5074\n",
            "Epoch 63/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2304 - accuracy: 0.5826\n",
            "Epoch 00063: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2305 - accuracy: 0.5826 - val_loss: 1.4247 - val_accuracy: 0.4999\n",
            "Epoch 64/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2301 - accuracy: 0.5820\n",
            "Epoch 00064: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2300 - accuracy: 0.5820 - val_loss: 1.4013 - val_accuracy: 0.5080\n",
            "Epoch 65/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2276 - accuracy: 0.5826\n",
            "Epoch 00065: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2276 - accuracy: 0.5823 - val_loss: 1.4458 - val_accuracy: 0.5028\n",
            "Epoch 66/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2282 - accuracy: 0.5817\n",
            "Epoch 00066: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2280 - accuracy: 0.5818 - val_loss: 1.3652 - val_accuracy: 0.5171\n",
            "Epoch 67/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2284 - accuracy: 0.5809\n",
            "Epoch 00067: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2281 - accuracy: 0.5811 - val_loss: 1.3422 - val_accuracy: 0.5331\n",
            "Epoch 68/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2280 - accuracy: 0.5823\n",
            "Epoch 00068: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2280 - accuracy: 0.5823 - val_loss: 1.3486 - val_accuracy: 0.5282\n",
            "Epoch 69/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2261 - accuracy: 0.5831\n",
            "Epoch 00069: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2267 - accuracy: 0.5828 - val_loss: 1.3531 - val_accuracy: 0.5230\n",
            "Epoch 70/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2271 - accuracy: 0.5842\n",
            "Epoch 00070: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2269 - accuracy: 0.5841 - val_loss: 1.4024 - val_accuracy: 0.5123\n",
            "Epoch 71/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2273 - accuracy: 0.5836\n",
            "Epoch 00071: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2270 - accuracy: 0.5836 - val_loss: 1.3426 - val_accuracy: 0.5324\n",
            "Epoch 72/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2252 - accuracy: 0.5816\n",
            "Epoch 00072: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2248 - accuracy: 0.5816 - val_loss: 1.3696 - val_accuracy: 0.5195\n",
            "Epoch 73/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2283 - accuracy: 0.5824\n",
            "Epoch 00073: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2276 - accuracy: 0.5828 - val_loss: 1.3839 - val_accuracy: 0.5124\n",
            "Epoch 74/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2231 - accuracy: 0.5844\n",
            "Epoch 00074: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2229 - accuracy: 0.5845 - val_loss: 1.4055 - val_accuracy: 0.5023\n",
            "Epoch 75/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2267 - accuracy: 0.5820\n",
            "Epoch 00075: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2264 - accuracy: 0.5820 - val_loss: 1.3810 - val_accuracy: 0.5173\n",
            "Epoch 76/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2242 - accuracy: 0.5831\n",
            "Epoch 00076: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2242 - accuracy: 0.5831 - val_loss: 1.3489 - val_accuracy: 0.5279\n",
            "Epoch 77/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2229 - accuracy: 0.5826\n",
            "Epoch 00077: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2225 - accuracy: 0.5830 - val_loss: 1.3868 - val_accuracy: 0.5152\n",
            "Epoch 78/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2220 - accuracy: 0.5846\n",
            "Epoch 00078: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2222 - accuracy: 0.5844 - val_loss: 1.3767 - val_accuracy: 0.5192\n",
            "Epoch 79/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2198 - accuracy: 0.5859\n",
            "Epoch 00079: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2206 - accuracy: 0.5854 - val_loss: 1.3434 - val_accuracy: 0.5350\n",
            "Epoch 80/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2216 - accuracy: 0.5846\n",
            "Epoch 00080: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2218 - accuracy: 0.5844 - val_loss: 1.3395 - val_accuracy: 0.5330\n",
            "Epoch 81/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2230 - accuracy: 0.5832\n",
            "Epoch 00081: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2229 - accuracy: 0.5832 - val_loss: 1.3650 - val_accuracy: 0.5294\n",
            "Epoch 82/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2196 - accuracy: 0.5860\n",
            "Epoch 00082: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2198 - accuracy: 0.5859 - val_loss: 1.3572 - val_accuracy: 0.5267\n",
            "Epoch 83/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2209 - accuracy: 0.5837\n",
            "Epoch 00083: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2211 - accuracy: 0.5836 - val_loss: 1.4088 - val_accuracy: 0.5117\n",
            "Epoch 84/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2249 - accuracy: 0.5822\n",
            "Epoch 00084: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2248 - accuracy: 0.5823 - val_loss: 1.4198 - val_accuracy: 0.5084\n",
            "Epoch 85/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2216 - accuracy: 0.5843\n",
            "Epoch 00085: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2224 - accuracy: 0.5840 - val_loss: 1.4247 - val_accuracy: 0.5039\n",
            "Epoch 86/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2219 - accuracy: 0.5835\n",
            "Epoch 00086: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2222 - accuracy: 0.5834 - val_loss: 1.4300 - val_accuracy: 0.5062\n",
            "Epoch 87/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2188 - accuracy: 0.5850\n",
            "Epoch 00087: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2189 - accuracy: 0.5848 - val_loss: 1.3563 - val_accuracy: 0.5248\n",
            "Epoch 88/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2191 - accuracy: 0.5847\n",
            "Epoch 00088: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2187 - accuracy: 0.5848 - val_loss: 1.3453 - val_accuracy: 0.5310\n",
            "Epoch 89/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2208 - accuracy: 0.5834\n",
            "Epoch 00089: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2208 - accuracy: 0.5834 - val_loss: 1.3394 - val_accuracy: 0.5350\n",
            "Epoch 90/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2175 - accuracy: 0.5850\n",
            "Epoch 00090: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2179 - accuracy: 0.5848 - val_loss: 1.3410 - val_accuracy: 0.5311\n",
            "Epoch 91/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2185 - accuracy: 0.5836\n",
            "Epoch 00091: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2180 - accuracy: 0.5836 - val_loss: 1.3625 - val_accuracy: 0.5182\n",
            "Epoch 92/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2193 - accuracy: 0.5835\n",
            "Epoch 00092: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2194 - accuracy: 0.5837 - val_loss: 1.3655 - val_accuracy: 0.5218\n",
            "Epoch 93/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2190 - accuracy: 0.5841\n",
            "Epoch 00093: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2193 - accuracy: 0.5841 - val_loss: 1.3598 - val_accuracy: 0.5187\n",
            "Epoch 94/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2185 - accuracy: 0.5845\n",
            "Epoch 00094: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2179 - accuracy: 0.5848 - val_loss: 1.3966 - val_accuracy: 0.5102\n",
            "Epoch 95/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2163 - accuracy: 0.5848\n",
            "Epoch 00095: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2163 - accuracy: 0.5848 - val_loss: 1.3276 - val_accuracy: 0.5351\n",
            "Epoch 96/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2157 - accuracy: 0.5852\n",
            "Epoch 00096: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2156 - accuracy: 0.5850 - val_loss: 1.3378 - val_accuracy: 0.5339\n",
            "Epoch 97/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2177 - accuracy: 0.5849\n",
            "Epoch 00097: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2174 - accuracy: 0.5850 - val_loss: 1.3485 - val_accuracy: 0.5302\n",
            "Epoch 98/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2145 - accuracy: 0.5848\n",
            "Epoch 00098: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2147 - accuracy: 0.5848 - val_loss: 1.3514 - val_accuracy: 0.5317\n",
            "Epoch 99/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2170 - accuracy: 0.5855\n",
            "Epoch 00099: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2170 - accuracy: 0.5856 - val_loss: 1.3761 - val_accuracy: 0.5176\n",
            "Epoch 100/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2151 - accuracy: 0.5860\n",
            "Epoch 00100: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2151 - accuracy: 0.5860 - val_loss: 1.4215 - val_accuracy: 0.5085\n",
            "Epoch 101/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2115 - accuracy: 0.5871\n",
            "Epoch 00101: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2110 - accuracy: 0.5872 - val_loss: 1.3597 - val_accuracy: 0.5236\n",
            "Epoch 102/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2140 - accuracy: 0.5848\n",
            "Epoch 00102: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2143 - accuracy: 0.5846 - val_loss: 1.3369 - val_accuracy: 0.5314\n",
            "Epoch 103/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2170 - accuracy: 0.5851\n",
            "Epoch 00103: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2176 - accuracy: 0.5848 - val_loss: 1.4084 - val_accuracy: 0.5110\n",
            "Epoch 104/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2124 - accuracy: 0.5871\n",
            "Epoch 00104: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2120 - accuracy: 0.5873 - val_loss: 1.3322 - val_accuracy: 0.5363\n",
            "Epoch 105/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2104 - accuracy: 0.5867\n",
            "Epoch 00105: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2104 - accuracy: 0.5867 - val_loss: 1.3326 - val_accuracy: 0.5355\n",
            "Epoch 106/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2172 - accuracy: 0.5859\n",
            "Epoch 00106: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2168 - accuracy: 0.5861 - val_loss: 1.3301 - val_accuracy: 0.5354\n",
            "Epoch 107/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2122 - accuracy: 0.5878\n",
            "Epoch 00107: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2123 - accuracy: 0.5877 - val_loss: 1.3564 - val_accuracy: 0.5288\n",
            "Epoch 108/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2137 - accuracy: 0.5861\n",
            "Epoch 00108: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2135 - accuracy: 0.5863 - val_loss: 1.3411 - val_accuracy: 0.5277\n",
            "Epoch 109/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2145 - accuracy: 0.5861\n",
            "Epoch 00109: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2145 - accuracy: 0.5861 - val_loss: 1.3324 - val_accuracy: 0.5347\n",
            "Epoch 110/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2213 - accuracy: 0.5833\n",
            "Epoch 00110: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2211 - accuracy: 0.5834 - val_loss: 1.3558 - val_accuracy: 0.5183\n",
            "Epoch 111/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2085 - accuracy: 0.5879\n",
            "Epoch 00111: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2093 - accuracy: 0.5876 - val_loss: 1.4012 - val_accuracy: 0.5131\n",
            "Epoch 112/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2085 - accuracy: 0.5885\n",
            "Epoch 00112: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2092 - accuracy: 0.5881 - val_loss: 1.4066 - val_accuracy: 0.5141\n",
            "Epoch 113/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2168 - accuracy: 0.5859\n",
            "Epoch 00113: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2167 - accuracy: 0.5859 - val_loss: 1.3568 - val_accuracy: 0.5244\n",
            "Epoch 114/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2142 - accuracy: 0.5869\n",
            "Epoch 00114: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2141 - accuracy: 0.5869 - val_loss: 1.3428 - val_accuracy: 0.5257\n",
            "Epoch 115/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2129 - accuracy: 0.5853\n",
            "Epoch 00115: val_accuracy did not improve from 0.53703\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2128 - accuracy: 0.5854 - val_loss: 1.3246 - val_accuracy: 0.5338\n",
            "Epoch 116/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2133 - accuracy: 0.5852\n",
            "Epoch 00116: val_accuracy improved from 0.53703 to 0.53778, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2136 - accuracy: 0.5852 - val_loss: 1.3201 - val_accuracy: 0.5378\n",
            "Epoch 117/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2081 - accuracy: 0.5872\n",
            "Epoch 00117: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2085 - accuracy: 0.5871 - val_loss: 1.3503 - val_accuracy: 0.5299\n",
            "Epoch 118/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2159 - accuracy: 0.5840\n",
            "Epoch 00118: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2159 - accuracy: 0.5840 - val_loss: 1.3859 - val_accuracy: 0.5217\n",
            "Epoch 119/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2120 - accuracy: 0.5866\n",
            "Epoch 00119: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2117 - accuracy: 0.5868 - val_loss: 1.3362 - val_accuracy: 0.5308\n",
            "Epoch 120/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2122 - accuracy: 0.5853\n",
            "Epoch 00120: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2125 - accuracy: 0.5852 - val_loss: 1.3332 - val_accuracy: 0.5338\n",
            "Epoch 121/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2102 - accuracy: 0.5857\n",
            "Epoch 00121: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2100 - accuracy: 0.5858 - val_loss: 1.3422 - val_accuracy: 0.5251\n",
            "Epoch 122/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2122 - accuracy: 0.5867\n",
            "Epoch 00122: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2117 - accuracy: 0.5869 - val_loss: 1.4462 - val_accuracy: 0.5023\n",
            "Epoch 123/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2120 - accuracy: 0.5861\n",
            "Epoch 00123: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2117 - accuracy: 0.5861 - val_loss: 1.3841 - val_accuracy: 0.5193\n",
            "Epoch 124/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2106 - accuracy: 0.5866\n",
            "Epoch 00124: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2108 - accuracy: 0.5865 - val_loss: 1.4313 - val_accuracy: 0.4969\n",
            "Epoch 125/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2096 - accuracy: 0.5866\n",
            "Epoch 00125: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2095 - accuracy: 0.5866 - val_loss: 1.4190 - val_accuracy: 0.5077\n",
            "Epoch 126/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2113 - accuracy: 0.5867\n",
            "Epoch 00126: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2114 - accuracy: 0.5866 - val_loss: 1.3513 - val_accuracy: 0.5303\n",
            "Epoch 127/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2116 - accuracy: 0.5862\n",
            "Epoch 00127: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2116 - accuracy: 0.5862 - val_loss: 1.3350 - val_accuracy: 0.5307\n",
            "Epoch 128/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2063 - accuracy: 0.5877\n",
            "Epoch 00128: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2069 - accuracy: 0.5876 - val_loss: 1.3226 - val_accuracy: 0.5365\n",
            "Epoch 129/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2082 - accuracy: 0.5878\n",
            "Epoch 00129: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2089 - accuracy: 0.5874 - val_loss: 1.3561 - val_accuracy: 0.5237\n",
            "Epoch 130/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2079 - accuracy: 0.5868\n",
            "Epoch 00130: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2079 - accuracy: 0.5866 - val_loss: 1.3996 - val_accuracy: 0.5140\n",
            "Epoch 131/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2080 - accuracy: 0.5863\n",
            "Epoch 00131: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2080 - accuracy: 0.5863 - val_loss: 1.3256 - val_accuracy: 0.5319\n",
            "Epoch 132/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2073 - accuracy: 0.5869\n",
            "Epoch 00132: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2070 - accuracy: 0.5869 - val_loss: 1.3784 - val_accuracy: 0.5150\n",
            "Epoch 133/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2070 - accuracy: 0.5864\n",
            "Epoch 00133: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2073 - accuracy: 0.5864 - val_loss: 1.3551 - val_accuracy: 0.5197\n",
            "Epoch 134/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2085 - accuracy: 0.5867\n",
            "Epoch 00134: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2085 - accuracy: 0.5867 - val_loss: 1.3310 - val_accuracy: 0.5325\n",
            "Epoch 135/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2082 - accuracy: 0.5873\n",
            "Epoch 00135: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2076 - accuracy: 0.5875 - val_loss: 1.3576 - val_accuracy: 0.5217\n",
            "Epoch 136/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2079 - accuracy: 0.5866\n",
            "Epoch 00136: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2067 - accuracy: 0.5872 - val_loss: 1.3221 - val_accuracy: 0.5344\n",
            "Epoch 137/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2112 - accuracy: 0.5854\n",
            "Epoch 00137: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2114 - accuracy: 0.5853 - val_loss: 1.4037 - val_accuracy: 0.5083\n",
            "Epoch 138/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2063 - accuracy: 0.5879\n",
            "Epoch 00138: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2056 - accuracy: 0.5882 - val_loss: 1.3653 - val_accuracy: 0.5174\n",
            "Epoch 139/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2034 - accuracy: 0.5887\n",
            "Epoch 00139: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2036 - accuracy: 0.5885 - val_loss: 1.3611 - val_accuracy: 0.5157\n",
            "Epoch 140/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2038 - accuracy: 0.5878\n",
            "Epoch 00140: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2038 - accuracy: 0.5878 - val_loss: 1.3926 - val_accuracy: 0.5124\n",
            "Epoch 141/500\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2083 - accuracy: 0.5869\n",
            "Epoch 00141: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2076 - accuracy: 0.5871 - val_loss: 1.3692 - val_accuracy: 0.5139\n",
            "Epoch 142/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2039 - accuracy: 0.5878\n",
            "Epoch 00142: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2038 - accuracy: 0.5878 - val_loss: 1.3503 - val_accuracy: 0.5261\n",
            "Epoch 143/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2108 - accuracy: 0.5847\n",
            "Epoch 00143: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2110 - accuracy: 0.5846 - val_loss: 1.3934 - val_accuracy: 0.5077\n",
            "Epoch 144/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2056 - accuracy: 0.5876\n",
            "Epoch 00144: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2058 - accuracy: 0.5875 - val_loss: 1.3226 - val_accuracy: 0.5348\n",
            "Epoch 145/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2053 - accuracy: 0.5877\n",
            "Epoch 00145: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2053 - accuracy: 0.5877 - val_loss: 1.3621 - val_accuracy: 0.5247\n",
            "Epoch 146/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2083 - accuracy: 0.5876\n",
            "Epoch 00146: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 5s 14ms/step - loss: 1.2075 - accuracy: 0.5877 - val_loss: 1.3649 - val_accuracy: 0.5221\n",
            "Epoch 147/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2043 - accuracy: 0.5880\n",
            "Epoch 00147: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2044 - accuracy: 0.5880 - val_loss: 1.3398 - val_accuracy: 0.5291\n",
            "Epoch 148/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2042 - accuracy: 0.5871\n",
            "Epoch 00148: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2040 - accuracy: 0.5872 - val_loss: 1.4164 - val_accuracy: 0.5014\n",
            "Epoch 149/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2056 - accuracy: 0.5868\n",
            "Epoch 00149: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2056 - accuracy: 0.5868 - val_loss: 1.3461 - val_accuracy: 0.5299\n",
            "Epoch 150/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2033 - accuracy: 0.5883\n",
            "Epoch 00150: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2033 - accuracy: 0.5883 - val_loss: 1.3652 - val_accuracy: 0.5152\n",
            "Epoch 151/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2052 - accuracy: 0.5872\n",
            "Epoch 00151: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2051 - accuracy: 0.5873 - val_loss: 1.3773 - val_accuracy: 0.5167\n",
            "Epoch 152/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2019 - accuracy: 0.5876\n",
            "Epoch 00152: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2015 - accuracy: 0.5877 - val_loss: 1.3376 - val_accuracy: 0.5311\n",
            "Epoch 153/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2014 - accuracy: 0.5879\n",
            "Epoch 00153: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2012 - accuracy: 0.5880 - val_loss: 1.3435 - val_accuracy: 0.5291\n",
            "Epoch 154/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2065 - accuracy: 0.5863\n",
            "Epoch 00154: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2069 - accuracy: 0.5861 - val_loss: 1.3350 - val_accuracy: 0.5288\n",
            "Epoch 155/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2035 - accuracy: 0.5880\n",
            "Epoch 00155: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2036 - accuracy: 0.5879 - val_loss: 1.3542 - val_accuracy: 0.5240\n",
            "Epoch 156/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2025 - accuracy: 0.5870\n",
            "Epoch 00156: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2025 - accuracy: 0.5870 - val_loss: 1.3780 - val_accuracy: 0.5159\n",
            "Epoch 157/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2036 - accuracy: 0.5876\n",
            "Epoch 00157: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2035 - accuracy: 0.5876 - val_loss: 1.3411 - val_accuracy: 0.5324\n",
            "Epoch 158/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2023 - accuracy: 0.5876\n",
            "Epoch 00158: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2023 - accuracy: 0.5875 - val_loss: 1.3354 - val_accuracy: 0.5338\n",
            "Epoch 159/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2021 - accuracy: 0.5883\n",
            "Epoch 00159: val_accuracy did not improve from 0.53778\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2021 - accuracy: 0.5883 - val_loss: 1.4019 - val_accuracy: 0.5112\n",
            "Epoch 160/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2013 - accuracy: 0.5886\n",
            "Epoch 00160: val_accuracy improved from 0.53778 to 0.53833, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.2012 - accuracy: 0.5886 - val_loss: 1.3203 - val_accuracy: 0.5383\n",
            "Epoch 161/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2057 - accuracy: 0.5860\n",
            "Epoch 00161: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2057 - accuracy: 0.5860 - val_loss: 1.3332 - val_accuracy: 0.5339\n",
            "Epoch 162/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2029 - accuracy: 0.5874\n",
            "Epoch 00162: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2029 - accuracy: 0.5874 - val_loss: 1.3667 - val_accuracy: 0.5155\n",
            "Epoch 163/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2037 - accuracy: 0.5878\n",
            "Epoch 00163: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2036 - accuracy: 0.5877 - val_loss: 1.3541 - val_accuracy: 0.5201\n",
            "Epoch 164/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2038 - accuracy: 0.5882\n",
            "Epoch 00164: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2039 - accuracy: 0.5883 - val_loss: 1.3454 - val_accuracy: 0.5318\n",
            "Epoch 165/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2019 - accuracy: 0.5869\n",
            "Epoch 00165: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2019 - accuracy: 0.5869 - val_loss: 1.3506 - val_accuracy: 0.5195\n",
            "Epoch 166/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2054 - accuracy: 0.5882\n",
            "Epoch 00166: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2053 - accuracy: 0.5882 - val_loss: 1.3700 - val_accuracy: 0.5178\n",
            "Epoch 167/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2060 - accuracy: 0.5867\n",
            "Epoch 00167: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2061 - accuracy: 0.5867 - val_loss: 1.3702 - val_accuracy: 0.5151\n",
            "Epoch 168/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2063 - accuracy: 0.5866\n",
            "Epoch 00168: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2063 - accuracy: 0.5865 - val_loss: 1.3353 - val_accuracy: 0.5335\n",
            "Epoch 169/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2054 - accuracy: 0.5865\n",
            "Epoch 00169: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2060 - accuracy: 0.5861 - val_loss: 1.3531 - val_accuracy: 0.5222\n",
            "Epoch 170/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2025 - accuracy: 0.5878\n",
            "Epoch 00170: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2035 - accuracy: 0.5874 - val_loss: 1.4414 - val_accuracy: 0.4964\n",
            "Epoch 171/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2041 - accuracy: 0.5866\n",
            "Epoch 00171: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2041 - accuracy: 0.5866 - val_loss: 1.3464 - val_accuracy: 0.5302\n",
            "Epoch 172/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2073 - accuracy: 0.5869\n",
            "Epoch 00172: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2072 - accuracy: 0.5869 - val_loss: 1.4024 - val_accuracy: 0.5109\n",
            "Epoch 173/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2037 - accuracy: 0.5870\n",
            "Epoch 00173: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2041 - accuracy: 0.5868 - val_loss: 1.3855 - val_accuracy: 0.5136\n",
            "Epoch 174/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2040 - accuracy: 0.5869\n",
            "Epoch 00174: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2040 - accuracy: 0.5869 - val_loss: 1.3330 - val_accuracy: 0.5305\n",
            "Epoch 175/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2050 - accuracy: 0.5870\n",
            "Epoch 00175: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2051 - accuracy: 0.5870 - val_loss: 1.3704 - val_accuracy: 0.5201\n",
            "Epoch 176/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2027 - accuracy: 0.5877\n",
            "Epoch 00176: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.2032 - accuracy: 0.5875 - val_loss: 1.3427 - val_accuracy: 0.5300\n",
            "Epoch 177/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2060 - accuracy: 0.5859\n",
            "Epoch 00177: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2059 - accuracy: 0.5860 - val_loss: 1.3698 - val_accuracy: 0.5161\n",
            "Epoch 178/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2019 - accuracy: 0.5869\n",
            "Epoch 00178: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.2019 - accuracy: 0.5869 - val_loss: 1.3346 - val_accuracy: 0.5275\n",
            "Epoch 179/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2069 - accuracy: 0.5864\n",
            "Epoch 00179: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2069 - accuracy: 0.5864 - val_loss: 1.3709 - val_accuracy: 0.5208\n",
            "Epoch 180/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2063 - accuracy: 0.5874\n",
            "Epoch 00180: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.2064 - accuracy: 0.5873 - val_loss: 1.4122 - val_accuracy: 0.5087\n",
            "Epoch 181/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2094 - accuracy: 0.5852\n",
            "Epoch 00181: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2094 - accuracy: 0.5852 - val_loss: 1.3615 - val_accuracy: 0.5210\n",
            "Epoch 182/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2029 - accuracy: 0.5885\n",
            "Epoch 00182: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2032 - accuracy: 0.5882 - val_loss: 1.3358 - val_accuracy: 0.5336\n",
            "Epoch 183/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2076 - accuracy: 0.5852\n",
            "Epoch 00183: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2075 - accuracy: 0.5853 - val_loss: 1.3736 - val_accuracy: 0.5139\n",
            "Epoch 184/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2049 - accuracy: 0.5876\n",
            "Epoch 00184: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2050 - accuracy: 0.5876 - val_loss: 1.3617 - val_accuracy: 0.5154\n",
            "Epoch 185/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2031 - accuracy: 0.5874\n",
            "Epoch 00185: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2032 - accuracy: 0.5874 - val_loss: 1.3592 - val_accuracy: 0.5221\n",
            "Epoch 186/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2050 - accuracy: 0.5870\n",
            "Epoch 00186: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2050 - accuracy: 0.5870 - val_loss: 1.3461 - val_accuracy: 0.5332\n",
            "Epoch 187/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2076 - accuracy: 0.5865\n",
            "Epoch 00187: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2074 - accuracy: 0.5866 - val_loss: 1.4542 - val_accuracy: 0.4825\n",
            "Epoch 188/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2056 - accuracy: 0.5863\n",
            "Epoch 00188: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2061 - accuracy: 0.5861 - val_loss: 1.3931 - val_accuracy: 0.5127\n",
            "Epoch 189/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2033 - accuracy: 0.5875\n",
            "Epoch 00189: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2032 - accuracy: 0.5876 - val_loss: 1.3277 - val_accuracy: 0.5331\n",
            "Epoch 190/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2050 - accuracy: 0.5851\n",
            "Epoch 00190: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2052 - accuracy: 0.5851 - val_loss: 1.3481 - val_accuracy: 0.5254\n",
            "Epoch 191/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2039 - accuracy: 0.5879\n",
            "Epoch 00191: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2040 - accuracy: 0.5879 - val_loss: 1.3324 - val_accuracy: 0.5292\n",
            "Epoch 192/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2051 - accuracy: 0.5868\n",
            "Epoch 00192: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2052 - accuracy: 0.5868 - val_loss: 1.3591 - val_accuracy: 0.5190\n",
            "Epoch 193/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2079 - accuracy: 0.5852\n",
            "Epoch 00193: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2079 - accuracy: 0.5852 - val_loss: 1.3903 - val_accuracy: 0.5094\n",
            "Epoch 194/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2042 - accuracy: 0.5875\n",
            "Epoch 00194: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2043 - accuracy: 0.5874 - val_loss: 1.3313 - val_accuracy: 0.5340\n",
            "Epoch 195/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2027 - accuracy: 0.5881\n",
            "Epoch 00195: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2025 - accuracy: 0.5883 - val_loss: 1.3479 - val_accuracy: 0.5252\n",
            "Epoch 196/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2016 - accuracy: 0.5887\n",
            "Epoch 00196: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2011 - accuracy: 0.5888 - val_loss: 1.3416 - val_accuracy: 0.5264\n",
            "Epoch 197/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2012 - accuracy: 0.5884\n",
            "Epoch 00197: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2012 - accuracy: 0.5884 - val_loss: 1.3541 - val_accuracy: 0.5225\n",
            "Epoch 198/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2050 - accuracy: 0.5873\n",
            "Epoch 00198: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2050 - accuracy: 0.5873 - val_loss: 1.3598 - val_accuracy: 0.5221\n",
            "Epoch 199/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2025 - accuracy: 0.5879\n",
            "Epoch 00199: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2023 - accuracy: 0.5880 - val_loss: 1.3366 - val_accuracy: 0.5253\n",
            "Epoch 200/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2042 - accuracy: 0.5862\n",
            "Epoch 00200: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2045 - accuracy: 0.5861 - val_loss: 1.3421 - val_accuracy: 0.5333\n",
            "Epoch 201/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2041 - accuracy: 0.5882\n",
            "Epoch 00201: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2039 - accuracy: 0.5882 - val_loss: 1.3369 - val_accuracy: 0.5309\n",
            "Epoch 202/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1995 - accuracy: 0.5892\n",
            "Epoch 00202: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1988 - accuracy: 0.5893 - val_loss: 1.3841 - val_accuracy: 0.5135\n",
            "Epoch 203/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2021 - accuracy: 0.5889\n",
            "Epoch 00203: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2025 - accuracy: 0.5885 - val_loss: 1.3621 - val_accuracy: 0.5199\n",
            "Epoch 204/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2029 - accuracy: 0.5873\n",
            "Epoch 00204: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2026 - accuracy: 0.5875 - val_loss: 1.3917 - val_accuracy: 0.5083\n",
            "Epoch 205/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2003 - accuracy: 0.5880\n",
            "Epoch 00205: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2003 - accuracy: 0.5882 - val_loss: 1.3667 - val_accuracy: 0.5205\n",
            "Epoch 206/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2009 - accuracy: 0.5882\n",
            "Epoch 00206: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2007 - accuracy: 0.5881 - val_loss: 1.3826 - val_accuracy: 0.5180\n",
            "Epoch 207/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2022 - accuracy: 0.5891\n",
            "Epoch 00207: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2023 - accuracy: 0.5890 - val_loss: 1.3840 - val_accuracy: 0.5105\n",
            "Epoch 208/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2005 - accuracy: 0.5886\n",
            "Epoch 00208: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2014 - accuracy: 0.5883 - val_loss: 1.3536 - val_accuracy: 0.5213\n",
            "Epoch 209/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2007 - accuracy: 0.5879\n",
            "Epoch 00209: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2005 - accuracy: 0.5879 - val_loss: 1.3383 - val_accuracy: 0.5305\n",
            "Epoch 210/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2019 - accuracy: 0.5875\n",
            "Epoch 00210: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2018 - accuracy: 0.5875 - val_loss: 1.4129 - val_accuracy: 0.5051\n",
            "Epoch 211/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2042 - accuracy: 0.5858\n",
            "Epoch 00211: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2046 - accuracy: 0.5858 - val_loss: 1.3587 - val_accuracy: 0.5232\n",
            "Epoch 212/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2002 - accuracy: 0.5878\n",
            "Epoch 00212: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2003 - accuracy: 0.5878 - val_loss: 1.3559 - val_accuracy: 0.5238\n",
            "Epoch 213/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2039 - accuracy: 0.5880\n",
            "Epoch 00213: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2044 - accuracy: 0.5876 - val_loss: 1.3283 - val_accuracy: 0.5317\n",
            "Epoch 214/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2009 - accuracy: 0.5885\n",
            "Epoch 00214: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.2008 - accuracy: 0.5885 - val_loss: 1.3754 - val_accuracy: 0.5167\n",
            "Epoch 215/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1987 - accuracy: 0.5892\n",
            "Epoch 00215: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1987 - accuracy: 0.5891 - val_loss: 1.3907 - val_accuracy: 0.5120\n",
            "Epoch 216/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1998 - accuracy: 0.5892\n",
            "Epoch 00216: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1998 - accuracy: 0.5892 - val_loss: 1.3671 - val_accuracy: 0.5180\n",
            "Epoch 217/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1978 - accuracy: 0.5900\n",
            "Epoch 00217: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1975 - accuracy: 0.5901 - val_loss: 1.3234 - val_accuracy: 0.5293\n",
            "Epoch 218/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1980 - accuracy: 0.5908\n",
            "Epoch 00218: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1989 - accuracy: 0.5904 - val_loss: 1.3611 - val_accuracy: 0.5212\n",
            "Epoch 219/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2026 - accuracy: 0.5878\n",
            "Epoch 00219: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2028 - accuracy: 0.5877 - val_loss: 1.3429 - val_accuracy: 0.5289\n",
            "Epoch 220/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1996 - accuracy: 0.5892\n",
            "Epoch 00220: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1993 - accuracy: 0.5892 - val_loss: 1.3578 - val_accuracy: 0.5240\n",
            "Epoch 221/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2021 - accuracy: 0.5878\n",
            "Epoch 00221: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2021 - accuracy: 0.5877 - val_loss: 1.3792 - val_accuracy: 0.5114\n",
            "Epoch 222/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2027 - accuracy: 0.5874\n",
            "Epoch 00222: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2028 - accuracy: 0.5875 - val_loss: 1.3283 - val_accuracy: 0.5344\n",
            "Epoch 223/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2002 - accuracy: 0.5887\n",
            "Epoch 00223: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2001 - accuracy: 0.5889 - val_loss: 1.3573 - val_accuracy: 0.5214\n",
            "Epoch 224/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2008 - accuracy: 0.5880\n",
            "Epoch 00224: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2012 - accuracy: 0.5877 - val_loss: 1.3497 - val_accuracy: 0.5269\n",
            "Epoch 225/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1998 - accuracy: 0.5890\n",
            "Epoch 00225: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1998 - accuracy: 0.5889 - val_loss: 1.3998 - val_accuracy: 0.5090\n",
            "Epoch 226/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2008 - accuracy: 0.5885\n",
            "Epoch 00226: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2011 - accuracy: 0.5883 - val_loss: 1.3640 - val_accuracy: 0.5250\n",
            "Epoch 227/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1996 - accuracy: 0.5873\n",
            "Epoch 00227: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1993 - accuracy: 0.5872 - val_loss: 1.3590 - val_accuracy: 0.5200\n",
            "Epoch 228/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2003 - accuracy: 0.5881\n",
            "Epoch 00228: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.2003 - accuracy: 0.5882 - val_loss: 1.3237 - val_accuracy: 0.5353\n",
            "Epoch 229/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1996 - accuracy: 0.5890\n",
            "Epoch 00229: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2001 - accuracy: 0.5889 - val_loss: 1.3750 - val_accuracy: 0.5167\n",
            "Epoch 230/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2036 - accuracy: 0.5876\n",
            "Epoch 00230: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2036 - accuracy: 0.5877 - val_loss: 1.3732 - val_accuracy: 0.5105\n",
            "Epoch 231/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1964 - accuracy: 0.5886\n",
            "Epoch 00231: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1964 - accuracy: 0.5885 - val_loss: 1.3570 - val_accuracy: 0.5233\n",
            "Epoch 232/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1982 - accuracy: 0.5891\n",
            "Epoch 00232: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1976 - accuracy: 0.5893 - val_loss: 1.3245 - val_accuracy: 0.5383\n",
            "Epoch 233/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1993 - accuracy: 0.5897\n",
            "Epoch 00233: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1993 - accuracy: 0.5897 - val_loss: 1.3371 - val_accuracy: 0.5287\n",
            "Epoch 234/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1974 - accuracy: 0.5875\n",
            "Epoch 00234: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1974 - accuracy: 0.5876 - val_loss: 1.3179 - val_accuracy: 0.5381\n",
            "Epoch 235/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1994 - accuracy: 0.5889\n",
            "Epoch 00235: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1994 - accuracy: 0.5892 - val_loss: 1.3504 - val_accuracy: 0.5264\n",
            "Epoch 236/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1985 - accuracy: 0.5883\n",
            "Epoch 00236: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1981 - accuracy: 0.5885 - val_loss: 1.3500 - val_accuracy: 0.5260\n",
            "Epoch 237/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1982 - accuracy: 0.5887\n",
            "Epoch 00237: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1982 - accuracy: 0.5887 - val_loss: 1.3526 - val_accuracy: 0.5247\n",
            "Epoch 238/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1976 - accuracy: 0.5880\n",
            "Epoch 00238: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1974 - accuracy: 0.5880 - val_loss: 1.3429 - val_accuracy: 0.5281\n",
            "Epoch 239/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1996 - accuracy: 0.5883\n",
            "Epoch 00239: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1990 - accuracy: 0.5885 - val_loss: 1.3708 - val_accuracy: 0.5182\n",
            "Epoch 240/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1954 - accuracy: 0.5900\n",
            "Epoch 00240: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1954 - accuracy: 0.5900 - val_loss: 1.3794 - val_accuracy: 0.5164\n",
            "Epoch 241/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1977 - accuracy: 0.5884\n",
            "Epoch 00241: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1980 - accuracy: 0.5883 - val_loss: 1.3171 - val_accuracy: 0.5346\n",
            "Epoch 242/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1970 - accuracy: 0.5888\n",
            "Epoch 00242: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1967 - accuracy: 0.5890 - val_loss: 1.3237 - val_accuracy: 0.5342\n",
            "Epoch 243/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1977 - accuracy: 0.5883\n",
            "Epoch 00243: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1975 - accuracy: 0.5883 - val_loss: 1.3551 - val_accuracy: 0.5217\n",
            "Epoch 244/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1974 - accuracy: 0.5900\n",
            "Epoch 00244: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1975 - accuracy: 0.5899 - val_loss: 1.3340 - val_accuracy: 0.5346\n",
            "Epoch 245/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1962 - accuracy: 0.5899\n",
            "Epoch 00245: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1966 - accuracy: 0.5897 - val_loss: 1.3458 - val_accuracy: 0.5253\n",
            "Epoch 246/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1977 - accuracy: 0.5897\n",
            "Epoch 00246: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1977 - accuracy: 0.5897 - val_loss: 1.3388 - val_accuracy: 0.5273\n",
            "Epoch 247/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1964 - accuracy: 0.5892\n",
            "Epoch 00247: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1964 - accuracy: 0.5892 - val_loss: 1.3398 - val_accuracy: 0.5294\n",
            "Epoch 248/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1960 - accuracy: 0.5898\n",
            "Epoch 00248: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1962 - accuracy: 0.5896 - val_loss: 1.3873 - val_accuracy: 0.5210\n",
            "Epoch 249/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2018 - accuracy: 0.5902\n",
            "Epoch 00249: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.2018 - accuracy: 0.5902 - val_loss: 1.3898 - val_accuracy: 0.5180\n",
            "Epoch 250/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.2021 - accuracy: 0.5882\n",
            "Epoch 00250: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2020 - accuracy: 0.5882 - val_loss: 1.3766 - val_accuracy: 0.5161\n",
            "Epoch 251/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2041 - accuracy: 0.5888\n",
            "Epoch 00251: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.2041 - accuracy: 0.5888 - val_loss: 1.3427 - val_accuracy: 0.5267\n",
            "Epoch 252/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1974 - accuracy: 0.5891\n",
            "Epoch 00252: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1975 - accuracy: 0.5891 - val_loss: 1.3330 - val_accuracy: 0.5329\n",
            "Epoch 253/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1951 - accuracy: 0.5900\n",
            "Epoch 00253: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1956 - accuracy: 0.5899 - val_loss: 1.3805 - val_accuracy: 0.5178\n",
            "Epoch 254/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1966 - accuracy: 0.5892\n",
            "Epoch 00254: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1966 - accuracy: 0.5893 - val_loss: 1.3840 - val_accuracy: 0.5181\n",
            "Epoch 255/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1966 - accuracy: 0.5898\n",
            "Epoch 00255: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1966 - accuracy: 0.5898 - val_loss: 1.3733 - val_accuracy: 0.5177\n",
            "Epoch 256/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1961 - accuracy: 0.5889\n",
            "Epoch 00256: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1965 - accuracy: 0.5886 - val_loss: 1.3322 - val_accuracy: 0.5327\n",
            "Epoch 257/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1969 - accuracy: 0.5889\n",
            "Epoch 00257: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1969 - accuracy: 0.5889 - val_loss: 1.3522 - val_accuracy: 0.5236\n",
            "Epoch 258/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1961 - accuracy: 0.5898\n",
            "Epoch 00258: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1962 - accuracy: 0.5898 - val_loss: 1.3518 - val_accuracy: 0.5257\n",
            "Epoch 259/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1939 - accuracy: 0.5915\n",
            "Epoch 00259: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1940 - accuracy: 0.5915 - val_loss: 1.4304 - val_accuracy: 0.5058\n",
            "Epoch 260/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1992 - accuracy: 0.5887\n",
            "Epoch 00260: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1992 - accuracy: 0.5886 - val_loss: 1.3556 - val_accuracy: 0.5206\n",
            "Epoch 261/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1974 - accuracy: 0.5900\n",
            "Epoch 00261: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1974 - accuracy: 0.5900 - val_loss: 1.3215 - val_accuracy: 0.5346\n",
            "Epoch 262/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1967 - accuracy: 0.5896\n",
            "Epoch 00262: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1967 - accuracy: 0.5896 - val_loss: 1.3257 - val_accuracy: 0.5299\n",
            "Epoch 263/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1930 - accuracy: 0.5900\n",
            "Epoch 00263: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1932 - accuracy: 0.5900 - val_loss: 1.3454 - val_accuracy: 0.5280\n",
            "Epoch 264/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1972 - accuracy: 0.5893\n",
            "Epoch 00264: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1969 - accuracy: 0.5895 - val_loss: 1.3264 - val_accuracy: 0.5303\n",
            "Epoch 265/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1956 - accuracy: 0.5902\n",
            "Epoch 00265: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1955 - accuracy: 0.5901 - val_loss: 1.3301 - val_accuracy: 0.5296\n",
            "Epoch 266/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1951 - accuracy: 0.5907\n",
            "Epoch 00266: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1948 - accuracy: 0.5907 - val_loss: 1.3391 - val_accuracy: 0.5264\n",
            "Epoch 267/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1939 - accuracy: 0.5905\n",
            "Epoch 00267: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1941 - accuracy: 0.5905 - val_loss: 1.3385 - val_accuracy: 0.5291\n",
            "Epoch 268/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1947 - accuracy: 0.5897\n",
            "Epoch 00268: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1947 - accuracy: 0.5897 - val_loss: 1.3508 - val_accuracy: 0.5254\n",
            "Epoch 269/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1949 - accuracy: 0.5901\n",
            "Epoch 00269: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1947 - accuracy: 0.5903 - val_loss: 1.3420 - val_accuracy: 0.5323\n",
            "Epoch 270/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1954 - accuracy: 0.5890\n",
            "Epoch 00270: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1954 - accuracy: 0.5890 - val_loss: 1.3743 - val_accuracy: 0.5172\n",
            "Epoch 271/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1953 - accuracy: 0.5894\n",
            "Epoch 00271: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1953 - accuracy: 0.5895 - val_loss: 1.3402 - val_accuracy: 0.5278\n",
            "Epoch 272/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1974 - accuracy: 0.5890\n",
            "Epoch 00272: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1974 - accuracy: 0.5889 - val_loss: 1.3710 - val_accuracy: 0.5175\n",
            "Epoch 273/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1953 - accuracy: 0.5899\n",
            "Epoch 00273: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1954 - accuracy: 0.5898 - val_loss: 1.3503 - val_accuracy: 0.5212\n",
            "Epoch 274/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1933 - accuracy: 0.5899\n",
            "Epoch 00274: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1933 - accuracy: 0.5900 - val_loss: 1.4398 - val_accuracy: 0.5033\n",
            "Epoch 275/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1929 - accuracy: 0.5892\n",
            "Epoch 00275: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1931 - accuracy: 0.5890 - val_loss: 1.3855 - val_accuracy: 0.5206\n",
            "Epoch 276/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1959 - accuracy: 0.5898\n",
            "Epoch 00276: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1960 - accuracy: 0.5896 - val_loss: 1.3239 - val_accuracy: 0.5337\n",
            "Epoch 277/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1955 - accuracy: 0.5891\n",
            "Epoch 00277: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1955 - accuracy: 0.5891 - val_loss: 1.3265 - val_accuracy: 0.5288\n",
            "Epoch 278/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1930 - accuracy: 0.5911\n",
            "Epoch 00278: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1929 - accuracy: 0.5910 - val_loss: 1.3603 - val_accuracy: 0.5248\n",
            "Epoch 279/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1949 - accuracy: 0.5894\n",
            "Epoch 00279: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1949 - accuracy: 0.5895 - val_loss: 1.3334 - val_accuracy: 0.5321\n",
            "Epoch 280/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1944 - accuracy: 0.5897\n",
            "Epoch 00280: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1944 - accuracy: 0.5897 - val_loss: 1.3861 - val_accuracy: 0.5128\n",
            "Epoch 281/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1966 - accuracy: 0.5893\n",
            "Epoch 00281: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1966 - accuracy: 0.5893 - val_loss: 1.3417 - val_accuracy: 0.5289\n",
            "Epoch 282/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1937 - accuracy: 0.5916\n",
            "Epoch 00282: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1939 - accuracy: 0.5916 - val_loss: 1.3391 - val_accuracy: 0.5309\n",
            "Epoch 283/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1977 - accuracy: 0.5881\n",
            "Epoch 00283: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1971 - accuracy: 0.5883 - val_loss: 1.3789 - val_accuracy: 0.5136\n",
            "Epoch 284/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1931 - accuracy: 0.5899\n",
            "Epoch 00284: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1931 - accuracy: 0.5899 - val_loss: 1.3814 - val_accuracy: 0.5155\n",
            "Epoch 285/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1939 - accuracy: 0.5896\n",
            "Epoch 00285: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1936 - accuracy: 0.5897 - val_loss: 1.3454 - val_accuracy: 0.5260\n",
            "Epoch 286/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1942 - accuracy: 0.5896\n",
            "Epoch 00286: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1943 - accuracy: 0.5896 - val_loss: 1.4144 - val_accuracy: 0.4962\n",
            "Epoch 287/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1979 - accuracy: 0.5877\n",
            "Epoch 00287: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1988 - accuracy: 0.5873 - val_loss: 1.3938 - val_accuracy: 0.5087\n",
            "Epoch 288/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1943 - accuracy: 0.5891\n",
            "Epoch 00288: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1943 - accuracy: 0.5891 - val_loss: 1.3174 - val_accuracy: 0.5348\n",
            "Epoch 289/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1917 - accuracy: 0.5903\n",
            "Epoch 00289: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1917 - accuracy: 0.5903 - val_loss: 1.3796 - val_accuracy: 0.5203\n",
            "Epoch 290/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1941 - accuracy: 0.5901\n",
            "Epoch 00290: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1941 - accuracy: 0.5899 - val_loss: 1.3636 - val_accuracy: 0.5186\n",
            "Epoch 291/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1947 - accuracy: 0.5899\n",
            "Epoch 00291: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1947 - accuracy: 0.5899 - val_loss: 1.3705 - val_accuracy: 0.5202\n",
            "Epoch 292/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1935 - accuracy: 0.5894\n",
            "Epoch 00292: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1934 - accuracy: 0.5895 - val_loss: 1.3268 - val_accuracy: 0.5279\n",
            "Epoch 293/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1938 - accuracy: 0.5908\n",
            "Epoch 00293: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1938 - accuracy: 0.5906 - val_loss: 1.3107 - val_accuracy: 0.5382\n",
            "Epoch 294/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1941 - accuracy: 0.5903\n",
            "Epoch 00294: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1943 - accuracy: 0.5903 - val_loss: 1.3283 - val_accuracy: 0.5299\n",
            "Epoch 295/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1916 - accuracy: 0.5908\n",
            "Epoch 00295: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1914 - accuracy: 0.5909 - val_loss: 1.3699 - val_accuracy: 0.5190\n",
            "Epoch 296/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1914 - accuracy: 0.5899\n",
            "Epoch 00296: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1914 - accuracy: 0.5900 - val_loss: 1.3211 - val_accuracy: 0.5362\n",
            "Epoch 297/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1932 - accuracy: 0.5899\n",
            "Epoch 00297: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1930 - accuracy: 0.5900 - val_loss: 1.3454 - val_accuracy: 0.5223\n",
            "Epoch 298/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1938 - accuracy: 0.5891\n",
            "Epoch 00298: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1952 - accuracy: 0.5886 - val_loss: 1.3666 - val_accuracy: 0.5205\n",
            "Epoch 299/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1954 - accuracy: 0.5891\n",
            "Epoch 00299: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1952 - accuracy: 0.5892 - val_loss: 1.3454 - val_accuracy: 0.5254\n",
            "Epoch 300/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1926 - accuracy: 0.5910\n",
            "Epoch 00300: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1926 - accuracy: 0.5909 - val_loss: 1.3346 - val_accuracy: 0.5292\n",
            "Epoch 301/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1914 - accuracy: 0.5908\n",
            "Epoch 00301: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1914 - accuracy: 0.5908 - val_loss: 1.3288 - val_accuracy: 0.5341\n",
            "Epoch 302/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1913 - accuracy: 0.5897\n",
            "Epoch 00302: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1912 - accuracy: 0.5898 - val_loss: 1.3634 - val_accuracy: 0.5186\n",
            "Epoch 303/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1929 - accuracy: 0.5894\n",
            "Epoch 00303: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1931 - accuracy: 0.5894 - val_loss: 1.3222 - val_accuracy: 0.5303\n",
            "Epoch 304/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1920 - accuracy: 0.5904\n",
            "Epoch 00304: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1920 - accuracy: 0.5904 - val_loss: 1.3583 - val_accuracy: 0.5219\n",
            "Epoch 305/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1925 - accuracy: 0.5912\n",
            "Epoch 00305: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1925 - accuracy: 0.5912 - val_loss: 1.3671 - val_accuracy: 0.5234\n",
            "Epoch 306/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1901 - accuracy: 0.5911\n",
            "Epoch 00306: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1902 - accuracy: 0.5912 - val_loss: 1.3403 - val_accuracy: 0.5273\n",
            "Epoch 307/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1949 - accuracy: 0.5893\n",
            "Epoch 00307: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1951 - accuracy: 0.5893 - val_loss: 1.4008 - val_accuracy: 0.5138\n",
            "Epoch 308/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1946 - accuracy: 0.5899\n",
            "Epoch 00308: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1943 - accuracy: 0.5900 - val_loss: 1.4087 - val_accuracy: 0.5047\n",
            "Epoch 309/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1914 - accuracy: 0.5909\n",
            "Epoch 00309: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1914 - accuracy: 0.5909 - val_loss: 1.3572 - val_accuracy: 0.5223\n",
            "Epoch 310/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1965 - accuracy: 0.5876\n",
            "Epoch 00310: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1964 - accuracy: 0.5876 - val_loss: 1.4033 - val_accuracy: 0.5066\n",
            "Epoch 311/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1894 - accuracy: 0.5912\n",
            "Epoch 00311: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1894 - accuracy: 0.5912 - val_loss: 1.3973 - val_accuracy: 0.5115\n",
            "Epoch 312/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1919 - accuracy: 0.5901\n",
            "Epoch 00312: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1920 - accuracy: 0.5900 - val_loss: 1.3409 - val_accuracy: 0.5265\n",
            "Epoch 313/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1939 - accuracy: 0.5897\n",
            "Epoch 00313: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1938 - accuracy: 0.5899 - val_loss: 1.3684 - val_accuracy: 0.5198\n",
            "Epoch 314/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1917 - accuracy: 0.5906\n",
            "Epoch 00314: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1919 - accuracy: 0.5904 - val_loss: 1.3583 - val_accuracy: 0.5253\n",
            "Epoch 315/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1901 - accuracy: 0.5917\n",
            "Epoch 00315: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1902 - accuracy: 0.5915 - val_loss: 1.3154 - val_accuracy: 0.5353\n",
            "Epoch 316/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1880 - accuracy: 0.5912\n",
            "Epoch 00316: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1883 - accuracy: 0.5911 - val_loss: 1.3710 - val_accuracy: 0.5175\n",
            "Epoch 317/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1908 - accuracy: 0.5897\n",
            "Epoch 00317: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1908 - accuracy: 0.5897 - val_loss: 1.3642 - val_accuracy: 0.5247\n",
            "Epoch 318/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1916 - accuracy: 0.5916\n",
            "Epoch 00318: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1920 - accuracy: 0.5915 - val_loss: 1.3480 - val_accuracy: 0.5288\n",
            "Epoch 319/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1921 - accuracy: 0.5899\n",
            "Epoch 00319: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1921 - accuracy: 0.5900 - val_loss: 1.4127 - val_accuracy: 0.5066\n",
            "Epoch 320/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1944 - accuracy: 0.5901\n",
            "Epoch 00320: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1943 - accuracy: 0.5900 - val_loss: 1.3552 - val_accuracy: 0.5211\n",
            "Epoch 321/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1929 - accuracy: 0.5907\n",
            "Epoch 00321: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1929 - accuracy: 0.5907 - val_loss: 1.3715 - val_accuracy: 0.5144\n",
            "Epoch 322/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1915 - accuracy: 0.5902\n",
            "Epoch 00322: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1912 - accuracy: 0.5903 - val_loss: 1.3220 - val_accuracy: 0.5338\n",
            "Epoch 323/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1918 - accuracy: 0.5893\n",
            "Epoch 00323: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1920 - accuracy: 0.5892 - val_loss: 1.3360 - val_accuracy: 0.5221\n",
            "Epoch 324/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1922 - accuracy: 0.5900\n",
            "Epoch 00324: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 13ms/step - loss: 1.1922 - accuracy: 0.5899 - val_loss: 1.3753 - val_accuracy: 0.5176\n",
            "Epoch 325/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1896 - accuracy: 0.5901\n",
            "Epoch 00325: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 5s 15ms/step - loss: 1.1901 - accuracy: 0.5900 - val_loss: 1.3323 - val_accuracy: 0.5283\n",
            "Epoch 326/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1912 - accuracy: 0.5894\n",
            "Epoch 00326: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1908 - accuracy: 0.5896 - val_loss: 1.3841 - val_accuracy: 0.5152\n",
            "Epoch 327/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1894 - accuracy: 0.5917\n",
            "Epoch 00327: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1896 - accuracy: 0.5916 - val_loss: 1.3794 - val_accuracy: 0.5205\n",
            "Epoch 328/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1916 - accuracy: 0.5898\n",
            "Epoch 00328: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1916 - accuracy: 0.5898 - val_loss: 1.3514 - val_accuracy: 0.5227\n",
            "Epoch 329/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1905 - accuracy: 0.5909\n",
            "Epoch 00329: val_accuracy did not improve from 0.53833\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1911 - accuracy: 0.5908 - val_loss: 1.3306 - val_accuracy: 0.5321\n",
            "Epoch 330/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1931 - accuracy: 0.5914\n",
            "Epoch 00330: val_accuracy improved from 0.53833 to 0.54058, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1929 - accuracy: 0.5914 - val_loss: 1.3086 - val_accuracy: 0.5406\n",
            "Epoch 331/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1920 - accuracy: 0.5890\n",
            "Epoch 00331: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1919 - accuracy: 0.5891 - val_loss: 1.3397 - val_accuracy: 0.5227\n",
            "Epoch 332/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1886 - accuracy: 0.5917\n",
            "Epoch 00332: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1883 - accuracy: 0.5919 - val_loss: 1.3613 - val_accuracy: 0.5235\n",
            "Epoch 333/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1886 - accuracy: 0.5913\n",
            "Epoch 00333: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1886 - accuracy: 0.5913 - val_loss: 1.3212 - val_accuracy: 0.5332\n",
            "Epoch 334/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1888 - accuracy: 0.5912\n",
            "Epoch 00334: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1888 - accuracy: 0.5912 - val_loss: 1.3959 - val_accuracy: 0.5121\n",
            "Epoch 335/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1873 - accuracy: 0.5913\n",
            "Epoch 00335: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1872 - accuracy: 0.5914 - val_loss: 1.3391 - val_accuracy: 0.5286\n",
            "Epoch 336/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1912 - accuracy: 0.5901\n",
            "Epoch 00336: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1906 - accuracy: 0.5902 - val_loss: 1.3132 - val_accuracy: 0.5384\n",
            "Epoch 337/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1914 - accuracy: 0.5900\n",
            "Epoch 00337: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1912 - accuracy: 0.5900 - val_loss: 1.3177 - val_accuracy: 0.5330\n",
            "Epoch 338/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1913 - accuracy: 0.5922\n",
            "Epoch 00338: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1909 - accuracy: 0.5921 - val_loss: 1.3170 - val_accuracy: 0.5336\n",
            "Epoch 339/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1888 - accuracy: 0.5916\n",
            "Epoch 00339: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1888 - accuracy: 0.5916 - val_loss: 1.3524 - val_accuracy: 0.5248\n",
            "Epoch 340/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1873 - accuracy: 0.5923\n",
            "Epoch 00340: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1873 - accuracy: 0.5923 - val_loss: 1.3786 - val_accuracy: 0.5210\n",
            "Epoch 341/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1892 - accuracy: 0.5911\n",
            "Epoch 00341: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1892 - accuracy: 0.5911 - val_loss: 1.3092 - val_accuracy: 0.5373\n",
            "Epoch 342/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1880 - accuracy: 0.5896\n",
            "Epoch 00342: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1885 - accuracy: 0.5894 - val_loss: 1.3402 - val_accuracy: 0.5285\n",
            "Epoch 343/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1889 - accuracy: 0.5915\n",
            "Epoch 00343: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1891 - accuracy: 0.5915 - val_loss: 1.3667 - val_accuracy: 0.5189\n",
            "Epoch 344/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1883 - accuracy: 0.5913\n",
            "Epoch 00344: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1883 - accuracy: 0.5914 - val_loss: 1.3308 - val_accuracy: 0.5329\n",
            "Epoch 345/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1880 - accuracy: 0.5917\n",
            "Epoch 00345: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1877 - accuracy: 0.5917 - val_loss: 1.4009 - val_accuracy: 0.5085\n",
            "Epoch 346/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1899 - accuracy: 0.5905\n",
            "Epoch 00346: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1901 - accuracy: 0.5904 - val_loss: 1.3445 - val_accuracy: 0.5228\n",
            "Epoch 347/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1887 - accuracy: 0.5918\n",
            "Epoch 00347: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1889 - accuracy: 0.5919 - val_loss: 1.4141 - val_accuracy: 0.5055\n",
            "Epoch 348/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1897 - accuracy: 0.5909\n",
            "Epoch 00348: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1896 - accuracy: 0.5909 - val_loss: 1.3702 - val_accuracy: 0.5178\n",
            "Epoch 349/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1893 - accuracy: 0.5914\n",
            "Epoch 00349: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1888 - accuracy: 0.5916 - val_loss: 1.3587 - val_accuracy: 0.5230\n",
            "Epoch 350/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1876 - accuracy: 0.5915\n",
            "Epoch 00350: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1876 - accuracy: 0.5915 - val_loss: 1.3515 - val_accuracy: 0.5241\n",
            "Epoch 351/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1879 - accuracy: 0.5915\n",
            "Epoch 00351: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1888 - accuracy: 0.5913 - val_loss: 1.3816 - val_accuracy: 0.5091\n",
            "Epoch 352/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1898 - accuracy: 0.5908\n",
            "Epoch 00352: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1900 - accuracy: 0.5908 - val_loss: 1.3853 - val_accuracy: 0.5133\n",
            "Epoch 353/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1898 - accuracy: 0.5900\n",
            "Epoch 00353: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1897 - accuracy: 0.5901 - val_loss: 1.3310 - val_accuracy: 0.5341\n",
            "Epoch 354/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1896 - accuracy: 0.5911\n",
            "Epoch 00354: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1902 - accuracy: 0.5908 - val_loss: 1.3187 - val_accuracy: 0.5355\n",
            "Epoch 355/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1911 - accuracy: 0.5902\n",
            "Epoch 00355: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1908 - accuracy: 0.5904 - val_loss: 1.3270 - val_accuracy: 0.5331\n",
            "Epoch 356/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1886 - accuracy: 0.5910\n",
            "Epoch 00356: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1886 - accuracy: 0.5910 - val_loss: 1.3417 - val_accuracy: 0.5281\n",
            "Epoch 357/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1869 - accuracy: 0.5918\n",
            "Epoch 00357: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1865 - accuracy: 0.5920 - val_loss: 1.3605 - val_accuracy: 0.5223\n",
            "Epoch 358/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1873 - accuracy: 0.5916\n",
            "Epoch 00358: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1871 - accuracy: 0.5916 - val_loss: 1.3586 - val_accuracy: 0.5263\n",
            "Epoch 359/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1905 - accuracy: 0.5894\n",
            "Epoch 00359: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1900 - accuracy: 0.5895 - val_loss: 1.3518 - val_accuracy: 0.5236\n",
            "Epoch 360/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1870 - accuracy: 0.5917\n",
            "Epoch 00360: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1870 - accuracy: 0.5917 - val_loss: 1.3650 - val_accuracy: 0.5191\n",
            "Epoch 361/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1880 - accuracy: 0.5922\n",
            "Epoch 00361: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1879 - accuracy: 0.5922 - val_loss: 1.3357 - val_accuracy: 0.5325\n",
            "Epoch 362/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1879 - accuracy: 0.5912\n",
            "Epoch 00362: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1882 - accuracy: 0.5910 - val_loss: 1.3567 - val_accuracy: 0.5247\n",
            "Epoch 363/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1901 - accuracy: 0.5906\n",
            "Epoch 00363: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1904 - accuracy: 0.5903 - val_loss: 1.3589 - val_accuracy: 0.5195\n",
            "Epoch 364/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1871 - accuracy: 0.5909\n",
            "Epoch 00364: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1875 - accuracy: 0.5909 - val_loss: 1.3465 - val_accuracy: 0.5266\n",
            "Epoch 365/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1861 - accuracy: 0.5919\n",
            "Epoch 00365: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1861 - accuracy: 0.5919 - val_loss: 1.3343 - val_accuracy: 0.5269\n",
            "Epoch 366/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1887 - accuracy: 0.5912\n",
            "Epoch 00366: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1887 - accuracy: 0.5912 - val_loss: 1.3301 - val_accuracy: 0.5291\n",
            "Epoch 367/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1874 - accuracy: 0.5921\n",
            "Epoch 00367: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1875 - accuracy: 0.5920 - val_loss: 1.3876 - val_accuracy: 0.5146\n",
            "Epoch 368/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1890 - accuracy: 0.5922\n",
            "Epoch 00368: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1894 - accuracy: 0.5921 - val_loss: 1.3902 - val_accuracy: 0.5102\n",
            "Epoch 369/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1905 - accuracy: 0.5894\n",
            "Epoch 00369: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1905 - accuracy: 0.5894 - val_loss: 1.3291 - val_accuracy: 0.5317\n",
            "Epoch 370/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1921 - accuracy: 0.5907\n",
            "Epoch 00370: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1915 - accuracy: 0.5910 - val_loss: 1.3281 - val_accuracy: 0.5336\n",
            "Epoch 371/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1842 - accuracy: 0.5923\n",
            "Epoch 00371: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1842 - accuracy: 0.5922 - val_loss: 1.3229 - val_accuracy: 0.5325\n",
            "Epoch 372/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1879 - accuracy: 0.5921\n",
            "Epoch 00372: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1879 - accuracy: 0.5922 - val_loss: 1.3705 - val_accuracy: 0.5143\n",
            "Epoch 373/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1868 - accuracy: 0.5915\n",
            "Epoch 00373: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1863 - accuracy: 0.5917 - val_loss: 1.3763 - val_accuracy: 0.5146\n",
            "Epoch 374/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1867 - accuracy: 0.5920\n",
            "Epoch 00374: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1867 - accuracy: 0.5920 - val_loss: 1.3309 - val_accuracy: 0.5323\n",
            "Epoch 375/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1834 - accuracy: 0.5929\n",
            "Epoch 00375: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1835 - accuracy: 0.5929 - val_loss: 1.3370 - val_accuracy: 0.5308\n",
            "Epoch 376/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1879 - accuracy: 0.5918\n",
            "Epoch 00376: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1879 - accuracy: 0.5918 - val_loss: 1.3390 - val_accuracy: 0.5262\n",
            "Epoch 377/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1861 - accuracy: 0.5913\n",
            "Epoch 00377: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1858 - accuracy: 0.5914 - val_loss: 1.3420 - val_accuracy: 0.5260\n",
            "Epoch 378/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1846 - accuracy: 0.5923\n",
            "Epoch 00378: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1842 - accuracy: 0.5923 - val_loss: 1.3409 - val_accuracy: 0.5275\n",
            "Epoch 379/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1877 - accuracy: 0.5912\n",
            "Epoch 00379: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1877 - accuracy: 0.5912 - val_loss: 1.3671 - val_accuracy: 0.5189\n",
            "Epoch 380/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1865 - accuracy: 0.5913\n",
            "Epoch 00380: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1865 - accuracy: 0.5913 - val_loss: 1.3558 - val_accuracy: 0.5176\n",
            "Epoch 381/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5919\n",
            "Epoch 00381: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1840 - accuracy: 0.5921 - val_loss: 1.3408 - val_accuracy: 0.5256\n",
            "Epoch 382/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1883 - accuracy: 0.5906\n",
            "Epoch 00382: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1885 - accuracy: 0.5905 - val_loss: 1.3348 - val_accuracy: 0.5288\n",
            "Epoch 383/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1877 - accuracy: 0.5914\n",
            "Epoch 00383: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1880 - accuracy: 0.5912 - val_loss: 1.3343 - val_accuracy: 0.5351\n",
            "Epoch 384/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1847 - accuracy: 0.5926\n",
            "Epoch 00384: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1847 - accuracy: 0.5926 - val_loss: 1.3125 - val_accuracy: 0.5329\n",
            "Epoch 385/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1848 - accuracy: 0.5935\n",
            "Epoch 00385: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1849 - accuracy: 0.5934 - val_loss: 1.3315 - val_accuracy: 0.5216\n",
            "Epoch 386/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1889 - accuracy: 0.5912\n",
            "Epoch 00386: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1893 - accuracy: 0.5910 - val_loss: 1.4011 - val_accuracy: 0.5090\n",
            "Epoch 387/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1879 - accuracy: 0.5915\n",
            "Epoch 00387: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1874 - accuracy: 0.5917 - val_loss: 1.3389 - val_accuracy: 0.5283\n",
            "Epoch 388/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1825 - accuracy: 0.5945\n",
            "Epoch 00388: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1827 - accuracy: 0.5944 - val_loss: 1.3133 - val_accuracy: 0.5349\n",
            "Epoch 389/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1873 - accuracy: 0.5925\n",
            "Epoch 00389: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1873 - accuracy: 0.5924 - val_loss: 1.3335 - val_accuracy: 0.5283\n",
            "Epoch 390/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1867 - accuracy: 0.5911\n",
            "Epoch 00390: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1869 - accuracy: 0.5911 - val_loss: 1.3386 - val_accuracy: 0.5296\n",
            "Epoch 391/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1852 - accuracy: 0.5929\n",
            "Epoch 00391: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1850 - accuracy: 0.5929 - val_loss: 1.3388 - val_accuracy: 0.5278\n",
            "Epoch 392/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1817 - accuracy: 0.5944\n",
            "Epoch 00392: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1829 - accuracy: 0.5938 - val_loss: 1.3633 - val_accuracy: 0.5249\n",
            "Epoch 393/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5924\n",
            "Epoch 00393: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1842 - accuracy: 0.5926 - val_loss: 1.3381 - val_accuracy: 0.5279\n",
            "Epoch 394/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1846 - accuracy: 0.5928\n",
            "Epoch 00394: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1849 - accuracy: 0.5927 - val_loss: 1.3376 - val_accuracy: 0.5301\n",
            "Epoch 395/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5927\n",
            "Epoch 00395: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1844 - accuracy: 0.5927 - val_loss: 1.3150 - val_accuracy: 0.5322\n",
            "Epoch 396/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1920 - accuracy: 0.5888\n",
            "Epoch 00396: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1917 - accuracy: 0.5891 - val_loss: 1.3581 - val_accuracy: 0.5230\n",
            "Epoch 397/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1846 - accuracy: 0.5922\n",
            "Epoch 00397: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1845 - accuracy: 0.5922 - val_loss: 1.3127 - val_accuracy: 0.5332\n",
            "Epoch 398/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1896 - accuracy: 0.5904\n",
            "Epoch 00398: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1891 - accuracy: 0.5907 - val_loss: 1.4054 - val_accuracy: 0.5109\n",
            "Epoch 399/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1847 - accuracy: 0.5918\n",
            "Epoch 00399: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1843 - accuracy: 0.5919 - val_loss: 1.3160 - val_accuracy: 0.5319\n",
            "Epoch 400/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1842 - accuracy: 0.5928\n",
            "Epoch 00400: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1846 - accuracy: 0.5928 - val_loss: 1.3288 - val_accuracy: 0.5356\n",
            "Epoch 401/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1867 - accuracy: 0.5906\n",
            "Epoch 00401: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1867 - accuracy: 0.5906 - val_loss: 1.3507 - val_accuracy: 0.5248\n",
            "Epoch 402/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1856 - accuracy: 0.5916\n",
            "Epoch 00402: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1851 - accuracy: 0.5918 - val_loss: 1.3761 - val_accuracy: 0.5152\n",
            "Epoch 403/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1836 - accuracy: 0.5930\n",
            "Epoch 00403: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1835 - accuracy: 0.5931 - val_loss: 1.3190 - val_accuracy: 0.5288\n",
            "Epoch 404/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5931\n",
            "Epoch 00404: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1844 - accuracy: 0.5931 - val_loss: 1.3766 - val_accuracy: 0.5156\n",
            "Epoch 405/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1840 - accuracy: 0.5913\n",
            "Epoch 00405: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1836 - accuracy: 0.5914 - val_loss: 1.3717 - val_accuracy: 0.5206\n",
            "Epoch 406/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1833 - accuracy: 0.5932\n",
            "Epoch 00406: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1827 - accuracy: 0.5935 - val_loss: 1.3600 - val_accuracy: 0.5269\n",
            "Epoch 407/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5926\n",
            "Epoch 00407: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1844 - accuracy: 0.5927 - val_loss: 1.3814 - val_accuracy: 0.5134\n",
            "Epoch 408/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1851 - accuracy: 0.5922\n",
            "Epoch 00408: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1852 - accuracy: 0.5922 - val_loss: 1.3347 - val_accuracy: 0.5298\n",
            "Epoch 409/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1846 - accuracy: 0.5939\n",
            "Epoch 00409: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1846 - accuracy: 0.5939 - val_loss: 1.3477 - val_accuracy: 0.5290\n",
            "Epoch 410/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1835 - accuracy: 0.5931\n",
            "Epoch 00410: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1845 - accuracy: 0.5928 - val_loss: 1.3394 - val_accuracy: 0.5297\n",
            "Epoch 411/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5924\n",
            "Epoch 00411: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1844 - accuracy: 0.5923 - val_loss: 1.3410 - val_accuracy: 0.5273\n",
            "Epoch 412/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1833 - accuracy: 0.5922\n",
            "Epoch 00412: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1828 - accuracy: 0.5924 - val_loss: 1.3363 - val_accuracy: 0.5277\n",
            "Epoch 413/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1836 - accuracy: 0.5930\n",
            "Epoch 00413: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1832 - accuracy: 0.5932 - val_loss: 1.3404 - val_accuracy: 0.5329\n",
            "Epoch 414/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1833 - accuracy: 0.5928\n",
            "Epoch 00414: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1834 - accuracy: 0.5930 - val_loss: 1.3975 - val_accuracy: 0.5116\n",
            "Epoch 415/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1816 - accuracy: 0.5933\n",
            "Epoch 00415: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1819 - accuracy: 0.5931 - val_loss: 1.3602 - val_accuracy: 0.5236\n",
            "Epoch 416/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1857 - accuracy: 0.5912\n",
            "Epoch 00416: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1856 - accuracy: 0.5913 - val_loss: 1.3326 - val_accuracy: 0.5306\n",
            "Epoch 417/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1827 - accuracy: 0.5928\n",
            "Epoch 00417: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1828 - accuracy: 0.5928 - val_loss: 1.3450 - val_accuracy: 0.5260\n",
            "Epoch 418/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1866 - accuracy: 0.5903\n",
            "Epoch 00418: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1867 - accuracy: 0.5902 - val_loss: 1.3482 - val_accuracy: 0.5278\n",
            "Epoch 419/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1834 - accuracy: 0.5923\n",
            "Epoch 00419: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1838 - accuracy: 0.5922 - val_loss: 1.3349 - val_accuracy: 0.5323\n",
            "Epoch 420/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1866 - accuracy: 0.5903\n",
            "Epoch 00420: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1865 - accuracy: 0.5904 - val_loss: 1.3879 - val_accuracy: 0.5110\n",
            "Epoch 421/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1841 - accuracy: 0.5935\n",
            "Epoch 00421: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1837 - accuracy: 0.5934 - val_loss: 1.3416 - val_accuracy: 0.5296\n",
            "Epoch 422/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1807 - accuracy: 0.5948\n",
            "Epoch 00422: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1806 - accuracy: 0.5946 - val_loss: 1.3243 - val_accuracy: 0.5320\n",
            "Epoch 423/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1857 - accuracy: 0.5927\n",
            "Epoch 00423: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1857 - accuracy: 0.5927 - val_loss: 1.3139 - val_accuracy: 0.5374\n",
            "Epoch 424/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1825 - accuracy: 0.5926\n",
            "Epoch 00424: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1825 - accuracy: 0.5926 - val_loss: 1.3620 - val_accuracy: 0.5296\n",
            "Epoch 425/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1858 - accuracy: 0.5917\n",
            "Epoch 00425: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1858 - accuracy: 0.5916 - val_loss: 1.3292 - val_accuracy: 0.5262\n",
            "Epoch 426/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1855 - accuracy: 0.5932\n",
            "Epoch 00426: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1855 - accuracy: 0.5932 - val_loss: 1.3418 - val_accuracy: 0.5240\n",
            "Epoch 427/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1817 - accuracy: 0.5936\n",
            "Epoch 00427: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1817 - accuracy: 0.5936 - val_loss: 1.3456 - val_accuracy: 0.5253\n",
            "Epoch 428/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1858 - accuracy: 0.5915\n",
            "Epoch 00428: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1858 - accuracy: 0.5913 - val_loss: 1.3266 - val_accuracy: 0.5331\n",
            "Epoch 429/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1879 - accuracy: 0.5927\n",
            "Epoch 00429: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1881 - accuracy: 0.5926 - val_loss: 1.3416 - val_accuracy: 0.5296\n",
            "Epoch 430/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1887 - accuracy: 0.5906\n",
            "Epoch 00430: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1877 - accuracy: 0.5911 - val_loss: 1.3581 - val_accuracy: 0.5279\n",
            "Epoch 431/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1814 - accuracy: 0.5931\n",
            "Epoch 00431: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1813 - accuracy: 0.5931 - val_loss: 1.3478 - val_accuracy: 0.5256\n",
            "Epoch 432/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1839 - accuracy: 0.5922\n",
            "Epoch 00432: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1839 - accuracy: 0.5922 - val_loss: 1.3166 - val_accuracy: 0.5327\n",
            "Epoch 433/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1832 - accuracy: 0.5932\n",
            "Epoch 00433: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1832 - accuracy: 0.5933 - val_loss: 1.3560 - val_accuracy: 0.5257\n",
            "Epoch 434/500\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1823 - accuracy: 0.5928\n",
            "Epoch 00434: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1826 - accuracy: 0.5925 - val_loss: 1.3118 - val_accuracy: 0.5346\n",
            "Epoch 435/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1829 - accuracy: 0.5927\n",
            "Epoch 00435: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1830 - accuracy: 0.5927 - val_loss: 1.4139 - val_accuracy: 0.5061\n",
            "Epoch 436/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1826 - accuracy: 0.5925\n",
            "Epoch 00436: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1825 - accuracy: 0.5925 - val_loss: 1.3590 - val_accuracy: 0.5174\n",
            "Epoch 437/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1839 - accuracy: 0.5924\n",
            "Epoch 00437: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1840 - accuracy: 0.5924 - val_loss: 1.3478 - val_accuracy: 0.5221\n",
            "Epoch 438/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1834 - accuracy: 0.5922\n",
            "Epoch 00438: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1837 - accuracy: 0.5921 - val_loss: 1.3408 - val_accuracy: 0.5266\n",
            "Epoch 439/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1816 - accuracy: 0.5921\n",
            "Epoch 00439: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1816 - accuracy: 0.5921 - val_loss: 1.3574 - val_accuracy: 0.5206\n",
            "Epoch 440/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5923\n",
            "Epoch 00440: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1839 - accuracy: 0.5924 - val_loss: 1.3339 - val_accuracy: 0.5275\n",
            "Epoch 441/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1839 - accuracy: 0.5924\n",
            "Epoch 00441: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1838 - accuracy: 0.5922 - val_loss: 1.3876 - val_accuracy: 0.5077\n",
            "Epoch 442/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1831 - accuracy: 0.5924\n",
            "Epoch 00442: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1829 - accuracy: 0.5923 - val_loss: 1.3632 - val_accuracy: 0.5195\n",
            "Epoch 443/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1818 - accuracy: 0.5916\n",
            "Epoch 00443: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1820 - accuracy: 0.5916 - val_loss: 1.3285 - val_accuracy: 0.5270\n",
            "Epoch 444/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1821 - accuracy: 0.5934\n",
            "Epoch 00444: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1820 - accuracy: 0.5934 - val_loss: 1.3398 - val_accuracy: 0.5325\n",
            "Epoch 445/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1833 - accuracy: 0.5925\n",
            "Epoch 00445: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1833 - accuracy: 0.5926 - val_loss: 1.3235 - val_accuracy: 0.5359\n",
            "Epoch 446/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5936\n",
            "Epoch 00446: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1851 - accuracy: 0.5933 - val_loss: 1.4195 - val_accuracy: 0.5116\n",
            "Epoch 447/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1798 - accuracy: 0.5936\n",
            "Epoch 00447: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1804 - accuracy: 0.5934 - val_loss: 1.3559 - val_accuracy: 0.5213\n",
            "Epoch 448/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1820 - accuracy: 0.5927\n",
            "Epoch 00448: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1826 - accuracy: 0.5924 - val_loss: 1.3441 - val_accuracy: 0.5249\n",
            "Epoch 449/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1837 - accuracy: 0.5924\n",
            "Epoch 00449: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1833 - accuracy: 0.5925 - val_loss: 1.3174 - val_accuracy: 0.5341\n",
            "Epoch 450/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1840 - accuracy: 0.5917\n",
            "Epoch 00450: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1840 - accuracy: 0.5917 - val_loss: 1.3574 - val_accuracy: 0.5172\n",
            "Epoch 451/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1812 - accuracy: 0.5924\n",
            "Epoch 00451: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1812 - accuracy: 0.5923 - val_loss: 1.3520 - val_accuracy: 0.5269\n",
            "Epoch 452/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1832 - accuracy: 0.5927\n",
            "Epoch 00452: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1831 - accuracy: 0.5927 - val_loss: 1.3408 - val_accuracy: 0.5237\n",
            "Epoch 453/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1831 - accuracy: 0.5931\n",
            "Epoch 00453: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1835 - accuracy: 0.5929 - val_loss: 1.3527 - val_accuracy: 0.5270\n",
            "Epoch 454/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1840 - accuracy: 0.5936\n",
            "Epoch 00454: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1842 - accuracy: 0.5936 - val_loss: 1.3253 - val_accuracy: 0.5299\n",
            "Epoch 455/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1790 - accuracy: 0.5942\n",
            "Epoch 00455: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1790 - accuracy: 0.5942 - val_loss: 1.3216 - val_accuracy: 0.5303\n",
            "Epoch 456/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1821 - accuracy: 0.5928\n",
            "Epoch 00456: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1821 - accuracy: 0.5928 - val_loss: 1.3282 - val_accuracy: 0.5273\n",
            "Epoch 457/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1815 - accuracy: 0.5933\n",
            "Epoch 00457: val_accuracy did not improve from 0.54058\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1815 - accuracy: 0.5933 - val_loss: 1.3307 - val_accuracy: 0.5303\n",
            "Epoch 458/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1797 - accuracy: 0.5947\n",
            "Epoch 00458: val_accuracy improved from 0.54058 to 0.54099, saving model to /content/drive/MyDrive/data分析/Mediapipe/NN_Medaipipe_weight.h5\n",
            "322/322 [==============================] - 4s 13ms/step - loss: 1.1796 - accuracy: 0.5947 - val_loss: 1.3037 - val_accuracy: 0.5410\n",
            "Epoch 459/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1829 - accuracy: 0.5931\n",
            "Epoch 00459: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1825 - accuracy: 0.5933 - val_loss: 1.3874 - val_accuracy: 0.5139\n",
            "Epoch 460/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1814 - accuracy: 0.5928\n",
            "Epoch 00460: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1812 - accuracy: 0.5931 - val_loss: 1.3754 - val_accuracy: 0.5182\n",
            "Epoch 461/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1818 - accuracy: 0.5949\n",
            "Epoch 00461: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1818 - accuracy: 0.5949 - val_loss: 1.3487 - val_accuracy: 0.5235\n",
            "Epoch 462/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1811 - accuracy: 0.5936\n",
            "Epoch 00462: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1806 - accuracy: 0.5938 - val_loss: 1.3287 - val_accuracy: 0.5259\n",
            "Epoch 463/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1836 - accuracy: 0.5928\n",
            "Epoch 00463: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1836 - accuracy: 0.5928 - val_loss: 1.3157 - val_accuracy: 0.5318\n",
            "Epoch 464/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1819 - accuracy: 0.5946\n",
            "Epoch 00464: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1819 - accuracy: 0.5946 - val_loss: 1.3035 - val_accuracy: 0.5397\n",
            "Epoch 465/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1829 - accuracy: 0.5927\n",
            "Epoch 00465: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1828 - accuracy: 0.5927 - val_loss: 1.3283 - val_accuracy: 0.5284\n",
            "Epoch 466/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1806 - accuracy: 0.5940\n",
            "Epoch 00466: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1806 - accuracy: 0.5941 - val_loss: 1.3520 - val_accuracy: 0.5164\n",
            "Epoch 467/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1836 - accuracy: 0.5920\n",
            "Epoch 00467: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1835 - accuracy: 0.5921 - val_loss: 1.3520 - val_accuracy: 0.5232\n",
            "Epoch 468/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1813 - accuracy: 0.5941\n",
            "Epoch 00468: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1815 - accuracy: 0.5940 - val_loss: 1.3511 - val_accuracy: 0.5244\n",
            "Epoch 469/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1810 - accuracy: 0.5939\n",
            "Epoch 00469: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1812 - accuracy: 0.5937 - val_loss: 1.3397 - val_accuracy: 0.5219\n",
            "Epoch 470/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1809 - accuracy: 0.5936\n",
            "Epoch 00470: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1812 - accuracy: 0.5935 - val_loss: 1.3375 - val_accuracy: 0.5268\n",
            "Epoch 471/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1846 - accuracy: 0.5915\n",
            "Epoch 00471: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1846 - accuracy: 0.5914 - val_loss: 1.3240 - val_accuracy: 0.5342\n",
            "Epoch 472/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1848 - accuracy: 0.5922\n",
            "Epoch 00472: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1844 - accuracy: 0.5922 - val_loss: 1.3437 - val_accuracy: 0.5338\n",
            "Epoch 473/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1821 - accuracy: 0.5934\n",
            "Epoch 00473: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1821 - accuracy: 0.5933 - val_loss: 1.3460 - val_accuracy: 0.5252\n",
            "Epoch 474/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1798 - accuracy: 0.5929\n",
            "Epoch 00474: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1800 - accuracy: 0.5929 - val_loss: 1.3726 - val_accuracy: 0.5163\n",
            "Epoch 475/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1794 - accuracy: 0.5945\n",
            "Epoch 00475: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1794 - accuracy: 0.5945 - val_loss: 1.3251 - val_accuracy: 0.5343\n",
            "Epoch 476/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1809 - accuracy: 0.5942\n",
            "Epoch 00476: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1809 - accuracy: 0.5942 - val_loss: 1.3539 - val_accuracy: 0.5234\n",
            "Epoch 477/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1795 - accuracy: 0.5942\n",
            "Epoch 00477: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1801 - accuracy: 0.5940 - val_loss: 1.3893 - val_accuracy: 0.5137\n",
            "Epoch 478/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1813 - accuracy: 0.5925\n",
            "Epoch 00478: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1815 - accuracy: 0.5924 - val_loss: 1.3320 - val_accuracy: 0.5325\n",
            "Epoch 479/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1809 - accuracy: 0.5925\n",
            "Epoch 00479: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1802 - accuracy: 0.5929 - val_loss: 1.3058 - val_accuracy: 0.5399\n",
            "Epoch 480/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1791 - accuracy: 0.5931\n",
            "Epoch 00480: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1791 - accuracy: 0.5931 - val_loss: 1.3392 - val_accuracy: 0.5295\n",
            "Epoch 481/500\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1835 - accuracy: 0.5931\n",
            "Epoch 00481: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1830 - accuracy: 0.5932 - val_loss: 1.3346 - val_accuracy: 0.5293\n",
            "Epoch 482/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1800 - accuracy: 0.5930\n",
            "Epoch 00482: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1800 - accuracy: 0.5930 - val_loss: 1.3573 - val_accuracy: 0.5233\n",
            "Epoch 483/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1843 - accuracy: 0.5931\n",
            "Epoch 00483: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1841 - accuracy: 0.5932 - val_loss: 1.3827 - val_accuracy: 0.5230\n",
            "Epoch 484/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1792 - accuracy: 0.5939\n",
            "Epoch 00484: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1795 - accuracy: 0.5937 - val_loss: 1.3338 - val_accuracy: 0.5313\n",
            "Epoch 485/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1818 - accuracy: 0.5925\n",
            "Epoch 00485: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1814 - accuracy: 0.5928 - val_loss: 1.3224 - val_accuracy: 0.5342\n",
            "Epoch 486/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1805 - accuracy: 0.5937\n",
            "Epoch 00486: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1803 - accuracy: 0.5938 - val_loss: 1.3707 - val_accuracy: 0.5223\n",
            "Epoch 487/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1832 - accuracy: 0.5916\n",
            "Epoch 00487: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.1832 - accuracy: 0.5916 - val_loss: 1.3260 - val_accuracy: 0.5308\n",
            "Epoch 488/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1810 - accuracy: 0.5939\n",
            "Epoch 00488: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1811 - accuracy: 0.5938 - val_loss: 1.3299 - val_accuracy: 0.5306\n",
            "Epoch 489/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1819 - accuracy: 0.5939\n",
            "Epoch 00489: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1817 - accuracy: 0.5941 - val_loss: 1.3395 - val_accuracy: 0.5280\n",
            "Epoch 490/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1818 - accuracy: 0.5935\n",
            "Epoch 00490: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1813 - accuracy: 0.5936 - val_loss: 1.3537 - val_accuracy: 0.5203\n",
            "Epoch 491/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1815 - accuracy: 0.5936\n",
            "Epoch 00491: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1814 - accuracy: 0.5937 - val_loss: 1.3496 - val_accuracy: 0.5284\n",
            "Epoch 492/500\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1814 - accuracy: 0.5943\n",
            "Epoch 00492: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1812 - accuracy: 0.5944 - val_loss: 1.3224 - val_accuracy: 0.5342\n",
            "Epoch 493/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1787 - accuracy: 0.5950\n",
            "Epoch 00493: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1787 - accuracy: 0.5950 - val_loss: 1.3265 - val_accuracy: 0.5322\n",
            "Epoch 494/500\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1796 - accuracy: 0.5945\n",
            "Epoch 00494: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1795 - accuracy: 0.5946 - val_loss: 1.3424 - val_accuracy: 0.5310\n",
            "Epoch 495/500\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1796 - accuracy: 0.5930\n",
            "Epoch 00495: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1794 - accuracy: 0.5932 - val_loss: 1.3226 - val_accuracy: 0.5375\n",
            "Epoch 496/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1813 - accuracy: 0.5923\n",
            "Epoch 00496: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1815 - accuracy: 0.5922 - val_loss: 1.3673 - val_accuracy: 0.5210\n",
            "Epoch 497/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1767 - accuracy: 0.5946\n",
            "Epoch 00497: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1765 - accuracy: 0.5947 - val_loss: 1.3421 - val_accuracy: 0.5284\n",
            "Epoch 498/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1826 - accuracy: 0.5926\n",
            "Epoch 00498: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1826 - accuracy: 0.5926 - val_loss: 1.3180 - val_accuracy: 0.5339\n",
            "Epoch 499/500\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1827 - accuracy: 0.5926\n",
            "Epoch 00499: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.1827 - accuracy: 0.5926 - val_loss: 1.3310 - val_accuracy: 0.5272\n",
            "Epoch 500/500\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1803 - accuracy: 0.5944\n",
            "Epoch 00500: val_accuracy did not improve from 0.54099\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.1807 - accuracy: 0.5944 - val_loss: 1.3222 - val_accuracy: 0.5275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDSyKZrgXdaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92da4a3f-e942-4599-b98a-b825f711db4c"
      },
      "source": [
        "score = model.evaluate(x_test,y_test,verbose=1)\n",
        "print(\"\\n\")\n",
        "print(\"Test loss:\",score[0])\n",
        "print(\"Test accuracy:\",score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "461/461 [==============================] - 1s 2ms/step - loss: 1.2965 - accuracy: 0.5392\n",
            "\n",
            "\n",
            "Test loss: 1.2965295314788818\n",
            "Test accuracy: 0.5391664505004883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhG1k3kVXkO9"
      },
      "source": [
        "def plot_history(history):\n",
        "    # print(history.history.keys())\n",
        "\n",
        "    # 精度の履歴をプロット\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['accuracy', 'val_accuracy'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # 損失の履歴をプロット\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['loss', 'val_loss'], loc='lower right')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gyxnkxyXlr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "394ffc61-d133-4555-aabd-555c59f9a4b2"
      },
      "source": [
        "# 学習履歴をプロット\n",
        "plot_history(history)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}