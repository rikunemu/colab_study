{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mediapipe_csv_deepLaeraing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKmOaw4sxlQI47P7k1HKp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikunemu/colab_study/blob/main/mediapipe_csv_deepLaeraing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12_PCa7i1Bzr"
      },
      "source": [
        "#  mediapipeで取った特徴点を分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rptBUegwvb1"
      },
      "source": [
        "90分対策  \n",
        "F12を開いて以下のスクリプトをコンソールで実行  \n",
        "function KeepClicking(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(KeepClicking,600000)\n",
        "\n",
        "  \n",
        "10分ごとに接続"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQzvztFP513g",
        "outputId": "e9d8fe67-e601-4078-b39d-51c50c87a228"
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import os\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import utils as np_utils\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3KcXQxqltwZ"
      },
      "source": [
        "## csvデータ読み込み\n",
        "train,test,validそれぞれ7感情の特徴点を記録"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYz-88dFlO4e"
      },
      "source": [
        "fp_train='/content/drive/MyDrive/data分析/Mediapipe/csvtraintotal.csv'\n",
        "df_train=pd.read_csv(fp_train)\n",
        "fp_test='/content/drive/MyDrive/data分析/Mediapipe/csvtesttotal.csv'\n",
        "df_test=pd.read_csv(fp_test)\n",
        "fp_valid='/content/drive/MyDrive/data分析/Mediapipe/csvvalidtotal.csv'\n",
        "df_valid=pd.read_csv(fp_valid)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "OnuDyefiX3F1",
        "outputId": "a6fc7e83-1572-4572-d575-8eb2274a2d43"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "      <td>8108</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "      <td>8109</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "      <td>8110</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>8111</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "      <td>8112</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.470799  0.682127 -0.080930  ... -0.045183           0        0\n",
              "1      0.469567  0.799870 -0.017644  ... -0.206077           3        0\n",
              "2      0.711111  0.717369 -0.049849  ... -0.146556           4        0\n",
              "3      0.483527  0.809326 -0.011760  ... -0.195280           5        0\n",
              "4      0.061688  0.692855 -0.050553  ... -0.058228           6        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "82190  0.436367  0.787778 -0.124726  ... -0.051585        8108        6\n",
              "82191  0.456891  0.704518 -0.093469  ... -0.061855        8109        6\n",
              "82192  0.502264  0.729350 -0.124422  ... -0.034920        8110        6\n",
              "82193  0.539922  0.733853 -0.116346  ... -0.023272        8111        6\n",
              "82194  0.508446  0.805487 -0.070251  ... -0.153780        8112        6\n",
              "\n",
              "[82195 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "IwOLXOUD1NKy",
        "outputId": "1f3d0e1e-9721-4617-c873-050a563d8705"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.516601</td>\n",
              "      <td>0.585682</td>\n",
              "      <td>-0.128171</td>\n",
              "      <td>0.354741</td>\n",
              "      <td>0.407085</td>\n",
              "      <td>-0.017539</td>\n",
              "      <td>0.303413</td>\n",
              "      <td>0.437223</td>\n",
              "      <td>-0.022323</td>\n",
              "      <td>0.385631</td>\n",
              "      <td>0.474988</td>\n",
              "      <td>-0.077427</td>\n",
              "      <td>0.192280</td>\n",
              "      <td>0.117430</td>\n",
              "      <td>0.133606</td>\n",
              "      <td>0.227670</td>\n",
              "      <td>0.153081</td>\n",
              "      <td>0.075319</td>\n",
              "      <td>0.261869</td>\n",
              "      <td>0.196746</td>\n",
              "      <td>0.025099</td>\n",
              "      <td>0.384131</td>\n",
              "      <td>0.734853</td>\n",
              "      <td>-0.092528</td>\n",
              "      <td>0.409821</td>\n",
              "      <td>0.185824</td>\n",
              "      <td>-0.000665</td>\n",
              "      <td>0.388524</td>\n",
              "      <td>0.114111</td>\n",
              "      <td>0.043742</td>\n",
              "      <td>0.366814</td>\n",
              "      <td>0.051379</td>\n",
              "      <td>0.084901</td>\n",
              "      <td>0.466509</td>\n",
              "      <td>0.041154</td>\n",
              "      <td>0.091456</td>\n",
              "      <td>0.247168</td>\n",
              "      <td>0.348231</td>\n",
              "      <td>0.038491</td>\n",
              "      <td>0.162550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.673695</td>\n",
              "      <td>-0.099631</td>\n",
              "      <td>0.486085</td>\n",
              "      <td>0.227604</td>\n",
              "      <td>-0.000800</td>\n",
              "      <td>0.411432</td>\n",
              "      <td>0.685160</td>\n",
              "      <td>-0.107264</td>\n",
              "      <td>0.403327</td>\n",
              "      <td>0.699680</td>\n",
              "      <td>-0.105119</td>\n",
              "      <td>0.365605</td>\n",
              "      <td>0.599879</td>\n",
              "      <td>-0.087972</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>0.566646</td>\n",
              "      <td>0.268614</td>\n",
              "      <td>0.517140</td>\n",
              "      <td>0.487549</td>\n",
              "      <td>-0.123952</td>\n",
              "      <td>0.407326</td>\n",
              "      <td>0.672411</td>\n",
              "      <td>-0.080824</td>\n",
              "      <td>0.400786</td>\n",
              "      <td>0.675086</td>\n",
              "      <td>-0.085923</td>\n",
              "      <td>0.467047</td>\n",
              "      <td>0.510292</td>\n",
              "      <td>-0.103923</td>\n",
              "      <td>0.406921</td>\n",
              "      <td>0.514333</td>\n",
              "      <td>-0.071427</td>\n",
              "      <td>0.460763</td>\n",
              "      <td>0.503372</td>\n",
              "      <td>-0.100683</td>\n",
              "      <td>0.483841</td>\n",
              "      <td>0.187230</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.350142</td>\n",
              "      <td>0.704083</td>\n",
              "      <td>-0.051000</td>\n",
              "      <td>0.258494</td>\n",
              "      <td>0.467294</td>\n",
              "      <td>0.006417</td>\n",
              "      <td>0.210762</td>\n",
              "      <td>0.484544</td>\n",
              "      <td>0.028842</td>\n",
              "      <td>0.256672</td>\n",
              "      <td>0.560486</td>\n",
              "      <td>-0.029805</td>\n",
              "      <td>0.163514</td>\n",
              "      <td>0.115912</td>\n",
              "      <td>0.038936</td>\n",
              "      <td>0.169306</td>\n",
              "      <td>0.169693</td>\n",
              "      <td>-0.000869</td>\n",
              "      <td>0.178214</td>\n",
              "      <td>0.229622</td>\n",
              "      <td>-0.030477</td>\n",
              "      <td>0.262613</td>\n",
              "      <td>0.771681</td>\n",
              "      <td>0.078652</td>\n",
              "      <td>0.294888</td>\n",
              "      <td>0.263658</td>\n",
              "      <td>-0.103125</td>\n",
              "      <td>0.291660</td>\n",
              "      <td>0.172433</td>\n",
              "      <td>-0.100819</td>\n",
              "      <td>0.290796</td>\n",
              "      <td>0.086691</td>\n",
              "      <td>-0.094484</td>\n",
              "      <td>0.382622</td>\n",
              "      <td>0.088537</td>\n",
              "      <td>-0.127045</td>\n",
              "      <td>0.190766</td>\n",
              "      <td>0.370796</td>\n",
              "      <td>0.059996</td>\n",
              "      <td>0.127881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.731658</td>\n",
              "      <td>0.042865</td>\n",
              "      <td>0.366543</td>\n",
              "      <td>0.318007</td>\n",
              "      <td>-0.107573</td>\n",
              "      <td>0.279443</td>\n",
              "      <td>0.738324</td>\n",
              "      <td>0.041889</td>\n",
              "      <td>0.274230</td>\n",
              "      <td>0.747920</td>\n",
              "      <td>0.050578</td>\n",
              "      <td>0.236568</td>\n",
              "      <td>0.660978</td>\n",
              "      <td>0.032212</td>\n",
              "      <td>0.120189</td>\n",
              "      <td>0.495042</td>\n",
              "      <td>0.403236</td>\n",
              "      <td>0.337132</td>\n",
              "      <td>0.624860</td>\n",
              "      <td>-0.094728</td>\n",
              "      <td>0.281208</td>\n",
              "      <td>0.722307</td>\n",
              "      <td>0.068394</td>\n",
              "      <td>0.274025</td>\n",
              "      <td>0.721999</td>\n",
              "      <td>0.067390</td>\n",
              "      <td>0.308012</td>\n",
              "      <td>0.624537</td>\n",
              "      <td>-0.054114</td>\n",
              "      <td>0.268657</td>\n",
              "      <td>0.601183</td>\n",
              "      <td>-0.016952</td>\n",
              "      <td>0.302620</td>\n",
              "      <td>0.616359</td>\n",
              "      <td>-0.057269</td>\n",
              "      <td>0.366551</td>\n",
              "      <td>0.274136</td>\n",
              "      <td>-0.123009</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.478097</td>\n",
              "      <td>0.829416</td>\n",
              "      <td>-0.035864</td>\n",
              "      <td>0.338219</td>\n",
              "      <td>0.586676</td>\n",
              "      <td>-0.044701</td>\n",
              "      <td>0.281508</td>\n",
              "      <td>0.608822</td>\n",
              "      <td>-0.030072</td>\n",
              "      <td>0.362336</td>\n",
              "      <td>0.690208</td>\n",
              "      <td>-0.055267</td>\n",
              "      <td>0.158889</td>\n",
              "      <td>0.208705</td>\n",
              "      <td>-0.127339</td>\n",
              "      <td>0.195300</td>\n",
              "      <td>0.286799</td>\n",
              "      <td>-0.150213</td>\n",
              "      <td>0.229269</td>\n",
              "      <td>0.368802</td>\n",
              "      <td>-0.164157</td>\n",
              "      <td>0.357517</td>\n",
              "      <td>0.888281</td>\n",
              "      <td>0.091781</td>\n",
              "      <td>0.391913</td>\n",
              "      <td>0.394345</td>\n",
              "      <td>-0.200909</td>\n",
              "      <td>0.374054</td>\n",
              "      <td>0.287385</td>\n",
              "      <td>-0.220581</td>\n",
              "      <td>0.355048</td>\n",
              "      <td>0.184166</td>\n",
              "      <td>-0.231421</td>\n",
              "      <td>0.476613</td>\n",
              "      <td>0.184120</td>\n",
              "      <td>-0.238496</td>\n",
              "      <td>0.228681</td>\n",
              "      <td>0.488865</td>\n",
              "      <td>-0.035819</td>\n",
              "      <td>0.134504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.857773</td>\n",
              "      <td>0.046278</td>\n",
              "      <td>0.478034</td>\n",
              "      <td>0.444558</td>\n",
              "      <td>-0.172615</td>\n",
              "      <td>0.386257</td>\n",
              "      <td>0.865752</td>\n",
              "      <td>0.048123</td>\n",
              "      <td>0.378545</td>\n",
              "      <td>0.874142</td>\n",
              "      <td>0.057897</td>\n",
              "      <td>0.340892</td>\n",
              "      <td>0.793487</td>\n",
              "      <td>0.010045</td>\n",
              "      <td>0.054897</td>\n",
              "      <td>0.550742</td>\n",
              "      <td>0.351424</td>\n",
              "      <td>0.478415</td>\n",
              "      <td>0.751673</td>\n",
              "      <td>-0.092016</td>\n",
              "      <td>0.385269</td>\n",
              "      <td>0.849144</td>\n",
              "      <td>0.059494</td>\n",
              "      <td>0.379350</td>\n",
              "      <td>0.852295</td>\n",
              "      <td>0.059011</td>\n",
              "      <td>0.434535</td>\n",
              "      <td>0.750683</td>\n",
              "      <td>-0.052339</td>\n",
              "      <td>0.380534</td>\n",
              "      <td>0.728616</td>\n",
              "      <td>-0.024325</td>\n",
              "      <td>0.428548</td>\n",
              "      <td>0.743129</td>\n",
              "      <td>-0.058275</td>\n",
              "      <td>0.478435</td>\n",
              "      <td>0.401297</td>\n",
              "      <td>-0.201465</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.440407</td>\n",
              "      <td>0.633268</td>\n",
              "      <td>-0.130409</td>\n",
              "      <td>0.313094</td>\n",
              "      <td>0.456206</td>\n",
              "      <td>0.001129</td>\n",
              "      <td>0.255751</td>\n",
              "      <td>0.477937</td>\n",
              "      <td>0.012412</td>\n",
              "      <td>0.327564</td>\n",
              "      <td>0.527424</td>\n",
              "      <td>-0.067629</td>\n",
              "      <td>0.187209</td>\n",
              "      <td>0.126621</td>\n",
              "      <td>0.163992</td>\n",
              "      <td>0.209704</td>\n",
              "      <td>0.178016</td>\n",
              "      <td>0.096623</td>\n",
              "      <td>0.230991</td>\n",
              "      <td>0.237480</td>\n",
              "      <td>0.042394</td>\n",
              "      <td>0.312069</td>\n",
              "      <td>0.801668</td>\n",
              "      <td>0.008883</td>\n",
              "      <td>0.372335</td>\n",
              "      <td>0.248090</td>\n",
              "      <td>-0.028703</td>\n",
              "      <td>0.360934</td>\n",
              "      <td>0.151648</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.349558</td>\n",
              "      <td>0.068262</td>\n",
              "      <td>0.044676</td>\n",
              "      <td>0.458189</td>\n",
              "      <td>0.061373</td>\n",
              "      <td>0.022868</td>\n",
              "      <td>0.221603</td>\n",
              "      <td>0.385956</td>\n",
              "      <td>0.085039</td>\n",
              "      <td>0.143525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.774976</td>\n",
              "      <td>-0.017430</td>\n",
              "      <td>0.452601</td>\n",
              "      <td>0.292870</td>\n",
              "      <td>-0.042988</td>\n",
              "      <td>0.331663</td>\n",
              "      <td>0.783315</td>\n",
              "      <td>-0.020361</td>\n",
              "      <td>0.322092</td>\n",
              "      <td>0.792106</td>\n",
              "      <td>-0.013262</td>\n",
              "      <td>0.298874</td>\n",
              "      <td>0.636628</td>\n",
              "      <td>-0.034253</td>\n",
              "      <td>0.112172</td>\n",
              "      <td>0.549183</td>\n",
              "      <td>0.409472</td>\n",
              "      <td>0.433293</td>\n",
              "      <td>0.558691</td>\n",
              "      <td>-0.145084</td>\n",
              "      <td>0.341583</td>\n",
              "      <td>0.752286</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.752438</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.395023</td>\n",
              "      <td>0.569501</td>\n",
              "      <td>-0.106214</td>\n",
              "      <td>0.345461</td>\n",
              "      <td>0.563145</td>\n",
              "      <td>-0.061551</td>\n",
              "      <td>0.390786</td>\n",
              "      <td>0.563328</td>\n",
              "      <td>-0.106485</td>\n",
              "      <td>0.452236</td>\n",
              "      <td>0.249019</td>\n",
              "      <td>-0.042549</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.508843</td>\n",
              "      <td>0.783509</td>\n",
              "      <td>-0.075715</td>\n",
              "      <td>0.342278</td>\n",
              "      <td>0.582771</td>\n",
              "      <td>-0.031950</td>\n",
              "      <td>0.289867</td>\n",
              "      <td>0.611026</td>\n",
              "      <td>-0.020946</td>\n",
              "      <td>0.375631</td>\n",
              "      <td>0.676300</td>\n",
              "      <td>-0.067914</td>\n",
              "      <td>0.148420</td>\n",
              "      <td>0.218699</td>\n",
              "      <td>-0.009825</td>\n",
              "      <td>0.183308</td>\n",
              "      <td>0.281258</td>\n",
              "      <td>-0.048717</td>\n",
              "      <td>0.216023</td>\n",
              "      <td>0.349559</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>0.396198</td>\n",
              "      <td>0.916336</td>\n",
              "      <td>0.045910</td>\n",
              "      <td>0.370082</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>-0.122842</td>\n",
              "      <td>0.345563</td>\n",
              "      <td>0.249492</td>\n",
              "      <td>-0.119133</td>\n",
              "      <td>0.317737</td>\n",
              "      <td>0.152755</td>\n",
              "      <td>-0.109723</td>\n",
              "      <td>0.428981</td>\n",
              "      <td>0.135849</td>\n",
              "      <td>-0.117880</td>\n",
              "      <td>0.226637</td>\n",
              "      <td>0.504523</td>\n",
              "      <td>0.004935</td>\n",
              "      <td>0.154398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.875826</td>\n",
              "      <td>0.011859</td>\n",
              "      <td>0.461890</td>\n",
              "      <td>0.394244</td>\n",
              "      <td>-0.109819</td>\n",
              "      <td>0.415073</td>\n",
              "      <td>0.887762</td>\n",
              "      <td>0.011303</td>\n",
              "      <td>0.407975</td>\n",
              "      <td>0.899817</td>\n",
              "      <td>0.019094</td>\n",
              "      <td>0.362383</td>\n",
              "      <td>0.781520</td>\n",
              "      <td>-0.017575</td>\n",
              "      <td>0.112559</td>\n",
              "      <td>0.630753</td>\n",
              "      <td>0.341884</td>\n",
              "      <td>0.502403</td>\n",
              "      <td>0.719198</td>\n",
              "      <td>-0.114545</td>\n",
              "      <td>0.415689</td>\n",
              "      <td>0.858130</td>\n",
              "      <td>0.028379</td>\n",
              "      <td>0.405933</td>\n",
              "      <td>0.861435</td>\n",
              "      <td>0.026839</td>\n",
              "      <td>0.456500</td>\n",
              "      <td>0.724646</td>\n",
              "      <td>-0.078316</td>\n",
              "      <td>0.398571</td>\n",
              "      <td>0.712659</td>\n",
              "      <td>-0.046950</td>\n",
              "      <td>0.450282</td>\n",
              "      <td>0.718590</td>\n",
              "      <td>-0.081530</td>\n",
              "      <td>0.455816</td>\n",
              "      <td>0.346828</td>\n",
              "      <td>-0.125290</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14727</th>\n",
              "      <td>0.560496</td>\n",
              "      <td>0.769144</td>\n",
              "      <td>-0.111749</td>\n",
              "      <td>0.395681</td>\n",
              "      <td>0.565257</td>\n",
              "      <td>-0.017007</td>\n",
              "      <td>0.342978</td>\n",
              "      <td>0.594650</td>\n",
              "      <td>-0.008701</td>\n",
              "      <td>0.431437</td>\n",
              "      <td>0.644474</td>\n",
              "      <td>-0.072280</td>\n",
              "      <td>0.212711</td>\n",
              "      <td>0.199529</td>\n",
              "      <td>0.095772</td>\n",
              "      <td>0.235907</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>0.043887</td>\n",
              "      <td>0.261247</td>\n",
              "      <td>0.286265</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>0.444982</td>\n",
              "      <td>0.899928</td>\n",
              "      <td>-0.032473</td>\n",
              "      <td>0.421535</td>\n",
              "      <td>0.284107</td>\n",
              "      <td>-0.043228</td>\n",
              "      <td>0.409811</td>\n",
              "      <td>0.212278</td>\n",
              "      <td>-0.018695</td>\n",
              "      <td>0.392661</td>\n",
              "      <td>0.132065</td>\n",
              "      <td>0.010029</td>\n",
              "      <td>0.507709</td>\n",
              "      <td>0.118564</td>\n",
              "      <td>0.007106</td>\n",
              "      <td>0.279881</td>\n",
              "      <td>0.494782</td>\n",
              "      <td>0.037404</td>\n",
              "      <td>0.207185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.865063</td>\n",
              "      <td>-0.052868</td>\n",
              "      <td>0.521913</td>\n",
              "      <td>0.335642</td>\n",
              "      <td>-0.041206</td>\n",
              "      <td>0.468909</td>\n",
              "      <td>0.876704</td>\n",
              "      <td>-0.059213</td>\n",
              "      <td>0.459812</td>\n",
              "      <td>0.887839</td>\n",
              "      <td>-0.054344</td>\n",
              "      <td>0.422744</td>\n",
              "      <td>0.770018</td>\n",
              "      <td>-0.057373</td>\n",
              "      <td>0.168215</td>\n",
              "      <td>0.636903</td>\n",
              "      <td>0.338159</td>\n",
              "      <td>0.546422</td>\n",
              "      <td>0.660709</td>\n",
              "      <td>-0.123501</td>\n",
              "      <td>0.473307</td>\n",
              "      <td>0.854591</td>\n",
              "      <td>-0.037045</td>\n",
              "      <td>0.464889</td>\n",
              "      <td>0.859350</td>\n",
              "      <td>-0.040605</td>\n",
              "      <td>0.506149</td>\n",
              "      <td>0.682234</td>\n",
              "      <td>-0.094430</td>\n",
              "      <td>0.455463</td>\n",
              "      <td>0.684035</td>\n",
              "      <td>-0.061109</td>\n",
              "      <td>0.500817</td>\n",
              "      <td>0.675388</td>\n",
              "      <td>-0.092881</td>\n",
              "      <td>0.517969</td>\n",
              "      <td>0.281741</td>\n",
              "      <td>-0.043033</td>\n",
              "      <td>1470</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14728</th>\n",
              "      <td>0.491013</td>\n",
              "      <td>0.712008</td>\n",
              "      <td>-0.122762</td>\n",
              "      <td>0.370838</td>\n",
              "      <td>0.521274</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.321248</td>\n",
              "      <td>0.549112</td>\n",
              "      <td>0.029226</td>\n",
              "      <td>0.397335</td>\n",
              "      <td>0.598340</td>\n",
              "      <td>-0.050178</td>\n",
              "      <td>0.231024</td>\n",
              "      <td>0.155950</td>\n",
              "      <td>0.198852</td>\n",
              "      <td>0.244556</td>\n",
              "      <td>0.193912</td>\n",
              "      <td>0.136056</td>\n",
              "      <td>0.262082</td>\n",
              "      <td>0.232480</td>\n",
              "      <td>0.084471</td>\n",
              "      <td>0.410388</td>\n",
              "      <td>0.861402</td>\n",
              "      <td>-0.035873</td>\n",
              "      <td>0.396322</td>\n",
              "      <td>0.229427</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.388710</td>\n",
              "      <td>0.161868</td>\n",
              "      <td>0.052182</td>\n",
              "      <td>0.378511</td>\n",
              "      <td>0.085723</td>\n",
              "      <td>0.091270</td>\n",
              "      <td>0.480310</td>\n",
              "      <td>0.072318</td>\n",
              "      <td>0.071249</td>\n",
              "      <td>0.277920</td>\n",
              "      <td>0.448965</td>\n",
              "      <td>0.099246</td>\n",
              "      <td>0.215149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820451</td>\n",
              "      <td>-0.054481</td>\n",
              "      <td>0.485971</td>\n",
              "      <td>0.284793</td>\n",
              "      <td>-0.002835</td>\n",
              "      <td>0.426605</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>-0.060726</td>\n",
              "      <td>0.418768</td>\n",
              "      <td>0.843263</td>\n",
              "      <td>-0.056251</td>\n",
              "      <td>0.383449</td>\n",
              "      <td>0.718743</td>\n",
              "      <td>-0.044822</td>\n",
              "      <td>0.213713</td>\n",
              "      <td>0.614015</td>\n",
              "      <td>0.380890</td>\n",
              "      <td>0.485615</td>\n",
              "      <td>0.622031</td>\n",
              "      <td>-0.120505</td>\n",
              "      <td>0.436568</td>\n",
              "      <td>0.807114</td>\n",
              "      <td>-0.036760</td>\n",
              "      <td>0.426990</td>\n",
              "      <td>0.810267</td>\n",
              "      <td>-0.040235</td>\n",
              "      <td>0.456969</td>\n",
              "      <td>0.639337</td>\n",
              "      <td>-0.088925</td>\n",
              "      <td>0.418537</td>\n",
              "      <td>0.638220</td>\n",
              "      <td>-0.049196</td>\n",
              "      <td>0.452996</td>\n",
              "      <td>0.632927</td>\n",
              "      <td>-0.086052</td>\n",
              "      <td>0.482375</td>\n",
              "      <td>0.228888</td>\n",
              "      <td>0.002328</td>\n",
              "      <td>1471</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14729</th>\n",
              "      <td>0.527434</td>\n",
              "      <td>0.738273</td>\n",
              "      <td>-0.079975</td>\n",
              "      <td>0.328769</td>\n",
              "      <td>0.457341</td>\n",
              "      <td>-0.067171</td>\n",
              "      <td>0.263649</td>\n",
              "      <td>0.484076</td>\n",
              "      <td>-0.066484</td>\n",
              "      <td>0.365410</td>\n",
              "      <td>0.582288</td>\n",
              "      <td>-0.097822</td>\n",
              "      <td>0.136217</td>\n",
              "      <td>0.021140</td>\n",
              "      <td>-0.074558</td>\n",
              "      <td>0.175402</td>\n",
              "      <td>0.085695</td>\n",
              "      <td>-0.108161</td>\n",
              "      <td>0.210071</td>\n",
              "      <td>0.146272</td>\n",
              "      <td>-0.132795</td>\n",
              "      <td>0.343199</td>\n",
              "      <td>0.830920</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>0.406371</td>\n",
              "      <td>0.175288</td>\n",
              "      <td>-0.141420</td>\n",
              "      <td>0.396257</td>\n",
              "      <td>0.092930</td>\n",
              "      <td>-0.141256</td>\n",
              "      <td>0.377956</td>\n",
              "      <td>-0.004383</td>\n",
              "      <td>-0.133272</td>\n",
              "      <td>0.510257</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>-0.116076</td>\n",
              "      <td>0.188901</td>\n",
              "      <td>0.338566</td>\n",
              "      <td>-0.055147</td>\n",
              "      <td>0.084274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798353</td>\n",
              "      <td>-0.025366</td>\n",
              "      <td>0.508404</td>\n",
              "      <td>0.246948</td>\n",
              "      <td>-0.110513</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>0.809397</td>\n",
              "      <td>-0.028493</td>\n",
              "      <td>0.373280</td>\n",
              "      <td>0.819410</td>\n",
              "      <td>-0.022557</td>\n",
              "      <td>0.337109</td>\n",
              "      <td>0.707447</td>\n",
              "      <td>-0.056202</td>\n",
              "      <td>-0.054723</td>\n",
              "      <td>0.439372</td>\n",
              "      <td>0.278810</td>\n",
              "      <td>0.532050</td>\n",
              "      <td>0.647020</td>\n",
              "      <td>-0.122507</td>\n",
              "      <td>0.381318</td>\n",
              "      <td>0.785802</td>\n",
              "      <td>-0.011244</td>\n",
              "      <td>0.373739</td>\n",
              "      <td>0.790772</td>\n",
              "      <td>-0.014106</td>\n",
              "      <td>0.463954</td>\n",
              "      <td>0.650501</td>\n",
              "      <td>-0.093005</td>\n",
              "      <td>0.385598</td>\n",
              "      <td>0.631218</td>\n",
              "      <td>-0.068217</td>\n",
              "      <td>0.455586</td>\n",
              "      <td>0.642425</td>\n",
              "      <td>-0.095598</td>\n",
              "      <td>0.511577</td>\n",
              "      <td>0.192825</td>\n",
              "      <td>-0.125415</td>\n",
              "      <td>1472</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14730</th>\n",
              "      <td>0.528845</td>\n",
              "      <td>0.735577</td>\n",
              "      <td>-0.041305</td>\n",
              "      <td>0.342966</td>\n",
              "      <td>0.537490</td>\n",
              "      <td>-0.021240</td>\n",
              "      <td>0.291597</td>\n",
              "      <td>0.566791</td>\n",
              "      <td>-0.001343</td>\n",
              "      <td>0.376911</td>\n",
              "      <td>0.633230</td>\n",
              "      <td>-0.048700</td>\n",
              "      <td>0.118212</td>\n",
              "      <td>0.205293</td>\n",
              "      <td>-0.040242</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.265181</td>\n",
              "      <td>-0.071281</td>\n",
              "      <td>0.188816</td>\n",
              "      <td>0.329133</td>\n",
              "      <td>-0.094329</td>\n",
              "      <td>0.402845</td>\n",
              "      <td>0.801184</td>\n",
              "      <td>0.070091</td>\n",
              "      <td>0.351299</td>\n",
              "      <td>0.319882</td>\n",
              "      <td>-0.148940</td>\n",
              "      <td>0.321788</td>\n",
              "      <td>0.228375</td>\n",
              "      <td>-0.157065</td>\n",
              "      <td>0.290454</td>\n",
              "      <td>0.137655</td>\n",
              "      <td>-0.158898</td>\n",
              "      <td>0.406340</td>\n",
              "      <td>0.110980</td>\n",
              "      <td>-0.176145</td>\n",
              "      <td>0.219970</td>\n",
              "      <td>0.473707</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>0.150090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.768059</td>\n",
              "      <td>0.035476</td>\n",
              "      <td>0.450491</td>\n",
              "      <td>0.353091</td>\n",
              "      <td>-0.136861</td>\n",
              "      <td>0.419335</td>\n",
              "      <td>0.775359</td>\n",
              "      <td>0.032669</td>\n",
              "      <td>0.414294</td>\n",
              "      <td>0.783746</td>\n",
              "      <td>0.040324</td>\n",
              "      <td>0.367322</td>\n",
              "      <td>0.733595</td>\n",
              "      <td>0.017130</td>\n",
              "      <td>0.119499</td>\n",
              "      <td>0.563274</td>\n",
              "      <td>0.379588</td>\n",
              "      <td>0.510960</td>\n",
              "      <td>0.668780</td>\n",
              "      <td>-0.098167</td>\n",
              "      <td>0.413863</td>\n",
              "      <td>0.767714</td>\n",
              "      <td>0.057815</td>\n",
              "      <td>0.407349</td>\n",
              "      <td>0.770133</td>\n",
              "      <td>0.055871</td>\n",
              "      <td>0.462658</td>\n",
              "      <td>0.677776</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.400108</td>\n",
              "      <td>0.668224</td>\n",
              "      <td>-0.026347</td>\n",
              "      <td>0.454312</td>\n",
              "      <td>0.672526</td>\n",
              "      <td>-0.062448</td>\n",
              "      <td>0.442451</td>\n",
              "      <td>0.308193</td>\n",
              "      <td>-0.157336</td>\n",
              "      <td>1473</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14731</th>\n",
              "      <td>0.452533</td>\n",
              "      <td>0.791973</td>\n",
              "      <td>-0.099548</td>\n",
              "      <td>0.335543</td>\n",
              "      <td>0.535332</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.284853</td>\n",
              "      <td>0.564165</td>\n",
              "      <td>0.029366</td>\n",
              "      <td>0.351197</td>\n",
              "      <td>0.637508</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>0.184646</td>\n",
              "      <td>0.126609</td>\n",
              "      <td>0.138878</td>\n",
              "      <td>0.201092</td>\n",
              "      <td>0.182896</td>\n",
              "      <td>0.081410</td>\n",
              "      <td>0.219815</td>\n",
              "      <td>0.240867</td>\n",
              "      <td>0.035379</td>\n",
              "      <td>0.358867</td>\n",
              "      <td>0.892018</td>\n",
              "      <td>-0.009873</td>\n",
              "      <td>0.362378</td>\n",
              "      <td>0.247935</td>\n",
              "      <td>-0.036624</td>\n",
              "      <td>0.353144</td>\n",
              "      <td>0.162810</td>\n",
              "      <td>-0.012710</td>\n",
              "      <td>0.343602</td>\n",
              "      <td>0.071618</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.451449</td>\n",
              "      <td>0.063975</td>\n",
              "      <td>-0.008453</td>\n",
              "      <td>0.239168</td>\n",
              "      <td>0.439385</td>\n",
              "      <td>0.086585</td>\n",
              "      <td>0.170206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.848754</td>\n",
              "      <td>-0.034963</td>\n",
              "      <td>0.452250</td>\n",
              "      <td>0.307410</td>\n",
              "      <td>-0.048951</td>\n",
              "      <td>0.375028</td>\n",
              "      <td>0.858712</td>\n",
              "      <td>-0.041104</td>\n",
              "      <td>0.368408</td>\n",
              "      <td>0.870020</td>\n",
              "      <td>-0.035212</td>\n",
              "      <td>0.335122</td>\n",
              "      <td>0.770662</td>\n",
              "      <td>-0.023500</td>\n",
              "      <td>0.156868</td>\n",
              "      <td>0.588514</td>\n",
              "      <td>0.422205</td>\n",
              "      <td>0.446972</td>\n",
              "      <td>0.681988</td>\n",
              "      <td>-0.115878</td>\n",
              "      <td>0.378910</td>\n",
              "      <td>0.843526</td>\n",
              "      <td>-0.012812</td>\n",
              "      <td>0.371462</td>\n",
              "      <td>0.847001</td>\n",
              "      <td>-0.016520</td>\n",
              "      <td>0.412317</td>\n",
              "      <td>0.695334</td>\n",
              "      <td>-0.079299</td>\n",
              "      <td>0.370427</td>\n",
              "      <td>0.685399</td>\n",
              "      <td>-0.037217</td>\n",
              "      <td>0.407154</td>\n",
              "      <td>0.687007</td>\n",
              "      <td>-0.078226</td>\n",
              "      <td>0.449482</td>\n",
              "      <td>0.251166</td>\n",
              "      <td>-0.053193</td>\n",
              "      <td>1474</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14732 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.516601  0.585682 -0.128171  ...  0.006673           0        0\n",
              "1      0.350142  0.704083 -0.051000  ... -0.123009           1        0\n",
              "2      0.478097  0.829416 -0.035864  ... -0.201465           2        0\n",
              "3      0.440407  0.633268 -0.130409  ... -0.042549           3        0\n",
              "4      0.508843  0.783509 -0.075715  ... -0.125290           4        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "14727  0.560496  0.769144 -0.111749  ... -0.043033        1470        6\n",
              "14728  0.491013  0.712008 -0.122762  ...  0.002328        1471        6\n",
              "14729  0.527434  0.738273 -0.079975  ... -0.125415        1472        6\n",
              "14730  0.528845  0.735577 -0.041305  ... -0.157336        1473        6\n",
              "14731  0.452533  0.791973 -0.099548  ... -0.053193        1474        6\n",
              "\n",
              "[14732 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "SGSiuoOG1Peq",
        "outputId": "e5676657-cf8c-4248-e055-436f81eaf209"
      },
      "source": [
        "df_valid"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.646184</td>\n",
              "      <td>0.771045</td>\n",
              "      <td>-0.064509</td>\n",
              "      <td>0.479793</td>\n",
              "      <td>0.566351</td>\n",
              "      <td>-0.088064</td>\n",
              "      <td>0.426735</td>\n",
              "      <td>0.590875</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>0.520683</td>\n",
              "      <td>0.649389</td>\n",
              "      <td>-0.101141</td>\n",
              "      <td>0.297111</td>\n",
              "      <td>0.243108</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>0.351343</td>\n",
              "      <td>0.308081</td>\n",
              "      <td>-0.161240</td>\n",
              "      <td>0.396239</td>\n",
              "      <td>0.377762</td>\n",
              "      <td>-0.175256</td>\n",
              "      <td>0.463307</td>\n",
              "      <td>0.887630</td>\n",
              "      <td>-0.003552</td>\n",
              "      <td>0.560809</td>\n",
              "      <td>0.397581</td>\n",
              "      <td>-0.148304</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.302792</td>\n",
              "      <td>-0.148388</td>\n",
              "      <td>0.513932</td>\n",
              "      <td>0.218670</td>\n",
              "      <td>-0.145369</td>\n",
              "      <td>0.618115</td>\n",
              "      <td>0.221551</td>\n",
              "      <td>-0.109952</td>\n",
              "      <td>0.356944</td>\n",
              "      <td>0.489348</td>\n",
              "      <td>-0.107089</td>\n",
              "      <td>0.258764</td>\n",
              "      <td>...</td>\n",
              "      <td>0.861163</td>\n",
              "      <td>-0.021057</td>\n",
              "      <td>0.630754</td>\n",
              "      <td>0.437828</td>\n",
              "      <td>-0.104779</td>\n",
              "      <td>0.503365</td>\n",
              "      <td>0.870509</td>\n",
              "      <td>-0.021248</td>\n",
              "      <td>0.491106</td>\n",
              "      <td>0.878412</td>\n",
              "      <td>-0.018093</td>\n",
              "      <td>0.482451</td>\n",
              "      <td>0.760005</td>\n",
              "      <td>-0.072059</td>\n",
              "      <td>0.072792</td>\n",
              "      <td>0.583212</td>\n",
              "      <td>0.110986</td>\n",
              "      <td>0.669708</td>\n",
              "      <td>0.690351</td>\n",
              "      <td>-0.098678</td>\n",
              "      <td>0.497744</td>\n",
              "      <td>0.844740</td>\n",
              "      <td>-0.020899</td>\n",
              "      <td>0.490786</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>-0.021804</td>\n",
              "      <td>0.605350</td>\n",
              "      <td>0.697076</td>\n",
              "      <td>-0.082129</td>\n",
              "      <td>0.535840</td>\n",
              "      <td>0.685600</td>\n",
              "      <td>-0.071414</td>\n",
              "      <td>0.600393</td>\n",
              "      <td>0.690829</td>\n",
              "      <td>-0.085879</td>\n",
              "      <td>0.633086</td>\n",
              "      <td>0.403363</td>\n",
              "      <td>-0.117815</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.505669</td>\n",
              "      <td>0.783729</td>\n",
              "      <td>-0.018900</td>\n",
              "      <td>0.383348</td>\n",
              "      <td>0.551676</td>\n",
              "      <td>-0.021686</td>\n",
              "      <td>0.335575</td>\n",
              "      <td>0.573097</td>\n",
              "      <td>-0.002283</td>\n",
              "      <td>0.399952</td>\n",
              "      <td>0.650358</td>\n",
              "      <td>-0.031835</td>\n",
              "      <td>0.229131</td>\n",
              "      <td>0.181487</td>\n",
              "      <td>-0.092331</td>\n",
              "      <td>0.258003</td>\n",
              "      <td>0.257158</td>\n",
              "      <td>-0.111276</td>\n",
              "      <td>0.284237</td>\n",
              "      <td>0.338158</td>\n",
              "      <td>-0.123996</td>\n",
              "      <td>0.408235</td>\n",
              "      <td>0.862809</td>\n",
              "      <td>0.118722</td>\n",
              "      <td>0.414858</td>\n",
              "      <td>0.365437</td>\n",
              "      <td>-0.173962</td>\n",
              "      <td>0.397574</td>\n",
              "      <td>0.252688</td>\n",
              "      <td>-0.191834</td>\n",
              "      <td>0.380021</td>\n",
              "      <td>0.148873</td>\n",
              "      <td>-0.204391</td>\n",
              "      <td>0.480591</td>\n",
              "      <td>0.143603</td>\n",
              "      <td>-0.224993</td>\n",
              "      <td>0.294881</td>\n",
              "      <td>0.460291</td>\n",
              "      <td>-0.004822</td>\n",
              "      <td>0.217984</td>\n",
              "      <td>...</td>\n",
              "      <td>0.831755</td>\n",
              "      <td>0.075494</td>\n",
              "      <td>0.490290</td>\n",
              "      <td>0.410842</td>\n",
              "      <td>-0.158440</td>\n",
              "      <td>0.426757</td>\n",
              "      <td>0.840886</td>\n",
              "      <td>0.078719</td>\n",
              "      <td>0.419972</td>\n",
              "      <td>0.850027</td>\n",
              "      <td>0.088572</td>\n",
              "      <td>0.383190</td>\n",
              "      <td>0.750469</td>\n",
              "      <td>0.039978</td>\n",
              "      <td>0.169034</td>\n",
              "      <td>0.535456</td>\n",
              "      <td>0.333599</td>\n",
              "      <td>0.499905</td>\n",
              "      <td>0.714165</td>\n",
              "      <td>-0.075163</td>\n",
              "      <td>0.430074</td>\n",
              "      <td>0.816941</td>\n",
              "      <td>0.089510</td>\n",
              "      <td>0.422110</td>\n",
              "      <td>0.818853</td>\n",
              "      <td>0.090431</td>\n",
              "      <td>0.462675</td>\n",
              "      <td>0.711085</td>\n",
              "      <td>-0.036148</td>\n",
              "      <td>0.415931</td>\n",
              "      <td>0.687106</td>\n",
              "      <td>-0.008565</td>\n",
              "      <td>0.457287</td>\n",
              "      <td>0.704161</td>\n",
              "      <td>-0.042259</td>\n",
              "      <td>0.487358</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>-0.183734</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.570398</td>\n",
              "      <td>0.522947</td>\n",
              "      <td>-0.131259</td>\n",
              "      <td>0.385422</td>\n",
              "      <td>0.250885</td>\n",
              "      <td>-0.050247</td>\n",
              "      <td>0.313107</td>\n",
              "      <td>0.278246</td>\n",
              "      <td>-0.048979</td>\n",
              "      <td>0.409144</td>\n",
              "      <td>0.370157</td>\n",
              "      <td>-0.113957</td>\n",
              "      <td>0.206225</td>\n",
              "      <td>-0.195378</td>\n",
              "      <td>0.061084</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>-0.130779</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.276074</td>\n",
              "      <td>-0.063120</td>\n",
              "      <td>-0.046078</td>\n",
              "      <td>0.358516</td>\n",
              "      <td>0.730264</td>\n",
              "      <td>0.017391</td>\n",
              "      <td>0.468482</td>\n",
              "      <td>-0.048374</td>\n",
              "      <td>-0.084189</td>\n",
              "      <td>0.454622</td>\n",
              "      <td>-0.150500</td>\n",
              "      <td>-0.053712</td>\n",
              "      <td>0.437219</td>\n",
              "      <td>-0.252034</td>\n",
              "      <td>-0.025488</td>\n",
              "      <td>0.571085</td>\n",
              "      <td>-0.254265</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.253504</td>\n",
              "      <td>0.142910</td>\n",
              "      <td>0.007577</td>\n",
              "      <td>0.149256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.706014</td>\n",
              "      <td>-0.006454</td>\n",
              "      <td>0.569965</td>\n",
              "      <td>0.023101</td>\n",
              "      <td>-0.076176</td>\n",
              "      <td>0.390708</td>\n",
              "      <td>0.716519</td>\n",
              "      <td>-0.009776</td>\n",
              "      <td>0.376529</td>\n",
              "      <td>0.725704</td>\n",
              "      <td>-0.003093</td>\n",
              "      <td>0.352764</td>\n",
              "      <td>0.512117</td>\n",
              "      <td>-0.059380</td>\n",
              "      <td>0.041792</td>\n",
              "      <td>0.330219</td>\n",
              "      <td>0.345695</td>\n",
              "      <td>0.581562</td>\n",
              "      <td>0.438015</td>\n",
              "      <td>-0.170266</td>\n",
              "      <td>0.391589</td>\n",
              "      <td>0.667979</td>\n",
              "      <td>0.011425</td>\n",
              "      <td>0.375232</td>\n",
              "      <td>0.668794</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.507817</td>\n",
              "      <td>0.442177</td>\n",
              "      <td>-0.133319</td>\n",
              "      <td>0.426135</td>\n",
              "      <td>0.421102</td>\n",
              "      <td>-0.094905</td>\n",
              "      <td>0.501055</td>\n",
              "      <td>0.433366</td>\n",
              "      <td>-0.135095</td>\n",
              "      <td>0.571534</td>\n",
              "      <td>-0.036601</td>\n",
              "      <td>-0.077871</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.550640</td>\n",
              "      <td>0.693204</td>\n",
              "      <td>-0.039426</td>\n",
              "      <td>0.377895</td>\n",
              "      <td>0.420303</td>\n",
              "      <td>-0.049635</td>\n",
              "      <td>0.310557</td>\n",
              "      <td>0.444947</td>\n",
              "      <td>-0.030445</td>\n",
              "      <td>0.395424</td>\n",
              "      <td>0.535625</td>\n",
              "      <td>-0.071252</td>\n",
              "      <td>0.196327</td>\n",
              "      <td>0.037854</td>\n",
              "      <td>-0.122527</td>\n",
              "      <td>0.226640</td>\n",
              "      <td>0.102505</td>\n",
              "      <td>-0.149672</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.168133</td>\n",
              "      <td>-0.169691</td>\n",
              "      <td>0.385633</td>\n",
              "      <td>0.777333</td>\n",
              "      <td>0.130624</td>\n",
              "      <td>0.440921</td>\n",
              "      <td>0.182286</td>\n",
              "      <td>-0.220092</td>\n",
              "      <td>0.426143</td>\n",
              "      <td>0.093054</td>\n",
              "      <td>-0.238092</td>\n",
              "      <td>0.408764</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.251741</td>\n",
              "      <td>0.543859</td>\n",
              "      <td>-0.005355</td>\n",
              "      <td>-0.265706</td>\n",
              "      <td>0.259817</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>-0.029762</td>\n",
              "      <td>0.159518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.744009</td>\n",
              "      <td>0.082338</td>\n",
              "      <td>0.545075</td>\n",
              "      <td>0.237159</td>\n",
              "      <td>-0.195135</td>\n",
              "      <td>0.414130</td>\n",
              "      <td>0.752458</td>\n",
              "      <td>0.083467</td>\n",
              "      <td>0.404759</td>\n",
              "      <td>0.761788</td>\n",
              "      <td>0.094698</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>0.646520</td>\n",
              "      <td>0.029753</td>\n",
              "      <td>0.087381</td>\n",
              "      <td>0.425480</td>\n",
              "      <td>0.414335</td>\n",
              "      <td>0.550101</td>\n",
              "      <td>0.611782</td>\n",
              "      <td>-0.120470</td>\n",
              "      <td>0.411218</td>\n",
              "      <td>0.722901</td>\n",
              "      <td>0.103741</td>\n",
              "      <td>0.399978</td>\n",
              "      <td>0.723879</td>\n",
              "      <td>0.104076</td>\n",
              "      <td>0.488015</td>\n",
              "      <td>0.608906</td>\n",
              "      <td>-0.072477</td>\n",
              "      <td>0.414568</td>\n",
              "      <td>0.579445</td>\n",
              "      <td>-0.037483</td>\n",
              "      <td>0.479306</td>\n",
              "      <td>0.600204</td>\n",
              "      <td>-0.079331</td>\n",
              "      <td>0.544766</td>\n",
              "      <td>0.188114</td>\n",
              "      <td>-0.223748</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.524232</td>\n",
              "      <td>0.722622</td>\n",
              "      <td>-0.049771</td>\n",
              "      <td>0.378299</td>\n",
              "      <td>0.537701</td>\n",
              "      <td>-0.022929</td>\n",
              "      <td>0.332367</td>\n",
              "      <td>0.561998</td>\n",
              "      <td>-0.011429</td>\n",
              "      <td>0.401680</td>\n",
              "      <td>0.617495</td>\n",
              "      <td>-0.051351</td>\n",
              "      <td>0.205039</td>\n",
              "      <td>0.227481</td>\n",
              "      <td>-0.018862</td>\n",
              "      <td>0.235042</td>\n",
              "      <td>0.282814</td>\n",
              "      <td>-0.050678</td>\n",
              "      <td>0.265167</td>\n",
              "      <td>0.346141</td>\n",
              "      <td>-0.075391</td>\n",
              "      <td>0.425222</td>\n",
              "      <td>0.818088</td>\n",
              "      <td>0.049098</td>\n",
              "      <td>0.399753</td>\n",
              "      <td>0.352436</td>\n",
              "      <td>-0.117018</td>\n",
              "      <td>0.375707</td>\n",
              "      <td>0.255840</td>\n",
              "      <td>-0.116756</td>\n",
              "      <td>0.351857</td>\n",
              "      <td>0.170662</td>\n",
              "      <td>-0.111585</td>\n",
              "      <td>0.448775</td>\n",
              "      <td>0.154980</td>\n",
              "      <td>-0.122691</td>\n",
              "      <td>0.283837</td>\n",
              "      <td>0.472371</td>\n",
              "      <td>0.007503</td>\n",
              "      <td>0.215384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.779411</td>\n",
              "      <td>0.023573</td>\n",
              "      <td>0.478230</td>\n",
              "      <td>0.386732</td>\n",
              "      <td>-0.106638</td>\n",
              "      <td>0.441388</td>\n",
              "      <td>0.787471</td>\n",
              "      <td>0.022020</td>\n",
              "      <td>0.435998</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.027950</td>\n",
              "      <td>0.390656</td>\n",
              "      <td>0.715117</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.174290</td>\n",
              "      <td>0.598808</td>\n",
              "      <td>0.309484</td>\n",
              "      <td>0.511831</td>\n",
              "      <td>0.657377</td>\n",
              "      <td>-0.094378</td>\n",
              "      <td>0.437287</td>\n",
              "      <td>0.769950</td>\n",
              "      <td>0.040689</td>\n",
              "      <td>0.429929</td>\n",
              "      <td>0.771605</td>\n",
              "      <td>0.039537</td>\n",
              "      <td>0.471688</td>\n",
              "      <td>0.662292</td>\n",
              "      <td>-0.062175</td>\n",
              "      <td>0.420203</td>\n",
              "      <td>0.650105</td>\n",
              "      <td>-0.033778</td>\n",
              "      <td>0.465479</td>\n",
              "      <td>0.656399</td>\n",
              "      <td>-0.065202</td>\n",
              "      <td>0.473144</td>\n",
              "      <td>0.346925</td>\n",
              "      <td>-0.121656</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14631</th>\n",
              "      <td>0.529442</td>\n",
              "      <td>0.667210</td>\n",
              "      <td>-0.066573</td>\n",
              "      <td>0.386126</td>\n",
              "      <td>0.486287</td>\n",
              "      <td>-0.033634</td>\n",
              "      <td>0.341563</td>\n",
              "      <td>0.510485</td>\n",
              "      <td>-0.029726</td>\n",
              "      <td>0.418692</td>\n",
              "      <td>0.560215</td>\n",
              "      <td>-0.061615</td>\n",
              "      <td>0.219367</td>\n",
              "      <td>0.183714</td>\n",
              "      <td>-0.007092</td>\n",
              "      <td>0.248392</td>\n",
              "      <td>0.228448</td>\n",
              "      <td>-0.037222</td>\n",
              "      <td>0.276280</td>\n",
              "      <td>0.271904</td>\n",
              "      <td>-0.061118</td>\n",
              "      <td>0.423037</td>\n",
              "      <td>0.759102</td>\n",
              "      <td>-0.001626</td>\n",
              "      <td>0.413478</td>\n",
              "      <td>0.273705</td>\n",
              "      <td>-0.080519</td>\n",
              "      <td>0.398730</td>\n",
              "      <td>0.209388</td>\n",
              "      <td>-0.074626</td>\n",
              "      <td>0.379055</td>\n",
              "      <td>0.137959</td>\n",
              "      <td>-0.063146</td>\n",
              "      <td>0.475969</td>\n",
              "      <td>0.127867</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>0.284961</td>\n",
              "      <td>0.422919</td>\n",
              "      <td>-0.011928</td>\n",
              "      <td>0.216206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.732706</td>\n",
              "      <td>-0.021548</td>\n",
              "      <td>0.494808</td>\n",
              "      <td>0.313197</td>\n",
              "      <td>-0.065520</td>\n",
              "      <td>0.445838</td>\n",
              "      <td>0.741607</td>\n",
              "      <td>-0.024574</td>\n",
              "      <td>0.437814</td>\n",
              "      <td>0.749662</td>\n",
              "      <td>-0.020488</td>\n",
              "      <td>0.407561</td>\n",
              "      <td>0.660633</td>\n",
              "      <td>-0.038206</td>\n",
              "      <td>0.153465</td>\n",
              "      <td>0.517849</td>\n",
              "      <td>0.231584</td>\n",
              "      <td>0.524237</td>\n",
              "      <td>0.586032</td>\n",
              "      <td>-0.089157</td>\n",
              "      <td>0.447015</td>\n",
              "      <td>0.723673</td>\n",
              "      <td>-0.012398</td>\n",
              "      <td>0.440423</td>\n",
              "      <td>0.727365</td>\n",
              "      <td>-0.014400</td>\n",
              "      <td>0.484241</td>\n",
              "      <td>0.597897</td>\n",
              "      <td>-0.066358</td>\n",
              "      <td>0.436844</td>\n",
              "      <td>0.592494</td>\n",
              "      <td>-0.044895</td>\n",
              "      <td>0.479310</td>\n",
              "      <td>0.592026</td>\n",
              "      <td>-0.067038</td>\n",
              "      <td>0.491907</td>\n",
              "      <td>0.271605</td>\n",
              "      <td>-0.073554</td>\n",
              "      <td>1469</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14632</th>\n",
              "      <td>0.500539</td>\n",
              "      <td>0.734860</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>0.327387</td>\n",
              "      <td>0.516931</td>\n",
              "      <td>-0.010588</td>\n",
              "      <td>0.265571</td>\n",
              "      <td>0.545812</td>\n",
              "      <td>-0.007016</td>\n",
              "      <td>0.361814</td>\n",
              "      <td>0.604152</td>\n",
              "      <td>-0.083395</td>\n",
              "      <td>0.160265</td>\n",
              "      <td>0.151897</td>\n",
              "      <td>0.173618</td>\n",
              "      <td>0.184835</td>\n",
              "      <td>0.188428</td>\n",
              "      <td>0.104391</td>\n",
              "      <td>0.210830</td>\n",
              "      <td>0.227435</td>\n",
              "      <td>0.047180</td>\n",
              "      <td>0.350158</td>\n",
              "      <td>0.870057</td>\n",
              "      <td>-0.090012</td>\n",
              "      <td>0.386060</td>\n",
              "      <td>0.230485</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.377281</td>\n",
              "      <td>0.166759</td>\n",
              "      <td>0.043994</td>\n",
              "      <td>0.364579</td>\n",
              "      <td>0.095391</td>\n",
              "      <td>0.089954</td>\n",
              "      <td>0.487561</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.089601</td>\n",
              "      <td>0.203078</td>\n",
              "      <td>0.435127</td>\n",
              "      <td>0.063342</td>\n",
              "      <td>0.114195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820860</td>\n",
              "      <td>-0.104007</td>\n",
              "      <td>0.487804</td>\n",
              "      <td>0.291259</td>\n",
              "      <td>-0.005642</td>\n",
              "      <td>0.382895</td>\n",
              "      <td>0.832251</td>\n",
              "      <td>-0.113706</td>\n",
              "      <td>0.372782</td>\n",
              "      <td>0.845191</td>\n",
              "      <td>-0.110114</td>\n",
              "      <td>0.337598</td>\n",
              "      <td>0.735444</td>\n",
              "      <td>-0.089631</td>\n",
              "      <td>0.034739</td>\n",
              "      <td>0.615029</td>\n",
              "      <td>0.373997</td>\n",
              "      <td>0.498705</td>\n",
              "      <td>0.626003</td>\n",
              "      <td>-0.145978</td>\n",
              "      <td>0.384501</td>\n",
              "      <td>0.817192</td>\n",
              "      <td>-0.083325</td>\n",
              "      <td>0.376356</td>\n",
              "      <td>0.821481</td>\n",
              "      <td>-0.089008</td>\n",
              "      <td>0.447718</td>\n",
              "      <td>0.647504</td>\n",
              "      <td>-0.116903</td>\n",
              "      <td>0.385622</td>\n",
              "      <td>0.649125</td>\n",
              "      <td>-0.076227</td>\n",
              "      <td>0.441510</td>\n",
              "      <td>0.639893</td>\n",
              "      <td>-0.113286</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.238567</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>1470</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14633</th>\n",
              "      <td>0.494283</td>\n",
              "      <td>0.710024</td>\n",
              "      <td>-0.116081</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.497803</td>\n",
              "      <td>-0.009445</td>\n",
              "      <td>0.295208</td>\n",
              "      <td>0.520917</td>\n",
              "      <td>-0.001318</td>\n",
              "      <td>0.381529</td>\n",
              "      <td>0.580622</td>\n",
              "      <td>-0.068246</td>\n",
              "      <td>0.204759</td>\n",
              "      <td>0.123697</td>\n",
              "      <td>0.127479</td>\n",
              "      <td>0.225363</td>\n",
              "      <td>0.164128</td>\n",
              "      <td>0.072507</td>\n",
              "      <td>0.249508</td>\n",
              "      <td>0.206157</td>\n",
              "      <td>0.027697</td>\n",
              "      <td>0.370819</td>\n",
              "      <td>0.833998</td>\n",
              "      <td>-0.037612</td>\n",
              "      <td>0.400362</td>\n",
              "      <td>0.218823</td>\n",
              "      <td>-0.020845</td>\n",
              "      <td>0.388722</td>\n",
              "      <td>0.150536</td>\n",
              "      <td>0.007938</td>\n",
              "      <td>0.374952</td>\n",
              "      <td>0.073443</td>\n",
              "      <td>0.040432</td>\n",
              "      <td>0.483528</td>\n",
              "      <td>0.071126</td>\n",
              "      <td>0.034102</td>\n",
              "      <td>0.244181</td>\n",
              "      <td>0.414586</td>\n",
              "      <td>0.053564</td>\n",
              "      <td>0.170244</td>\n",
              "      <td>...</td>\n",
              "      <td>0.797283</td>\n",
              "      <td>-0.057481</td>\n",
              "      <td>0.491774</td>\n",
              "      <td>0.282192</td>\n",
              "      <td>-0.024813</td>\n",
              "      <td>0.390480</td>\n",
              "      <td>0.806401</td>\n",
              "      <td>-0.063569</td>\n",
              "      <td>0.381867</td>\n",
              "      <td>0.816910</td>\n",
              "      <td>-0.058509</td>\n",
              "      <td>0.356374</td>\n",
              "      <td>0.697895</td>\n",
              "      <td>-0.055486</td>\n",
              "      <td>0.130981</td>\n",
              "      <td>0.558962</td>\n",
              "      <td>0.341036</td>\n",
              "      <td>0.493862</td>\n",
              "      <td>0.611363</td>\n",
              "      <td>-0.122746</td>\n",
              "      <td>0.395654</td>\n",
              "      <td>0.785397</td>\n",
              "      <td>-0.038678</td>\n",
              "      <td>0.386269</td>\n",
              "      <td>0.787321</td>\n",
              "      <td>-0.042230</td>\n",
              "      <td>0.452422</td>\n",
              "      <td>0.626979</td>\n",
              "      <td>-0.094577</td>\n",
              "      <td>0.401670</td>\n",
              "      <td>0.622043</td>\n",
              "      <td>-0.060001</td>\n",
              "      <td>0.447939</td>\n",
              "      <td>0.619601</td>\n",
              "      <td>-0.092344</td>\n",
              "      <td>0.490952</td>\n",
              "      <td>0.228598</td>\n",
              "      <td>-0.023898</td>\n",
              "      <td>1471</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14634</th>\n",
              "      <td>0.480344</td>\n",
              "      <td>0.614472</td>\n",
              "      <td>-0.073619</td>\n",
              "      <td>0.351348</td>\n",
              "      <td>0.359926</td>\n",
              "      <td>-0.029388</td>\n",
              "      <td>0.296622</td>\n",
              "      <td>0.382071</td>\n",
              "      <td>-0.013036</td>\n",
              "      <td>0.371280</td>\n",
              "      <td>0.466668</td>\n",
              "      <td>-0.062456</td>\n",
              "      <td>0.200838</td>\n",
              "      <td>-0.066494</td>\n",
              "      <td>-0.011770</td>\n",
              "      <td>0.220040</td>\n",
              "      <td>-0.001829</td>\n",
              "      <td>-0.048701</td>\n",
              "      <td>0.240770</td>\n",
              "      <td>0.061476</td>\n",
              "      <td>-0.075901</td>\n",
              "      <td>0.363895</td>\n",
              "      <td>0.734271</td>\n",
              "      <td>0.071702</td>\n",
              "      <td>0.399486</td>\n",
              "      <td>0.096403</td>\n",
              "      <td>-0.128011</td>\n",
              "      <td>0.393551</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.130443</td>\n",
              "      <td>0.384674</td>\n",
              "      <td>-0.095847</td>\n",
              "      <td>-0.123506</td>\n",
              "      <td>0.504907</td>\n",
              "      <td>-0.092631</td>\n",
              "      <td>-0.136478</td>\n",
              "      <td>0.245395</td>\n",
              "      <td>0.251886</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.169527</td>\n",
              "      <td>...</td>\n",
              "      <td>0.721825</td>\n",
              "      <td>0.031216</td>\n",
              "      <td>0.494581</td>\n",
              "      <td>0.161664</td>\n",
              "      <td>-0.117363</td>\n",
              "      <td>0.387729</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.031917</td>\n",
              "      <td>0.376631</td>\n",
              "      <td>0.738281</td>\n",
              "      <td>0.040350</td>\n",
              "      <td>0.353397</td>\n",
              "      <td>0.587014</td>\n",
              "      <td>-0.009165</td>\n",
              "      <td>0.138784</td>\n",
              "      <td>0.330477</td>\n",
              "      <td>0.364592</td>\n",
              "      <td>0.478526</td>\n",
              "      <td>0.523328</td>\n",
              "      <td>-0.111924</td>\n",
              "      <td>0.401928</td>\n",
              "      <td>0.697973</td>\n",
              "      <td>0.040930</td>\n",
              "      <td>0.390769</td>\n",
              "      <td>0.701493</td>\n",
              "      <td>0.040458</td>\n",
              "      <td>0.437818</td>\n",
              "      <td>0.528118</td>\n",
              "      <td>-0.072447</td>\n",
              "      <td>0.389704</td>\n",
              "      <td>0.510692</td>\n",
              "      <td>-0.040840</td>\n",
              "      <td>0.432847</td>\n",
              "      <td>0.520244</td>\n",
              "      <td>-0.076076</td>\n",
              "      <td>0.496153</td>\n",
              "      <td>0.107584</td>\n",
              "      <td>-0.134429</td>\n",
              "      <td>1472</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>0.451848</td>\n",
              "      <td>0.622114</td>\n",
              "      <td>-0.181673</td>\n",
              "      <td>0.305574</td>\n",
              "      <td>0.401656</td>\n",
              "      <td>-0.015953</td>\n",
              "      <td>0.233015</td>\n",
              "      <td>0.421863</td>\n",
              "      <td>-0.015226</td>\n",
              "      <td>0.335668</td>\n",
              "      <td>0.477531</td>\n",
              "      <td>-0.099770</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>0.018560</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.221613</td>\n",
              "      <td>0.058618</td>\n",
              "      <td>0.131506</td>\n",
              "      <td>0.242395</td>\n",
              "      <td>0.106519</td>\n",
              "      <td>0.064972</td>\n",
              "      <td>0.257232</td>\n",
              "      <td>0.817205</td>\n",
              "      <td>-0.085517</td>\n",
              "      <td>0.429682</td>\n",
              "      <td>0.147278</td>\n",
              "      <td>0.012527</td>\n",
              "      <td>0.433578</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.067867</td>\n",
              "      <td>0.434073</td>\n",
              "      <td>-0.013070</td>\n",
              "      <td>0.124950</td>\n",
              "      <td>0.568935</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.123650</td>\n",
              "      <td>0.182558</td>\n",
              "      <td>0.309983</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>0.081712</td>\n",
              "      <td>...</td>\n",
              "      <td>0.787673</td>\n",
              "      <td>-0.104376</td>\n",
              "      <td>0.523025</td>\n",
              "      <td>0.221324</td>\n",
              "      <td>0.002962</td>\n",
              "      <td>0.306585</td>\n",
              "      <td>0.796358</td>\n",
              "      <td>-0.111493</td>\n",
              "      <td>0.289647</td>\n",
              "      <td>0.805510</td>\n",
              "      <td>-0.107110</td>\n",
              "      <td>0.283806</td>\n",
              "      <td>0.617631</td>\n",
              "      <td>-0.111347</td>\n",
              "      <td>-0.033763</td>\n",
              "      <td>0.493091</td>\n",
              "      <td>0.385915</td>\n",
              "      <td>0.465378</td>\n",
              "      <td>0.510059</td>\n",
              "      <td>-0.169199</td>\n",
              "      <td>0.322039</td>\n",
              "      <td>0.765549</td>\n",
              "      <td>-0.091213</td>\n",
              "      <td>0.309973</td>\n",
              "      <td>0.766603</td>\n",
              "      <td>-0.096392</td>\n",
              "      <td>0.413244</td>\n",
              "      <td>0.528470</td>\n",
              "      <td>-0.136841</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>0.525035</td>\n",
              "      <td>-0.093184</td>\n",
              "      <td>0.409769</td>\n",
              "      <td>0.520118</td>\n",
              "      <td>-0.133804</td>\n",
              "      <td>0.533610</td>\n",
              "      <td>0.171482</td>\n",
              "      <td>0.013007</td>\n",
              "      <td>1473</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14636 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.646184  0.771045 -0.064509  ... -0.117815           0        0\n",
              "1      0.505669  0.783729 -0.018900  ... -0.183734           1        0\n",
              "2      0.570398  0.522947 -0.131259  ... -0.077871           3        0\n",
              "3      0.550640  0.693204 -0.039426  ... -0.223748           4        0\n",
              "4      0.524232  0.722622 -0.049771  ... -0.121656           5        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "14631  0.529442  0.667210 -0.066573  ... -0.073554        1469        6\n",
              "14632  0.500539  0.734860 -0.148886  ...  0.000434        1470        6\n",
              "14633  0.494283  0.710024 -0.116081  ... -0.023898        1471        6\n",
              "14634  0.480344  0.614472 -0.073619  ... -0.134429        1472        6\n",
              "14635  0.451848  0.622114 -0.181673  ...  0.013007        1473        6\n",
              "\n",
              "[14636 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT4rg5Ugl6ll"
      },
      "source": [
        "Unnameはいらないので削除"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ZvzA8Gy1EUhN",
        "outputId": "102ce974-0b90-4bc3-9b37-c597fb8a011d"
      },
      "source": [
        "df_train.drop(columns=df_train.columns[[-2]],axis=1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405580</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.362563</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.629462</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.406199</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.077165</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.366158</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.392929</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.429142</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.469884</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405460</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1405 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_y       9_z  correct\n",
              "0      0.470799  0.682127 -0.080930  ...  0.269363 -0.045183        0\n",
              "1      0.469567  0.799870 -0.017644  ...  0.319253 -0.206077        0\n",
              "2      0.711111  0.717369 -0.049849  ...  0.328598 -0.146556        0\n",
              "3      0.483527  0.809326 -0.011760  ...  0.411362 -0.195280        0\n",
              "4      0.061688  0.692855 -0.050553  ...  0.306380 -0.058228        0\n",
              "...         ...       ...       ...  ...       ...       ...      ...\n",
              "82190  0.436367  0.787778 -0.124726  ...  0.236569 -0.051585        6\n",
              "82191  0.456891  0.704518 -0.093469  ...  0.282853 -0.061855        6\n",
              "82192  0.502264  0.729350 -0.124422  ...  0.268140 -0.034920        6\n",
              "82193  0.539922  0.733853 -0.116346  ...  0.276309 -0.023272        6\n",
              "82194  0.508446  0.805487 -0.070251  ...  0.245632 -0.153780        6\n",
              "\n",
              "[82195 rows x 1405 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OqO9I_q6S1z"
      },
      "source": [
        "967,5,17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "hrF0BgW-1UTy",
        "outputId": "bf5370c0-5eab-489c-e878-1d004d34d96b"
      },
      "source": [
        "df_test.drop(columns=df_test.columns[[-2]],axis=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.516601</td>\n",
              "      <td>0.585682</td>\n",
              "      <td>-0.128171</td>\n",
              "      <td>0.354741</td>\n",
              "      <td>0.407085</td>\n",
              "      <td>-0.017539</td>\n",
              "      <td>0.303413</td>\n",
              "      <td>0.437223</td>\n",
              "      <td>-0.022323</td>\n",
              "      <td>0.385631</td>\n",
              "      <td>0.474988</td>\n",
              "      <td>-0.077427</td>\n",
              "      <td>0.192280</td>\n",
              "      <td>0.117430</td>\n",
              "      <td>0.133606</td>\n",
              "      <td>0.227670</td>\n",
              "      <td>0.153081</td>\n",
              "      <td>0.075319</td>\n",
              "      <td>0.261869</td>\n",
              "      <td>0.196746</td>\n",
              "      <td>0.025099</td>\n",
              "      <td>0.384131</td>\n",
              "      <td>0.734853</td>\n",
              "      <td>-0.092528</td>\n",
              "      <td>0.409821</td>\n",
              "      <td>0.185824</td>\n",
              "      <td>-0.000665</td>\n",
              "      <td>0.388524</td>\n",
              "      <td>0.114111</td>\n",
              "      <td>0.043742</td>\n",
              "      <td>0.366814</td>\n",
              "      <td>0.051379</td>\n",
              "      <td>0.084901</td>\n",
              "      <td>0.466509</td>\n",
              "      <td>0.041154</td>\n",
              "      <td>0.091456</td>\n",
              "      <td>0.247168</td>\n",
              "      <td>0.348231</td>\n",
              "      <td>0.038491</td>\n",
              "      <td>0.162550</td>\n",
              "      <td>...</td>\n",
              "      <td>0.419576</td>\n",
              "      <td>0.673695</td>\n",
              "      <td>-0.099631</td>\n",
              "      <td>0.486085</td>\n",
              "      <td>0.227604</td>\n",
              "      <td>-0.000800</td>\n",
              "      <td>0.411432</td>\n",
              "      <td>0.685160</td>\n",
              "      <td>-0.107264</td>\n",
              "      <td>0.403327</td>\n",
              "      <td>0.699680</td>\n",
              "      <td>-0.105119</td>\n",
              "      <td>0.365605</td>\n",
              "      <td>0.599879</td>\n",
              "      <td>-0.087972</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>0.566646</td>\n",
              "      <td>0.268614</td>\n",
              "      <td>0.517140</td>\n",
              "      <td>0.487549</td>\n",
              "      <td>-0.123952</td>\n",
              "      <td>0.407326</td>\n",
              "      <td>0.672411</td>\n",
              "      <td>-0.080824</td>\n",
              "      <td>0.400786</td>\n",
              "      <td>0.675086</td>\n",
              "      <td>-0.085923</td>\n",
              "      <td>0.467047</td>\n",
              "      <td>0.510292</td>\n",
              "      <td>-0.103923</td>\n",
              "      <td>0.406921</td>\n",
              "      <td>0.514333</td>\n",
              "      <td>-0.071427</td>\n",
              "      <td>0.460763</td>\n",
              "      <td>0.503372</td>\n",
              "      <td>-0.100683</td>\n",
              "      <td>0.483841</td>\n",
              "      <td>0.187230</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.350142</td>\n",
              "      <td>0.704083</td>\n",
              "      <td>-0.051000</td>\n",
              "      <td>0.258494</td>\n",
              "      <td>0.467294</td>\n",
              "      <td>0.006417</td>\n",
              "      <td>0.210762</td>\n",
              "      <td>0.484544</td>\n",
              "      <td>0.028842</td>\n",
              "      <td>0.256672</td>\n",
              "      <td>0.560486</td>\n",
              "      <td>-0.029805</td>\n",
              "      <td>0.163514</td>\n",
              "      <td>0.115912</td>\n",
              "      <td>0.038936</td>\n",
              "      <td>0.169306</td>\n",
              "      <td>0.169693</td>\n",
              "      <td>-0.000869</td>\n",
              "      <td>0.178214</td>\n",
              "      <td>0.229622</td>\n",
              "      <td>-0.030477</td>\n",
              "      <td>0.262613</td>\n",
              "      <td>0.771681</td>\n",
              "      <td>0.078652</td>\n",
              "      <td>0.294888</td>\n",
              "      <td>0.263658</td>\n",
              "      <td>-0.103125</td>\n",
              "      <td>0.291660</td>\n",
              "      <td>0.172433</td>\n",
              "      <td>-0.100819</td>\n",
              "      <td>0.290796</td>\n",
              "      <td>0.086691</td>\n",
              "      <td>-0.094484</td>\n",
              "      <td>0.382622</td>\n",
              "      <td>0.088537</td>\n",
              "      <td>-0.127045</td>\n",
              "      <td>0.190766</td>\n",
              "      <td>0.370796</td>\n",
              "      <td>0.059996</td>\n",
              "      <td>0.127881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.286113</td>\n",
              "      <td>0.731658</td>\n",
              "      <td>0.042865</td>\n",
              "      <td>0.366543</td>\n",
              "      <td>0.318007</td>\n",
              "      <td>-0.107573</td>\n",
              "      <td>0.279443</td>\n",
              "      <td>0.738324</td>\n",
              "      <td>0.041889</td>\n",
              "      <td>0.274230</td>\n",
              "      <td>0.747920</td>\n",
              "      <td>0.050578</td>\n",
              "      <td>0.236568</td>\n",
              "      <td>0.660978</td>\n",
              "      <td>0.032212</td>\n",
              "      <td>0.120189</td>\n",
              "      <td>0.495042</td>\n",
              "      <td>0.403236</td>\n",
              "      <td>0.337132</td>\n",
              "      <td>0.624860</td>\n",
              "      <td>-0.094728</td>\n",
              "      <td>0.281208</td>\n",
              "      <td>0.722307</td>\n",
              "      <td>0.068394</td>\n",
              "      <td>0.274025</td>\n",
              "      <td>0.721999</td>\n",
              "      <td>0.067390</td>\n",
              "      <td>0.308012</td>\n",
              "      <td>0.624537</td>\n",
              "      <td>-0.054114</td>\n",
              "      <td>0.268657</td>\n",
              "      <td>0.601183</td>\n",
              "      <td>-0.016952</td>\n",
              "      <td>0.302620</td>\n",
              "      <td>0.616359</td>\n",
              "      <td>-0.057269</td>\n",
              "      <td>0.366551</td>\n",
              "      <td>0.274136</td>\n",
              "      <td>-0.123009</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.478097</td>\n",
              "      <td>0.829416</td>\n",
              "      <td>-0.035864</td>\n",
              "      <td>0.338219</td>\n",
              "      <td>0.586676</td>\n",
              "      <td>-0.044701</td>\n",
              "      <td>0.281508</td>\n",
              "      <td>0.608822</td>\n",
              "      <td>-0.030072</td>\n",
              "      <td>0.362336</td>\n",
              "      <td>0.690208</td>\n",
              "      <td>-0.055267</td>\n",
              "      <td>0.158889</td>\n",
              "      <td>0.208705</td>\n",
              "      <td>-0.127339</td>\n",
              "      <td>0.195300</td>\n",
              "      <td>0.286799</td>\n",
              "      <td>-0.150213</td>\n",
              "      <td>0.229269</td>\n",
              "      <td>0.368802</td>\n",
              "      <td>-0.164157</td>\n",
              "      <td>0.357517</td>\n",
              "      <td>0.888281</td>\n",
              "      <td>0.091781</td>\n",
              "      <td>0.391913</td>\n",
              "      <td>0.394345</td>\n",
              "      <td>-0.200909</td>\n",
              "      <td>0.374054</td>\n",
              "      <td>0.287385</td>\n",
              "      <td>-0.220581</td>\n",
              "      <td>0.355048</td>\n",
              "      <td>0.184166</td>\n",
              "      <td>-0.231421</td>\n",
              "      <td>0.476613</td>\n",
              "      <td>0.184120</td>\n",
              "      <td>-0.238496</td>\n",
              "      <td>0.228681</td>\n",
              "      <td>0.488865</td>\n",
              "      <td>-0.035819</td>\n",
              "      <td>0.134504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.394229</td>\n",
              "      <td>0.857773</td>\n",
              "      <td>0.046278</td>\n",
              "      <td>0.478034</td>\n",
              "      <td>0.444558</td>\n",
              "      <td>-0.172615</td>\n",
              "      <td>0.386257</td>\n",
              "      <td>0.865752</td>\n",
              "      <td>0.048123</td>\n",
              "      <td>0.378545</td>\n",
              "      <td>0.874142</td>\n",
              "      <td>0.057897</td>\n",
              "      <td>0.340892</td>\n",
              "      <td>0.793487</td>\n",
              "      <td>0.010045</td>\n",
              "      <td>0.054897</td>\n",
              "      <td>0.550742</td>\n",
              "      <td>0.351424</td>\n",
              "      <td>0.478415</td>\n",
              "      <td>0.751673</td>\n",
              "      <td>-0.092016</td>\n",
              "      <td>0.385269</td>\n",
              "      <td>0.849144</td>\n",
              "      <td>0.059494</td>\n",
              "      <td>0.379350</td>\n",
              "      <td>0.852295</td>\n",
              "      <td>0.059011</td>\n",
              "      <td>0.434535</td>\n",
              "      <td>0.750683</td>\n",
              "      <td>-0.052339</td>\n",
              "      <td>0.380534</td>\n",
              "      <td>0.728616</td>\n",
              "      <td>-0.024325</td>\n",
              "      <td>0.428548</td>\n",
              "      <td>0.743129</td>\n",
              "      <td>-0.058275</td>\n",
              "      <td>0.478435</td>\n",
              "      <td>0.401297</td>\n",
              "      <td>-0.201465</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.440407</td>\n",
              "      <td>0.633268</td>\n",
              "      <td>-0.130409</td>\n",
              "      <td>0.313094</td>\n",
              "      <td>0.456206</td>\n",
              "      <td>0.001129</td>\n",
              "      <td>0.255751</td>\n",
              "      <td>0.477937</td>\n",
              "      <td>0.012412</td>\n",
              "      <td>0.327564</td>\n",
              "      <td>0.527424</td>\n",
              "      <td>-0.067629</td>\n",
              "      <td>0.187209</td>\n",
              "      <td>0.126621</td>\n",
              "      <td>0.163992</td>\n",
              "      <td>0.209704</td>\n",
              "      <td>0.178016</td>\n",
              "      <td>0.096623</td>\n",
              "      <td>0.230991</td>\n",
              "      <td>0.237480</td>\n",
              "      <td>0.042394</td>\n",
              "      <td>0.312069</td>\n",
              "      <td>0.801668</td>\n",
              "      <td>0.008883</td>\n",
              "      <td>0.372335</td>\n",
              "      <td>0.248090</td>\n",
              "      <td>-0.028703</td>\n",
              "      <td>0.360934</td>\n",
              "      <td>0.151648</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.349558</td>\n",
              "      <td>0.068262</td>\n",
              "      <td>0.044676</td>\n",
              "      <td>0.458189</td>\n",
              "      <td>0.061373</td>\n",
              "      <td>0.022868</td>\n",
              "      <td>0.221603</td>\n",
              "      <td>0.385956</td>\n",
              "      <td>0.085039</td>\n",
              "      <td>0.143525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.344682</td>\n",
              "      <td>0.774976</td>\n",
              "      <td>-0.017430</td>\n",
              "      <td>0.452601</td>\n",
              "      <td>0.292870</td>\n",
              "      <td>-0.042988</td>\n",
              "      <td>0.331663</td>\n",
              "      <td>0.783315</td>\n",
              "      <td>-0.020361</td>\n",
              "      <td>0.322092</td>\n",
              "      <td>0.792106</td>\n",
              "      <td>-0.013262</td>\n",
              "      <td>0.298874</td>\n",
              "      <td>0.636628</td>\n",
              "      <td>-0.034253</td>\n",
              "      <td>0.112172</td>\n",
              "      <td>0.549183</td>\n",
              "      <td>0.409472</td>\n",
              "      <td>0.433293</td>\n",
              "      <td>0.558691</td>\n",
              "      <td>-0.145084</td>\n",
              "      <td>0.341583</td>\n",
              "      <td>0.752286</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.752438</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.395023</td>\n",
              "      <td>0.569501</td>\n",
              "      <td>-0.106214</td>\n",
              "      <td>0.345461</td>\n",
              "      <td>0.563145</td>\n",
              "      <td>-0.061551</td>\n",
              "      <td>0.390786</td>\n",
              "      <td>0.563328</td>\n",
              "      <td>-0.106485</td>\n",
              "      <td>0.452236</td>\n",
              "      <td>0.249019</td>\n",
              "      <td>-0.042549</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.508843</td>\n",
              "      <td>0.783509</td>\n",
              "      <td>-0.075715</td>\n",
              "      <td>0.342278</td>\n",
              "      <td>0.582771</td>\n",
              "      <td>-0.031950</td>\n",
              "      <td>0.289867</td>\n",
              "      <td>0.611026</td>\n",
              "      <td>-0.020946</td>\n",
              "      <td>0.375631</td>\n",
              "      <td>0.676300</td>\n",
              "      <td>-0.067914</td>\n",
              "      <td>0.148420</td>\n",
              "      <td>0.218699</td>\n",
              "      <td>-0.009825</td>\n",
              "      <td>0.183308</td>\n",
              "      <td>0.281258</td>\n",
              "      <td>-0.048717</td>\n",
              "      <td>0.216023</td>\n",
              "      <td>0.349559</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>0.396198</td>\n",
              "      <td>0.916336</td>\n",
              "      <td>0.045910</td>\n",
              "      <td>0.370082</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>-0.122842</td>\n",
              "      <td>0.345563</td>\n",
              "      <td>0.249492</td>\n",
              "      <td>-0.119133</td>\n",
              "      <td>0.317737</td>\n",
              "      <td>0.152755</td>\n",
              "      <td>-0.109723</td>\n",
              "      <td>0.428981</td>\n",
              "      <td>0.135849</td>\n",
              "      <td>-0.117880</td>\n",
              "      <td>0.226637</td>\n",
              "      <td>0.504523</td>\n",
              "      <td>0.004935</td>\n",
              "      <td>0.154398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.424438</td>\n",
              "      <td>0.875826</td>\n",
              "      <td>0.011859</td>\n",
              "      <td>0.461890</td>\n",
              "      <td>0.394244</td>\n",
              "      <td>-0.109819</td>\n",
              "      <td>0.415073</td>\n",
              "      <td>0.887762</td>\n",
              "      <td>0.011303</td>\n",
              "      <td>0.407975</td>\n",
              "      <td>0.899817</td>\n",
              "      <td>0.019094</td>\n",
              "      <td>0.362383</td>\n",
              "      <td>0.781520</td>\n",
              "      <td>-0.017575</td>\n",
              "      <td>0.112559</td>\n",
              "      <td>0.630753</td>\n",
              "      <td>0.341884</td>\n",
              "      <td>0.502403</td>\n",
              "      <td>0.719198</td>\n",
              "      <td>-0.114545</td>\n",
              "      <td>0.415689</td>\n",
              "      <td>0.858130</td>\n",
              "      <td>0.028379</td>\n",
              "      <td>0.405933</td>\n",
              "      <td>0.861435</td>\n",
              "      <td>0.026839</td>\n",
              "      <td>0.456500</td>\n",
              "      <td>0.724646</td>\n",
              "      <td>-0.078316</td>\n",
              "      <td>0.398571</td>\n",
              "      <td>0.712659</td>\n",
              "      <td>-0.046950</td>\n",
              "      <td>0.450282</td>\n",
              "      <td>0.718590</td>\n",
              "      <td>-0.081530</td>\n",
              "      <td>0.455816</td>\n",
              "      <td>0.346828</td>\n",
              "      <td>-0.125290</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14727</th>\n",
              "      <td>0.560496</td>\n",
              "      <td>0.769144</td>\n",
              "      <td>-0.111749</td>\n",
              "      <td>0.395681</td>\n",
              "      <td>0.565257</td>\n",
              "      <td>-0.017007</td>\n",
              "      <td>0.342978</td>\n",
              "      <td>0.594650</td>\n",
              "      <td>-0.008701</td>\n",
              "      <td>0.431437</td>\n",
              "      <td>0.644474</td>\n",
              "      <td>-0.072280</td>\n",
              "      <td>0.212711</td>\n",
              "      <td>0.199529</td>\n",
              "      <td>0.095772</td>\n",
              "      <td>0.235907</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>0.043887</td>\n",
              "      <td>0.261247</td>\n",
              "      <td>0.286265</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>0.444982</td>\n",
              "      <td>0.899928</td>\n",
              "      <td>-0.032473</td>\n",
              "      <td>0.421535</td>\n",
              "      <td>0.284107</td>\n",
              "      <td>-0.043228</td>\n",
              "      <td>0.409811</td>\n",
              "      <td>0.212278</td>\n",
              "      <td>-0.018695</td>\n",
              "      <td>0.392661</td>\n",
              "      <td>0.132065</td>\n",
              "      <td>0.010029</td>\n",
              "      <td>0.507709</td>\n",
              "      <td>0.118564</td>\n",
              "      <td>0.007106</td>\n",
              "      <td>0.279881</td>\n",
              "      <td>0.494782</td>\n",
              "      <td>0.037404</td>\n",
              "      <td>0.207185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.479727</td>\n",
              "      <td>0.865063</td>\n",
              "      <td>-0.052868</td>\n",
              "      <td>0.521913</td>\n",
              "      <td>0.335642</td>\n",
              "      <td>-0.041206</td>\n",
              "      <td>0.468909</td>\n",
              "      <td>0.876704</td>\n",
              "      <td>-0.059213</td>\n",
              "      <td>0.459812</td>\n",
              "      <td>0.887839</td>\n",
              "      <td>-0.054344</td>\n",
              "      <td>0.422744</td>\n",
              "      <td>0.770018</td>\n",
              "      <td>-0.057373</td>\n",
              "      <td>0.168215</td>\n",
              "      <td>0.636903</td>\n",
              "      <td>0.338159</td>\n",
              "      <td>0.546422</td>\n",
              "      <td>0.660709</td>\n",
              "      <td>-0.123501</td>\n",
              "      <td>0.473307</td>\n",
              "      <td>0.854591</td>\n",
              "      <td>-0.037045</td>\n",
              "      <td>0.464889</td>\n",
              "      <td>0.859350</td>\n",
              "      <td>-0.040605</td>\n",
              "      <td>0.506149</td>\n",
              "      <td>0.682234</td>\n",
              "      <td>-0.094430</td>\n",
              "      <td>0.455463</td>\n",
              "      <td>0.684035</td>\n",
              "      <td>-0.061109</td>\n",
              "      <td>0.500817</td>\n",
              "      <td>0.675388</td>\n",
              "      <td>-0.092881</td>\n",
              "      <td>0.517969</td>\n",
              "      <td>0.281741</td>\n",
              "      <td>-0.043033</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14728</th>\n",
              "      <td>0.491013</td>\n",
              "      <td>0.712008</td>\n",
              "      <td>-0.122762</td>\n",
              "      <td>0.370838</td>\n",
              "      <td>0.521274</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.321248</td>\n",
              "      <td>0.549112</td>\n",
              "      <td>0.029226</td>\n",
              "      <td>0.397335</td>\n",
              "      <td>0.598340</td>\n",
              "      <td>-0.050178</td>\n",
              "      <td>0.231024</td>\n",
              "      <td>0.155950</td>\n",
              "      <td>0.198852</td>\n",
              "      <td>0.244556</td>\n",
              "      <td>0.193912</td>\n",
              "      <td>0.136056</td>\n",
              "      <td>0.262082</td>\n",
              "      <td>0.232480</td>\n",
              "      <td>0.084471</td>\n",
              "      <td>0.410388</td>\n",
              "      <td>0.861402</td>\n",
              "      <td>-0.035873</td>\n",
              "      <td>0.396322</td>\n",
              "      <td>0.229427</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.388710</td>\n",
              "      <td>0.161868</td>\n",
              "      <td>0.052182</td>\n",
              "      <td>0.378511</td>\n",
              "      <td>0.085723</td>\n",
              "      <td>0.091270</td>\n",
              "      <td>0.480310</td>\n",
              "      <td>0.072318</td>\n",
              "      <td>0.071249</td>\n",
              "      <td>0.277920</td>\n",
              "      <td>0.448965</td>\n",
              "      <td>0.099246</td>\n",
              "      <td>0.215149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.437324</td>\n",
              "      <td>0.820451</td>\n",
              "      <td>-0.054481</td>\n",
              "      <td>0.485971</td>\n",
              "      <td>0.284793</td>\n",
              "      <td>-0.002835</td>\n",
              "      <td>0.426605</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>-0.060726</td>\n",
              "      <td>0.418768</td>\n",
              "      <td>0.843263</td>\n",
              "      <td>-0.056251</td>\n",
              "      <td>0.383449</td>\n",
              "      <td>0.718743</td>\n",
              "      <td>-0.044822</td>\n",
              "      <td>0.213713</td>\n",
              "      <td>0.614015</td>\n",
              "      <td>0.380890</td>\n",
              "      <td>0.485615</td>\n",
              "      <td>0.622031</td>\n",
              "      <td>-0.120505</td>\n",
              "      <td>0.436568</td>\n",
              "      <td>0.807114</td>\n",
              "      <td>-0.036760</td>\n",
              "      <td>0.426990</td>\n",
              "      <td>0.810267</td>\n",
              "      <td>-0.040235</td>\n",
              "      <td>0.456969</td>\n",
              "      <td>0.639337</td>\n",
              "      <td>-0.088925</td>\n",
              "      <td>0.418537</td>\n",
              "      <td>0.638220</td>\n",
              "      <td>-0.049196</td>\n",
              "      <td>0.452996</td>\n",
              "      <td>0.632927</td>\n",
              "      <td>-0.086052</td>\n",
              "      <td>0.482375</td>\n",
              "      <td>0.228888</td>\n",
              "      <td>0.002328</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14729</th>\n",
              "      <td>0.527434</td>\n",
              "      <td>0.738273</td>\n",
              "      <td>-0.079975</td>\n",
              "      <td>0.328769</td>\n",
              "      <td>0.457341</td>\n",
              "      <td>-0.067171</td>\n",
              "      <td>0.263649</td>\n",
              "      <td>0.484076</td>\n",
              "      <td>-0.066484</td>\n",
              "      <td>0.365410</td>\n",
              "      <td>0.582288</td>\n",
              "      <td>-0.097822</td>\n",
              "      <td>0.136217</td>\n",
              "      <td>0.021140</td>\n",
              "      <td>-0.074558</td>\n",
              "      <td>0.175402</td>\n",
              "      <td>0.085695</td>\n",
              "      <td>-0.108161</td>\n",
              "      <td>0.210071</td>\n",
              "      <td>0.146272</td>\n",
              "      <td>-0.132795</td>\n",
              "      <td>0.343199</td>\n",
              "      <td>0.830920</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>0.406371</td>\n",
              "      <td>0.175288</td>\n",
              "      <td>-0.141420</td>\n",
              "      <td>0.396257</td>\n",
              "      <td>0.092930</td>\n",
              "      <td>-0.141256</td>\n",
              "      <td>0.377956</td>\n",
              "      <td>-0.004383</td>\n",
              "      <td>-0.133272</td>\n",
              "      <td>0.510257</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>-0.116076</td>\n",
              "      <td>0.188901</td>\n",
              "      <td>0.338566</td>\n",
              "      <td>-0.055147</td>\n",
              "      <td>0.084274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.396311</td>\n",
              "      <td>0.798353</td>\n",
              "      <td>-0.025366</td>\n",
              "      <td>0.508404</td>\n",
              "      <td>0.246948</td>\n",
              "      <td>-0.110513</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>0.809397</td>\n",
              "      <td>-0.028493</td>\n",
              "      <td>0.373280</td>\n",
              "      <td>0.819410</td>\n",
              "      <td>-0.022557</td>\n",
              "      <td>0.337109</td>\n",
              "      <td>0.707447</td>\n",
              "      <td>-0.056202</td>\n",
              "      <td>-0.054723</td>\n",
              "      <td>0.439372</td>\n",
              "      <td>0.278810</td>\n",
              "      <td>0.532050</td>\n",
              "      <td>0.647020</td>\n",
              "      <td>-0.122507</td>\n",
              "      <td>0.381318</td>\n",
              "      <td>0.785802</td>\n",
              "      <td>-0.011244</td>\n",
              "      <td>0.373739</td>\n",
              "      <td>0.790772</td>\n",
              "      <td>-0.014106</td>\n",
              "      <td>0.463954</td>\n",
              "      <td>0.650501</td>\n",
              "      <td>-0.093005</td>\n",
              "      <td>0.385598</td>\n",
              "      <td>0.631218</td>\n",
              "      <td>-0.068217</td>\n",
              "      <td>0.455586</td>\n",
              "      <td>0.642425</td>\n",
              "      <td>-0.095598</td>\n",
              "      <td>0.511577</td>\n",
              "      <td>0.192825</td>\n",
              "      <td>-0.125415</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14730</th>\n",
              "      <td>0.528845</td>\n",
              "      <td>0.735577</td>\n",
              "      <td>-0.041305</td>\n",
              "      <td>0.342966</td>\n",
              "      <td>0.537490</td>\n",
              "      <td>-0.021240</td>\n",
              "      <td>0.291597</td>\n",
              "      <td>0.566791</td>\n",
              "      <td>-0.001343</td>\n",
              "      <td>0.376911</td>\n",
              "      <td>0.633230</td>\n",
              "      <td>-0.048700</td>\n",
              "      <td>0.118212</td>\n",
              "      <td>0.205293</td>\n",
              "      <td>-0.040242</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.265181</td>\n",
              "      <td>-0.071281</td>\n",
              "      <td>0.188816</td>\n",
              "      <td>0.329133</td>\n",
              "      <td>-0.094329</td>\n",
              "      <td>0.402845</td>\n",
              "      <td>0.801184</td>\n",
              "      <td>0.070091</td>\n",
              "      <td>0.351299</td>\n",
              "      <td>0.319882</td>\n",
              "      <td>-0.148940</td>\n",
              "      <td>0.321788</td>\n",
              "      <td>0.228375</td>\n",
              "      <td>-0.157065</td>\n",
              "      <td>0.290454</td>\n",
              "      <td>0.137655</td>\n",
              "      <td>-0.158898</td>\n",
              "      <td>0.406340</td>\n",
              "      <td>0.110980</td>\n",
              "      <td>-0.176145</td>\n",
              "      <td>0.219970</td>\n",
              "      <td>0.473707</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>0.150090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.425510</td>\n",
              "      <td>0.768059</td>\n",
              "      <td>0.035476</td>\n",
              "      <td>0.450491</td>\n",
              "      <td>0.353091</td>\n",
              "      <td>-0.136861</td>\n",
              "      <td>0.419335</td>\n",
              "      <td>0.775359</td>\n",
              "      <td>0.032669</td>\n",
              "      <td>0.414294</td>\n",
              "      <td>0.783746</td>\n",
              "      <td>0.040324</td>\n",
              "      <td>0.367322</td>\n",
              "      <td>0.733595</td>\n",
              "      <td>0.017130</td>\n",
              "      <td>0.119499</td>\n",
              "      <td>0.563274</td>\n",
              "      <td>0.379588</td>\n",
              "      <td>0.510960</td>\n",
              "      <td>0.668780</td>\n",
              "      <td>-0.098167</td>\n",
              "      <td>0.413863</td>\n",
              "      <td>0.767714</td>\n",
              "      <td>0.057815</td>\n",
              "      <td>0.407349</td>\n",
              "      <td>0.770133</td>\n",
              "      <td>0.055871</td>\n",
              "      <td>0.462658</td>\n",
              "      <td>0.677776</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.400108</td>\n",
              "      <td>0.668224</td>\n",
              "      <td>-0.026347</td>\n",
              "      <td>0.454312</td>\n",
              "      <td>0.672526</td>\n",
              "      <td>-0.062448</td>\n",
              "      <td>0.442451</td>\n",
              "      <td>0.308193</td>\n",
              "      <td>-0.157336</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14731</th>\n",
              "      <td>0.452533</td>\n",
              "      <td>0.791973</td>\n",
              "      <td>-0.099548</td>\n",
              "      <td>0.335543</td>\n",
              "      <td>0.535332</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.284853</td>\n",
              "      <td>0.564165</td>\n",
              "      <td>0.029366</td>\n",
              "      <td>0.351197</td>\n",
              "      <td>0.637508</td>\n",
              "      <td>-0.043723</td>\n",
              "      <td>0.184646</td>\n",
              "      <td>0.126609</td>\n",
              "      <td>0.138878</td>\n",
              "      <td>0.201092</td>\n",
              "      <td>0.182896</td>\n",
              "      <td>0.081410</td>\n",
              "      <td>0.219815</td>\n",
              "      <td>0.240867</td>\n",
              "      <td>0.035379</td>\n",
              "      <td>0.358867</td>\n",
              "      <td>0.892018</td>\n",
              "      <td>-0.009873</td>\n",
              "      <td>0.362378</td>\n",
              "      <td>0.247935</td>\n",
              "      <td>-0.036624</td>\n",
              "      <td>0.353144</td>\n",
              "      <td>0.162810</td>\n",
              "      <td>-0.012710</td>\n",
              "      <td>0.343602</td>\n",
              "      <td>0.071618</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.451449</td>\n",
              "      <td>0.063975</td>\n",
              "      <td>-0.008453</td>\n",
              "      <td>0.239168</td>\n",
              "      <td>0.439385</td>\n",
              "      <td>0.086585</td>\n",
              "      <td>0.170206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.383385</td>\n",
              "      <td>0.848754</td>\n",
              "      <td>-0.034963</td>\n",
              "      <td>0.452250</td>\n",
              "      <td>0.307410</td>\n",
              "      <td>-0.048951</td>\n",
              "      <td>0.375028</td>\n",
              "      <td>0.858712</td>\n",
              "      <td>-0.041104</td>\n",
              "      <td>0.368408</td>\n",
              "      <td>0.870020</td>\n",
              "      <td>-0.035212</td>\n",
              "      <td>0.335122</td>\n",
              "      <td>0.770662</td>\n",
              "      <td>-0.023500</td>\n",
              "      <td>0.156868</td>\n",
              "      <td>0.588514</td>\n",
              "      <td>0.422205</td>\n",
              "      <td>0.446972</td>\n",
              "      <td>0.681988</td>\n",
              "      <td>-0.115878</td>\n",
              "      <td>0.378910</td>\n",
              "      <td>0.843526</td>\n",
              "      <td>-0.012812</td>\n",
              "      <td>0.371462</td>\n",
              "      <td>0.847001</td>\n",
              "      <td>-0.016520</td>\n",
              "      <td>0.412317</td>\n",
              "      <td>0.695334</td>\n",
              "      <td>-0.079299</td>\n",
              "      <td>0.370427</td>\n",
              "      <td>0.685399</td>\n",
              "      <td>-0.037217</td>\n",
              "      <td>0.407154</td>\n",
              "      <td>0.687007</td>\n",
              "      <td>-0.078226</td>\n",
              "      <td>0.449482</td>\n",
              "      <td>0.251166</td>\n",
              "      <td>-0.053193</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14732 rows × 1405 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_y       9_z  correct\n",
              "0      0.516601  0.585682 -0.128171  ...  0.187230  0.006673        0\n",
              "1      0.350142  0.704083 -0.051000  ...  0.274136 -0.123009        0\n",
              "2      0.478097  0.829416 -0.035864  ...  0.401297 -0.201465        0\n",
              "3      0.440407  0.633268 -0.130409  ...  0.249019 -0.042549        0\n",
              "4      0.508843  0.783509 -0.075715  ...  0.346828 -0.125290        0\n",
              "...         ...       ...       ...  ...       ...       ...      ...\n",
              "14727  0.560496  0.769144 -0.111749  ...  0.281741 -0.043033        6\n",
              "14728  0.491013  0.712008 -0.122762  ...  0.228888  0.002328        6\n",
              "14729  0.527434  0.738273 -0.079975  ...  0.192825 -0.125415        6\n",
              "14730  0.528845  0.735577 -0.041305  ...  0.308193 -0.157336        6\n",
              "14731  0.452533  0.791973 -0.099548  ...  0.251166 -0.053193        6\n",
              "\n",
              "[14732 rows x 1405 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "pDKn2CRL1V_6",
        "outputId": "62afb4de-a251-42c8-d370-8559359b873b"
      },
      "source": [
        "df_valid.drop(columns=df_valid.columns[[-2]],axis=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.646184</td>\n",
              "      <td>0.771045</td>\n",
              "      <td>-0.064509</td>\n",
              "      <td>0.479793</td>\n",
              "      <td>0.566351</td>\n",
              "      <td>-0.088064</td>\n",
              "      <td>0.426735</td>\n",
              "      <td>0.590875</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>0.520683</td>\n",
              "      <td>0.649389</td>\n",
              "      <td>-0.101141</td>\n",
              "      <td>0.297111</td>\n",
              "      <td>0.243108</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>0.351343</td>\n",
              "      <td>0.308081</td>\n",
              "      <td>-0.161240</td>\n",
              "      <td>0.396239</td>\n",
              "      <td>0.377762</td>\n",
              "      <td>-0.175256</td>\n",
              "      <td>0.463307</td>\n",
              "      <td>0.887630</td>\n",
              "      <td>-0.003552</td>\n",
              "      <td>0.560809</td>\n",
              "      <td>0.397581</td>\n",
              "      <td>-0.148304</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.302792</td>\n",
              "      <td>-0.148388</td>\n",
              "      <td>0.513932</td>\n",
              "      <td>0.218670</td>\n",
              "      <td>-0.145369</td>\n",
              "      <td>0.618115</td>\n",
              "      <td>0.221551</td>\n",
              "      <td>-0.109952</td>\n",
              "      <td>0.356944</td>\n",
              "      <td>0.489348</td>\n",
              "      <td>-0.107089</td>\n",
              "      <td>0.258764</td>\n",
              "      <td>...</td>\n",
              "      <td>0.514319</td>\n",
              "      <td>0.861163</td>\n",
              "      <td>-0.021057</td>\n",
              "      <td>0.630754</td>\n",
              "      <td>0.437828</td>\n",
              "      <td>-0.104779</td>\n",
              "      <td>0.503365</td>\n",
              "      <td>0.870509</td>\n",
              "      <td>-0.021248</td>\n",
              "      <td>0.491106</td>\n",
              "      <td>0.878412</td>\n",
              "      <td>-0.018093</td>\n",
              "      <td>0.482451</td>\n",
              "      <td>0.760005</td>\n",
              "      <td>-0.072059</td>\n",
              "      <td>0.072792</td>\n",
              "      <td>0.583212</td>\n",
              "      <td>0.110986</td>\n",
              "      <td>0.669708</td>\n",
              "      <td>0.690351</td>\n",
              "      <td>-0.098678</td>\n",
              "      <td>0.497744</td>\n",
              "      <td>0.844740</td>\n",
              "      <td>-0.020899</td>\n",
              "      <td>0.490786</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>-0.021804</td>\n",
              "      <td>0.605350</td>\n",
              "      <td>0.697076</td>\n",
              "      <td>-0.082129</td>\n",
              "      <td>0.535840</td>\n",
              "      <td>0.685600</td>\n",
              "      <td>-0.071414</td>\n",
              "      <td>0.600393</td>\n",
              "      <td>0.690829</td>\n",
              "      <td>-0.085879</td>\n",
              "      <td>0.633086</td>\n",
              "      <td>0.403363</td>\n",
              "      <td>-0.117815</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.505669</td>\n",
              "      <td>0.783729</td>\n",
              "      <td>-0.018900</td>\n",
              "      <td>0.383348</td>\n",
              "      <td>0.551676</td>\n",
              "      <td>-0.021686</td>\n",
              "      <td>0.335575</td>\n",
              "      <td>0.573097</td>\n",
              "      <td>-0.002283</td>\n",
              "      <td>0.399952</td>\n",
              "      <td>0.650358</td>\n",
              "      <td>-0.031835</td>\n",
              "      <td>0.229131</td>\n",
              "      <td>0.181487</td>\n",
              "      <td>-0.092331</td>\n",
              "      <td>0.258003</td>\n",
              "      <td>0.257158</td>\n",
              "      <td>-0.111276</td>\n",
              "      <td>0.284237</td>\n",
              "      <td>0.338158</td>\n",
              "      <td>-0.123996</td>\n",
              "      <td>0.408235</td>\n",
              "      <td>0.862809</td>\n",
              "      <td>0.118722</td>\n",
              "      <td>0.414858</td>\n",
              "      <td>0.365437</td>\n",
              "      <td>-0.173962</td>\n",
              "      <td>0.397574</td>\n",
              "      <td>0.252688</td>\n",
              "      <td>-0.191834</td>\n",
              "      <td>0.380021</td>\n",
              "      <td>0.148873</td>\n",
              "      <td>-0.204391</td>\n",
              "      <td>0.480591</td>\n",
              "      <td>0.143603</td>\n",
              "      <td>-0.224993</td>\n",
              "      <td>0.294881</td>\n",
              "      <td>0.460291</td>\n",
              "      <td>-0.004822</td>\n",
              "      <td>0.217984</td>\n",
              "      <td>...</td>\n",
              "      <td>0.435181</td>\n",
              "      <td>0.831755</td>\n",
              "      <td>0.075494</td>\n",
              "      <td>0.490290</td>\n",
              "      <td>0.410842</td>\n",
              "      <td>-0.158440</td>\n",
              "      <td>0.426757</td>\n",
              "      <td>0.840886</td>\n",
              "      <td>0.078719</td>\n",
              "      <td>0.419972</td>\n",
              "      <td>0.850027</td>\n",
              "      <td>0.088572</td>\n",
              "      <td>0.383190</td>\n",
              "      <td>0.750469</td>\n",
              "      <td>0.039978</td>\n",
              "      <td>0.169034</td>\n",
              "      <td>0.535456</td>\n",
              "      <td>0.333599</td>\n",
              "      <td>0.499905</td>\n",
              "      <td>0.714165</td>\n",
              "      <td>-0.075163</td>\n",
              "      <td>0.430074</td>\n",
              "      <td>0.816941</td>\n",
              "      <td>0.089510</td>\n",
              "      <td>0.422110</td>\n",
              "      <td>0.818853</td>\n",
              "      <td>0.090431</td>\n",
              "      <td>0.462675</td>\n",
              "      <td>0.711085</td>\n",
              "      <td>-0.036148</td>\n",
              "      <td>0.415931</td>\n",
              "      <td>0.687106</td>\n",
              "      <td>-0.008565</td>\n",
              "      <td>0.457287</td>\n",
              "      <td>0.704161</td>\n",
              "      <td>-0.042259</td>\n",
              "      <td>0.487358</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>-0.183734</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.570398</td>\n",
              "      <td>0.522947</td>\n",
              "      <td>-0.131259</td>\n",
              "      <td>0.385422</td>\n",
              "      <td>0.250885</td>\n",
              "      <td>-0.050247</td>\n",
              "      <td>0.313107</td>\n",
              "      <td>0.278246</td>\n",
              "      <td>-0.048979</td>\n",
              "      <td>0.409144</td>\n",
              "      <td>0.370157</td>\n",
              "      <td>-0.113957</td>\n",
              "      <td>0.206225</td>\n",
              "      <td>-0.195378</td>\n",
              "      <td>0.061084</td>\n",
              "      <td>0.242512</td>\n",
              "      <td>-0.130779</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.276074</td>\n",
              "      <td>-0.063120</td>\n",
              "      <td>-0.046078</td>\n",
              "      <td>0.358516</td>\n",
              "      <td>0.730264</td>\n",
              "      <td>0.017391</td>\n",
              "      <td>0.468482</td>\n",
              "      <td>-0.048374</td>\n",
              "      <td>-0.084189</td>\n",
              "      <td>0.454622</td>\n",
              "      <td>-0.150500</td>\n",
              "      <td>-0.053712</td>\n",
              "      <td>0.437219</td>\n",
              "      <td>-0.252034</td>\n",
              "      <td>-0.025488</td>\n",
              "      <td>0.571085</td>\n",
              "      <td>-0.254265</td>\n",
              "      <td>-0.023915</td>\n",
              "      <td>0.253504</td>\n",
              "      <td>0.142910</td>\n",
              "      <td>0.007577</td>\n",
              "      <td>0.149256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.407505</td>\n",
              "      <td>0.706014</td>\n",
              "      <td>-0.006454</td>\n",
              "      <td>0.569965</td>\n",
              "      <td>0.023101</td>\n",
              "      <td>-0.076176</td>\n",
              "      <td>0.390708</td>\n",
              "      <td>0.716519</td>\n",
              "      <td>-0.009776</td>\n",
              "      <td>0.376529</td>\n",
              "      <td>0.725704</td>\n",
              "      <td>-0.003093</td>\n",
              "      <td>0.352764</td>\n",
              "      <td>0.512117</td>\n",
              "      <td>-0.059380</td>\n",
              "      <td>0.041792</td>\n",
              "      <td>0.330219</td>\n",
              "      <td>0.345695</td>\n",
              "      <td>0.581562</td>\n",
              "      <td>0.438015</td>\n",
              "      <td>-0.170266</td>\n",
              "      <td>0.391589</td>\n",
              "      <td>0.667979</td>\n",
              "      <td>0.011425</td>\n",
              "      <td>0.375232</td>\n",
              "      <td>0.668794</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.507817</td>\n",
              "      <td>0.442177</td>\n",
              "      <td>-0.133319</td>\n",
              "      <td>0.426135</td>\n",
              "      <td>0.421102</td>\n",
              "      <td>-0.094905</td>\n",
              "      <td>0.501055</td>\n",
              "      <td>0.433366</td>\n",
              "      <td>-0.135095</td>\n",
              "      <td>0.571534</td>\n",
              "      <td>-0.036601</td>\n",
              "      <td>-0.077871</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.550640</td>\n",
              "      <td>0.693204</td>\n",
              "      <td>-0.039426</td>\n",
              "      <td>0.377895</td>\n",
              "      <td>0.420303</td>\n",
              "      <td>-0.049635</td>\n",
              "      <td>0.310557</td>\n",
              "      <td>0.444947</td>\n",
              "      <td>-0.030445</td>\n",
              "      <td>0.395424</td>\n",
              "      <td>0.535625</td>\n",
              "      <td>-0.071252</td>\n",
              "      <td>0.196327</td>\n",
              "      <td>0.037854</td>\n",
              "      <td>-0.122527</td>\n",
              "      <td>0.226640</td>\n",
              "      <td>0.102505</td>\n",
              "      <td>-0.149672</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.168133</td>\n",
              "      <td>-0.169691</td>\n",
              "      <td>0.385633</td>\n",
              "      <td>0.777333</td>\n",
              "      <td>0.130624</td>\n",
              "      <td>0.440921</td>\n",
              "      <td>0.182286</td>\n",
              "      <td>-0.220092</td>\n",
              "      <td>0.426143</td>\n",
              "      <td>0.093054</td>\n",
              "      <td>-0.238092</td>\n",
              "      <td>0.408764</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.251741</td>\n",
              "      <td>0.543859</td>\n",
              "      <td>-0.005355</td>\n",
              "      <td>-0.265706</td>\n",
              "      <td>0.259817</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>-0.029762</td>\n",
              "      <td>0.159518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.425009</td>\n",
              "      <td>0.744009</td>\n",
              "      <td>0.082338</td>\n",
              "      <td>0.545075</td>\n",
              "      <td>0.237159</td>\n",
              "      <td>-0.195135</td>\n",
              "      <td>0.414130</td>\n",
              "      <td>0.752458</td>\n",
              "      <td>0.083467</td>\n",
              "      <td>0.404759</td>\n",
              "      <td>0.761788</td>\n",
              "      <td>0.094698</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>0.646520</td>\n",
              "      <td>0.029753</td>\n",
              "      <td>0.087381</td>\n",
              "      <td>0.425480</td>\n",
              "      <td>0.414335</td>\n",
              "      <td>0.550101</td>\n",
              "      <td>0.611782</td>\n",
              "      <td>-0.120470</td>\n",
              "      <td>0.411218</td>\n",
              "      <td>0.722901</td>\n",
              "      <td>0.103741</td>\n",
              "      <td>0.399978</td>\n",
              "      <td>0.723879</td>\n",
              "      <td>0.104076</td>\n",
              "      <td>0.488015</td>\n",
              "      <td>0.608906</td>\n",
              "      <td>-0.072477</td>\n",
              "      <td>0.414568</td>\n",
              "      <td>0.579445</td>\n",
              "      <td>-0.037483</td>\n",
              "      <td>0.479306</td>\n",
              "      <td>0.600204</td>\n",
              "      <td>-0.079331</td>\n",
              "      <td>0.544766</td>\n",
              "      <td>0.188114</td>\n",
              "      <td>-0.223748</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.524232</td>\n",
              "      <td>0.722622</td>\n",
              "      <td>-0.049771</td>\n",
              "      <td>0.378299</td>\n",
              "      <td>0.537701</td>\n",
              "      <td>-0.022929</td>\n",
              "      <td>0.332367</td>\n",
              "      <td>0.561998</td>\n",
              "      <td>-0.011429</td>\n",
              "      <td>0.401680</td>\n",
              "      <td>0.617495</td>\n",
              "      <td>-0.051351</td>\n",
              "      <td>0.205039</td>\n",
              "      <td>0.227481</td>\n",
              "      <td>-0.018862</td>\n",
              "      <td>0.235042</td>\n",
              "      <td>0.282814</td>\n",
              "      <td>-0.050678</td>\n",
              "      <td>0.265167</td>\n",
              "      <td>0.346141</td>\n",
              "      <td>-0.075391</td>\n",
              "      <td>0.425222</td>\n",
              "      <td>0.818088</td>\n",
              "      <td>0.049098</td>\n",
              "      <td>0.399753</td>\n",
              "      <td>0.352436</td>\n",
              "      <td>-0.117018</td>\n",
              "      <td>0.375707</td>\n",
              "      <td>0.255840</td>\n",
              "      <td>-0.116756</td>\n",
              "      <td>0.351857</td>\n",
              "      <td>0.170662</td>\n",
              "      <td>-0.111585</td>\n",
              "      <td>0.448775</td>\n",
              "      <td>0.154980</td>\n",
              "      <td>-0.122691</td>\n",
              "      <td>0.283837</td>\n",
              "      <td>0.472371</td>\n",
              "      <td>0.007503</td>\n",
              "      <td>0.215384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.447646</td>\n",
              "      <td>0.779411</td>\n",
              "      <td>0.023573</td>\n",
              "      <td>0.478230</td>\n",
              "      <td>0.386732</td>\n",
              "      <td>-0.106638</td>\n",
              "      <td>0.441388</td>\n",
              "      <td>0.787471</td>\n",
              "      <td>0.022020</td>\n",
              "      <td>0.435998</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.027950</td>\n",
              "      <td>0.390656</td>\n",
              "      <td>0.715117</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.174290</td>\n",
              "      <td>0.598808</td>\n",
              "      <td>0.309484</td>\n",
              "      <td>0.511831</td>\n",
              "      <td>0.657377</td>\n",
              "      <td>-0.094378</td>\n",
              "      <td>0.437287</td>\n",
              "      <td>0.769950</td>\n",
              "      <td>0.040689</td>\n",
              "      <td>0.429929</td>\n",
              "      <td>0.771605</td>\n",
              "      <td>0.039537</td>\n",
              "      <td>0.471688</td>\n",
              "      <td>0.662292</td>\n",
              "      <td>-0.062175</td>\n",
              "      <td>0.420203</td>\n",
              "      <td>0.650105</td>\n",
              "      <td>-0.033778</td>\n",
              "      <td>0.465479</td>\n",
              "      <td>0.656399</td>\n",
              "      <td>-0.065202</td>\n",
              "      <td>0.473144</td>\n",
              "      <td>0.346925</td>\n",
              "      <td>-0.121656</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14631</th>\n",
              "      <td>0.529442</td>\n",
              "      <td>0.667210</td>\n",
              "      <td>-0.066573</td>\n",
              "      <td>0.386126</td>\n",
              "      <td>0.486287</td>\n",
              "      <td>-0.033634</td>\n",
              "      <td>0.341563</td>\n",
              "      <td>0.510485</td>\n",
              "      <td>-0.029726</td>\n",
              "      <td>0.418692</td>\n",
              "      <td>0.560215</td>\n",
              "      <td>-0.061615</td>\n",
              "      <td>0.219367</td>\n",
              "      <td>0.183714</td>\n",
              "      <td>-0.007092</td>\n",
              "      <td>0.248392</td>\n",
              "      <td>0.228448</td>\n",
              "      <td>-0.037222</td>\n",
              "      <td>0.276280</td>\n",
              "      <td>0.271904</td>\n",
              "      <td>-0.061118</td>\n",
              "      <td>0.423037</td>\n",
              "      <td>0.759102</td>\n",
              "      <td>-0.001626</td>\n",
              "      <td>0.413478</td>\n",
              "      <td>0.273705</td>\n",
              "      <td>-0.080519</td>\n",
              "      <td>0.398730</td>\n",
              "      <td>0.209388</td>\n",
              "      <td>-0.074626</td>\n",
              "      <td>0.379055</td>\n",
              "      <td>0.137959</td>\n",
              "      <td>-0.063146</td>\n",
              "      <td>0.475969</td>\n",
              "      <td>0.127867</td>\n",
              "      <td>-0.057848</td>\n",
              "      <td>0.284961</td>\n",
              "      <td>0.422919</td>\n",
              "      <td>-0.011928</td>\n",
              "      <td>0.216206</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454311</td>\n",
              "      <td>0.732706</td>\n",
              "      <td>-0.021548</td>\n",
              "      <td>0.494808</td>\n",
              "      <td>0.313197</td>\n",
              "      <td>-0.065520</td>\n",
              "      <td>0.445838</td>\n",
              "      <td>0.741607</td>\n",
              "      <td>-0.024574</td>\n",
              "      <td>0.437814</td>\n",
              "      <td>0.749662</td>\n",
              "      <td>-0.020488</td>\n",
              "      <td>0.407561</td>\n",
              "      <td>0.660633</td>\n",
              "      <td>-0.038206</td>\n",
              "      <td>0.153465</td>\n",
              "      <td>0.517849</td>\n",
              "      <td>0.231584</td>\n",
              "      <td>0.524237</td>\n",
              "      <td>0.586032</td>\n",
              "      <td>-0.089157</td>\n",
              "      <td>0.447015</td>\n",
              "      <td>0.723673</td>\n",
              "      <td>-0.012398</td>\n",
              "      <td>0.440423</td>\n",
              "      <td>0.727365</td>\n",
              "      <td>-0.014400</td>\n",
              "      <td>0.484241</td>\n",
              "      <td>0.597897</td>\n",
              "      <td>-0.066358</td>\n",
              "      <td>0.436844</td>\n",
              "      <td>0.592494</td>\n",
              "      <td>-0.044895</td>\n",
              "      <td>0.479310</td>\n",
              "      <td>0.592026</td>\n",
              "      <td>-0.067038</td>\n",
              "      <td>0.491907</td>\n",
              "      <td>0.271605</td>\n",
              "      <td>-0.073554</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14632</th>\n",
              "      <td>0.500539</td>\n",
              "      <td>0.734860</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>0.327387</td>\n",
              "      <td>0.516931</td>\n",
              "      <td>-0.010588</td>\n",
              "      <td>0.265571</td>\n",
              "      <td>0.545812</td>\n",
              "      <td>-0.007016</td>\n",
              "      <td>0.361814</td>\n",
              "      <td>0.604152</td>\n",
              "      <td>-0.083395</td>\n",
              "      <td>0.160265</td>\n",
              "      <td>0.151897</td>\n",
              "      <td>0.173618</td>\n",
              "      <td>0.184835</td>\n",
              "      <td>0.188428</td>\n",
              "      <td>0.104391</td>\n",
              "      <td>0.210830</td>\n",
              "      <td>0.227435</td>\n",
              "      <td>0.047180</td>\n",
              "      <td>0.350158</td>\n",
              "      <td>0.870057</td>\n",
              "      <td>-0.090012</td>\n",
              "      <td>0.386060</td>\n",
              "      <td>0.230485</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.377281</td>\n",
              "      <td>0.166759</td>\n",
              "      <td>0.043994</td>\n",
              "      <td>0.364579</td>\n",
              "      <td>0.095391</td>\n",
              "      <td>0.089954</td>\n",
              "      <td>0.487561</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.089601</td>\n",
              "      <td>0.203078</td>\n",
              "      <td>0.435127</td>\n",
              "      <td>0.063342</td>\n",
              "      <td>0.114195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.393957</td>\n",
              "      <td>0.820860</td>\n",
              "      <td>-0.104007</td>\n",
              "      <td>0.487804</td>\n",
              "      <td>0.291259</td>\n",
              "      <td>-0.005642</td>\n",
              "      <td>0.382895</td>\n",
              "      <td>0.832251</td>\n",
              "      <td>-0.113706</td>\n",
              "      <td>0.372782</td>\n",
              "      <td>0.845191</td>\n",
              "      <td>-0.110114</td>\n",
              "      <td>0.337598</td>\n",
              "      <td>0.735444</td>\n",
              "      <td>-0.089631</td>\n",
              "      <td>0.034739</td>\n",
              "      <td>0.615029</td>\n",
              "      <td>0.373997</td>\n",
              "      <td>0.498705</td>\n",
              "      <td>0.626003</td>\n",
              "      <td>-0.145978</td>\n",
              "      <td>0.384501</td>\n",
              "      <td>0.817192</td>\n",
              "      <td>-0.083325</td>\n",
              "      <td>0.376356</td>\n",
              "      <td>0.821481</td>\n",
              "      <td>-0.089008</td>\n",
              "      <td>0.447718</td>\n",
              "      <td>0.647504</td>\n",
              "      <td>-0.116903</td>\n",
              "      <td>0.385622</td>\n",
              "      <td>0.649125</td>\n",
              "      <td>-0.076227</td>\n",
              "      <td>0.441510</td>\n",
              "      <td>0.639893</td>\n",
              "      <td>-0.113286</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.238567</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14633</th>\n",
              "      <td>0.494283</td>\n",
              "      <td>0.710024</td>\n",
              "      <td>-0.116081</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.497803</td>\n",
              "      <td>-0.009445</td>\n",
              "      <td>0.295208</td>\n",
              "      <td>0.520917</td>\n",
              "      <td>-0.001318</td>\n",
              "      <td>0.381529</td>\n",
              "      <td>0.580622</td>\n",
              "      <td>-0.068246</td>\n",
              "      <td>0.204759</td>\n",
              "      <td>0.123697</td>\n",
              "      <td>0.127479</td>\n",
              "      <td>0.225363</td>\n",
              "      <td>0.164128</td>\n",
              "      <td>0.072507</td>\n",
              "      <td>0.249508</td>\n",
              "      <td>0.206157</td>\n",
              "      <td>0.027697</td>\n",
              "      <td>0.370819</td>\n",
              "      <td>0.833998</td>\n",
              "      <td>-0.037612</td>\n",
              "      <td>0.400362</td>\n",
              "      <td>0.218823</td>\n",
              "      <td>-0.020845</td>\n",
              "      <td>0.388722</td>\n",
              "      <td>0.150536</td>\n",
              "      <td>0.007938</td>\n",
              "      <td>0.374952</td>\n",
              "      <td>0.073443</td>\n",
              "      <td>0.040432</td>\n",
              "      <td>0.483528</td>\n",
              "      <td>0.071126</td>\n",
              "      <td>0.034102</td>\n",
              "      <td>0.244181</td>\n",
              "      <td>0.414586</td>\n",
              "      <td>0.053564</td>\n",
              "      <td>0.170244</td>\n",
              "      <td>...</td>\n",
              "      <td>0.400971</td>\n",
              "      <td>0.797283</td>\n",
              "      <td>-0.057481</td>\n",
              "      <td>0.491774</td>\n",
              "      <td>0.282192</td>\n",
              "      <td>-0.024813</td>\n",
              "      <td>0.390480</td>\n",
              "      <td>0.806401</td>\n",
              "      <td>-0.063569</td>\n",
              "      <td>0.381867</td>\n",
              "      <td>0.816910</td>\n",
              "      <td>-0.058509</td>\n",
              "      <td>0.356374</td>\n",
              "      <td>0.697895</td>\n",
              "      <td>-0.055486</td>\n",
              "      <td>0.130981</td>\n",
              "      <td>0.558962</td>\n",
              "      <td>0.341036</td>\n",
              "      <td>0.493862</td>\n",
              "      <td>0.611363</td>\n",
              "      <td>-0.122746</td>\n",
              "      <td>0.395654</td>\n",
              "      <td>0.785397</td>\n",
              "      <td>-0.038678</td>\n",
              "      <td>0.386269</td>\n",
              "      <td>0.787321</td>\n",
              "      <td>-0.042230</td>\n",
              "      <td>0.452422</td>\n",
              "      <td>0.626979</td>\n",
              "      <td>-0.094577</td>\n",
              "      <td>0.401670</td>\n",
              "      <td>0.622043</td>\n",
              "      <td>-0.060001</td>\n",
              "      <td>0.447939</td>\n",
              "      <td>0.619601</td>\n",
              "      <td>-0.092344</td>\n",
              "      <td>0.490952</td>\n",
              "      <td>0.228598</td>\n",
              "      <td>-0.023898</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14634</th>\n",
              "      <td>0.480344</td>\n",
              "      <td>0.614472</td>\n",
              "      <td>-0.073619</td>\n",
              "      <td>0.351348</td>\n",
              "      <td>0.359926</td>\n",
              "      <td>-0.029388</td>\n",
              "      <td>0.296622</td>\n",
              "      <td>0.382071</td>\n",
              "      <td>-0.013036</td>\n",
              "      <td>0.371280</td>\n",
              "      <td>0.466668</td>\n",
              "      <td>-0.062456</td>\n",
              "      <td>0.200838</td>\n",
              "      <td>-0.066494</td>\n",
              "      <td>-0.011770</td>\n",
              "      <td>0.220040</td>\n",
              "      <td>-0.001829</td>\n",
              "      <td>-0.048701</td>\n",
              "      <td>0.240770</td>\n",
              "      <td>0.061476</td>\n",
              "      <td>-0.075901</td>\n",
              "      <td>0.363895</td>\n",
              "      <td>0.734271</td>\n",
              "      <td>0.071702</td>\n",
              "      <td>0.399486</td>\n",
              "      <td>0.096403</td>\n",
              "      <td>-0.128011</td>\n",
              "      <td>0.393551</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.130443</td>\n",
              "      <td>0.384674</td>\n",
              "      <td>-0.095847</td>\n",
              "      <td>-0.123506</td>\n",
              "      <td>0.504907</td>\n",
              "      <td>-0.092631</td>\n",
              "      <td>-0.136478</td>\n",
              "      <td>0.245395</td>\n",
              "      <td>0.251886</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.169527</td>\n",
              "      <td>...</td>\n",
              "      <td>0.401906</td>\n",
              "      <td>0.721825</td>\n",
              "      <td>0.031216</td>\n",
              "      <td>0.494581</td>\n",
              "      <td>0.161664</td>\n",
              "      <td>-0.117363</td>\n",
              "      <td>0.387729</td>\n",
              "      <td>0.731502</td>\n",
              "      <td>0.031917</td>\n",
              "      <td>0.376631</td>\n",
              "      <td>0.738281</td>\n",
              "      <td>0.040350</td>\n",
              "      <td>0.353397</td>\n",
              "      <td>0.587014</td>\n",
              "      <td>-0.009165</td>\n",
              "      <td>0.138784</td>\n",
              "      <td>0.330477</td>\n",
              "      <td>0.364592</td>\n",
              "      <td>0.478526</td>\n",
              "      <td>0.523328</td>\n",
              "      <td>-0.111924</td>\n",
              "      <td>0.401928</td>\n",
              "      <td>0.697973</td>\n",
              "      <td>0.040930</td>\n",
              "      <td>0.390769</td>\n",
              "      <td>0.701493</td>\n",
              "      <td>0.040458</td>\n",
              "      <td>0.437818</td>\n",
              "      <td>0.528118</td>\n",
              "      <td>-0.072447</td>\n",
              "      <td>0.389704</td>\n",
              "      <td>0.510692</td>\n",
              "      <td>-0.040840</td>\n",
              "      <td>0.432847</td>\n",
              "      <td>0.520244</td>\n",
              "      <td>-0.076076</td>\n",
              "      <td>0.496153</td>\n",
              "      <td>0.107584</td>\n",
              "      <td>-0.134429</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>0.451848</td>\n",
              "      <td>0.622114</td>\n",
              "      <td>-0.181673</td>\n",
              "      <td>0.305574</td>\n",
              "      <td>0.401656</td>\n",
              "      <td>-0.015953</td>\n",
              "      <td>0.233015</td>\n",
              "      <td>0.421863</td>\n",
              "      <td>-0.015226</td>\n",
              "      <td>0.335668</td>\n",
              "      <td>0.477531</td>\n",
              "      <td>-0.099770</td>\n",
              "      <td>0.200237</td>\n",
              "      <td>0.018560</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.221613</td>\n",
              "      <td>0.058618</td>\n",
              "      <td>0.131506</td>\n",
              "      <td>0.242395</td>\n",
              "      <td>0.106519</td>\n",
              "      <td>0.064972</td>\n",
              "      <td>0.257232</td>\n",
              "      <td>0.817205</td>\n",
              "      <td>-0.085517</td>\n",
              "      <td>0.429682</td>\n",
              "      <td>0.147278</td>\n",
              "      <td>0.012527</td>\n",
              "      <td>0.433578</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.067867</td>\n",
              "      <td>0.434073</td>\n",
              "      <td>-0.013070</td>\n",
              "      <td>0.124950</td>\n",
              "      <td>0.568935</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.123650</td>\n",
              "      <td>0.182558</td>\n",
              "      <td>0.309983</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>0.081712</td>\n",
              "      <td>...</td>\n",
              "      <td>0.325224</td>\n",
              "      <td>0.787673</td>\n",
              "      <td>-0.104376</td>\n",
              "      <td>0.523025</td>\n",
              "      <td>0.221324</td>\n",
              "      <td>0.002962</td>\n",
              "      <td>0.306585</td>\n",
              "      <td>0.796358</td>\n",
              "      <td>-0.111493</td>\n",
              "      <td>0.289647</td>\n",
              "      <td>0.805510</td>\n",
              "      <td>-0.107110</td>\n",
              "      <td>0.283806</td>\n",
              "      <td>0.617631</td>\n",
              "      <td>-0.111347</td>\n",
              "      <td>-0.033763</td>\n",
              "      <td>0.493091</td>\n",
              "      <td>0.385915</td>\n",
              "      <td>0.465378</td>\n",
              "      <td>0.510059</td>\n",
              "      <td>-0.169199</td>\n",
              "      <td>0.322039</td>\n",
              "      <td>0.765549</td>\n",
              "      <td>-0.091213</td>\n",
              "      <td>0.309973</td>\n",
              "      <td>0.766603</td>\n",
              "      <td>-0.096392</td>\n",
              "      <td>0.413244</td>\n",
              "      <td>0.528470</td>\n",
              "      <td>-0.136841</td>\n",
              "      <td>0.352832</td>\n",
              "      <td>0.525035</td>\n",
              "      <td>-0.093184</td>\n",
              "      <td>0.409769</td>\n",
              "      <td>0.520118</td>\n",
              "      <td>-0.133804</td>\n",
              "      <td>0.533610</td>\n",
              "      <td>0.171482</td>\n",
              "      <td>0.013007</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14636 rows × 1405 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_y       9_z  correct\n",
              "0      0.646184  0.771045 -0.064509  ...  0.403363 -0.117815        0\n",
              "1      0.505669  0.783729 -0.018900  ...  0.367928 -0.183734        0\n",
              "2      0.570398  0.522947 -0.131259  ... -0.036601 -0.077871        0\n",
              "3      0.550640  0.693204 -0.039426  ...  0.188114 -0.223748        0\n",
              "4      0.524232  0.722622 -0.049771  ...  0.346925 -0.121656        0\n",
              "...         ...       ...       ...  ...       ...       ...      ...\n",
              "14631  0.529442  0.667210 -0.066573  ...  0.271605 -0.073554        6\n",
              "14632  0.500539  0.734860 -0.148886  ...  0.238567  0.000434        6\n",
              "14633  0.494283  0.710024 -0.116081  ...  0.228598 -0.023898        6\n",
              "14634  0.480344  0.614472 -0.073619  ...  0.107584 -0.134429        6\n",
              "14635  0.451848  0.622114 -0.181673  ...  0.171482  0.013007        6\n",
              "\n",
              "[14636 rows x 1405 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4gkpPe6eh6W"
      },
      "source": [
        "3659,4  \n",
        "30,487"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBmlmqc1mNui"
      },
      "source": [
        "それぞれのデータを目的変数と説明変数に分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xf5l4ylWS_V"
      },
      "source": [
        "x_train=DataFrame(df_train.drop(\"correct\",axis=1))\n",
        "y_train=DataFrame(df_train[\"correct\"])\n",
        "\n",
        "x_test=DataFrame(df_test.drop(\"correct\",axis=1))\n",
        "y_test=DataFrame(df_test[\"correct\"])\n",
        "\n",
        "x_valid=DataFrame(df_valid.drop(\"correct\",axis=1))\n",
        "y_valid=DataFrame(df_valid[\"correct\"])\n",
        "\n",
        "#データの整形\n",
        "x_train = x_train.astype(np.float)\n",
        "x_test = x_test.astype(np.float)\n",
        "x_valid = x_valid.astype(np.float)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,7)\n",
        "y_test = np_utils.to_categorical(y_test,7)\n",
        "y_valid = np_utils.to_categorical(y_valid,7)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZIebnHV0MAz"
      },
      "source": [
        "batch_size=256 #バッチサイズ\n",
        "n_rnn = 1  # 時系列の数\n",
        "n_sample = len(x_train)-n_rnn  # サンプル数\n",
        "n_in=len(df_train.columns)-1 #入力層のニューロン数\n",
        "n_mid=128 #中間層のニューロン数\n",
        "n_out=7 #出力層のニューロン数"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUtFTKJkmdSl"
      },
      "source": [
        "モデル構築  \n",
        "SimpleRNN:全結合の中間層が再起的になる。  \n",
        "LSTM:RNNの発展版であるLSTMを活用できる。複雑な時系列データを扱えるが学習に時間がかかる  \n",
        "GRU: LSTMの簡易版。パラメータが少ないので学習に時間がかからない"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sL9ng-uKFK5"
      },
      "source": [
        "#ニューラルネットワークの実装①\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_in,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4rTm8KqW0jK",
        "outputId": "4be21ae7-f9d8-4900-a743-a29aac94a641"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 50)                70300     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 7)                 357       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 70,657\n",
            "Trainable params: 70,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdrsd9iXABvZ"
      },
      "source": [
        "adam = Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZDO6Y1hfTbP"
      },
      "source": [
        "model.save('/content/drive/MyDrive/data分析/Mediapipe/Mediapipemodel.h5')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGnhC5IwkpQu",
        "outputId": "8a059105-a74d-43fc-a4c7-c27dd5006799"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "modelCheckpoint = ModelCheckpoint(filepath ='/content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5',\n",
        "                                  monitor='val_accuracy',\n",
        "                                  verbose=1,\n",
        "                                  save_best_only=True,\n",
        "                                  save_weights_only=False,\n",
        "                                  mode='max',\n",
        "                                  period=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tqXuJjsmhTy"
      },
      "source": [
        "##  学習  \n",
        "とんでもない精度ですね"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "704KOTdfW_7y",
        "outputId": "dd8e3d9c-4611-4311-c09f-ca52624e1e7a"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    #steps_per_epoch=85,\n",
        "                    epochs=1000,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    validation_batch_size=64,\n",
        "                    #validation_steps=30,\n",
        "                    callbacks=[modelCheckpoint]\n",
        "                    )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 193.1222 - accuracy: 0.2094\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.33616, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 191.4446 - accuracy: 0.2106 - val_loss: 5.2774 - val_accuracy: 0.3362\n",
            "Epoch 2/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 69.4061 - accuracy: 0.2772\n",
            "Epoch 00002: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 69.2456 - accuracy: 0.2773 - val_loss: 4.1244 - val_accuracy: 0.2849\n",
            "Epoch 3/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 36.6842 - accuracy: 0.2876\n",
            "Epoch 00003: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 36.5158 - accuracy: 0.2877 - val_loss: 5.1947 - val_accuracy: 0.1947\n",
            "Epoch 4/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 14.1516 - accuracy: 0.2944\n",
            "Epoch 00004: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 14.0604 - accuracy: 0.2943 - val_loss: 2.5180 - val_accuracy: 0.1129\n",
            "Epoch 5/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 3.3327 - accuracy: 0.3058\n",
            "Epoch 00005: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 3.3327 - accuracy: 0.3058 - val_loss: 1.9661 - val_accuracy: 0.0532\n",
            "Epoch 6/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.2980\n",
            "Epoch 00006: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.5298 - accuracy: 0.2980 - val_loss: 1.9489 - val_accuracy: 0.2154\n",
            "Epoch 7/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.4930 - accuracy: 0.3428\n",
            "Epoch 00007: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.4927 - accuracy: 0.3430 - val_loss: 1.9302 - val_accuracy: 0.2009\n",
            "Epoch 8/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.4764 - accuracy: 0.3529\n",
            "Epoch 00008: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.4764 - accuracy: 0.3530 - val_loss: 1.9321 - val_accuracy: 0.1960\n",
            "Epoch 9/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.4616 - accuracy: 0.3629\n",
            "Epoch 00009: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.4616 - accuracy: 0.3629 - val_loss: 1.9125 - val_accuracy: 0.2409\n",
            "Epoch 10/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.4445 - accuracy: 0.3710\n",
            "Epoch 00010: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.4443 - accuracy: 0.3712 - val_loss: 1.9148 - val_accuracy: 0.2254\n",
            "Epoch 11/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.4289 - accuracy: 0.3783\n",
            "Epoch 00011: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.4293 - accuracy: 0.3785 - val_loss: 1.8982 - val_accuracy: 0.2572\n",
            "Epoch 12/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.4136 - accuracy: 0.3850\n",
            "Epoch 00012: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.4136 - accuracy: 0.3850 - val_loss: 1.8933 - val_accuracy: 0.2551\n",
            "Epoch 13/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.4030 - accuracy: 0.3907\n",
            "Epoch 00013: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.4029 - accuracy: 0.3908 - val_loss: 1.8629 - val_accuracy: 0.3024\n",
            "Epoch 14/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.3852 - accuracy: 0.4009\n",
            "Epoch 00014: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3852 - accuracy: 0.4009 - val_loss: 1.8803 - val_accuracy: 0.2626\n",
            "Epoch 15/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.3835 - accuracy: 0.3946\n",
            "Epoch 00015: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.3833 - accuracy: 0.3948 - val_loss: 1.8637 - val_accuracy: 0.2854\n",
            "Epoch 16/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.3619 - accuracy: 0.4161\n",
            "Epoch 00016: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3617 - accuracy: 0.4162 - val_loss: 1.8645 - val_accuracy: 0.2700\n",
            "Epoch 17/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.3496 - accuracy: 0.4259\n",
            "Epoch 00017: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3500 - accuracy: 0.4257 - val_loss: 1.8357 - val_accuracy: 0.2982\n",
            "Epoch 18/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.3410 - accuracy: 0.4307\n",
            "Epoch 00018: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3411 - accuracy: 0.4308 - val_loss: 1.8191 - val_accuracy: 0.3044\n",
            "Epoch 19/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.3342 - accuracy: 0.4336\n",
            "Epoch 00019: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.3343 - accuracy: 0.4334 - val_loss: 1.8402 - val_accuracy: 0.2774\n",
            "Epoch 20/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.3268 - accuracy: 0.4382\n",
            "Epoch 00020: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.3267 - accuracy: 0.4383 - val_loss: 1.8181 - val_accuracy: 0.3079\n",
            "Epoch 21/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.3165 - accuracy: 0.4396\n",
            "Epoch 00021: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3161 - accuracy: 0.4396 - val_loss: 1.8308 - val_accuracy: 0.2736\n",
            "Epoch 22/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.3094 - accuracy: 0.4440\n",
            "Epoch 00022: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.3094 - accuracy: 0.4439 - val_loss: 1.8346 - val_accuracy: 0.2775\n",
            "Epoch 23/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.3003 - accuracy: 0.4489\n",
            "Epoch 00023: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.3004 - accuracy: 0.4489 - val_loss: 1.8457 - val_accuracy: 0.2646\n",
            "Epoch 24/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2955 - accuracy: 0.4506\n",
            "Epoch 00024: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2955 - accuracy: 0.4506 - val_loss: 1.8069 - val_accuracy: 0.2897\n",
            "Epoch 25/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2877 - accuracy: 0.4536\n",
            "Epoch 00025: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.2876 - accuracy: 0.4539 - val_loss: 1.7860 - val_accuracy: 0.3116\n",
            "Epoch 26/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2790 - accuracy: 0.4601\n",
            "Epoch 00026: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2792 - accuracy: 0.4602 - val_loss: 1.7852 - val_accuracy: 0.3038\n",
            "Epoch 27/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2722 - accuracy: 0.4653\n",
            "Epoch 00027: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2722 - accuracy: 0.4653 - val_loss: 1.7652 - val_accuracy: 0.3196\n",
            "Epoch 28/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.2707 - accuracy: 0.4598\n",
            "Epoch 00028: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2705 - accuracy: 0.4598 - val_loss: 1.7811 - val_accuracy: 0.3055\n",
            "Epoch 29/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.2591 - accuracy: 0.4691\n",
            "Epoch 00029: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.2591 - accuracy: 0.4691 - val_loss: 1.7617 - val_accuracy: 0.3172\n",
            "Epoch 30/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2507 - accuracy: 0.4724\n",
            "Epoch 00030: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2511 - accuracy: 0.4721 - val_loss: 1.7523 - val_accuracy: 0.3207\n",
            "Epoch 31/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.2446 - accuracy: 0.4761\n",
            "Epoch 00031: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2442 - accuracy: 0.4762 - val_loss: 1.8103 - val_accuracy: 0.2754\n",
            "Epoch 32/1000\n",
            "314/322 [============================>.] - ETA: 0s - loss: 1.2348 - accuracy: 0.4791\n",
            "Epoch 00032: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2346 - accuracy: 0.4791 - val_loss: 1.7461 - val_accuracy: 0.3206\n",
            "Epoch 33/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.2337 - accuracy: 0.4765\n",
            "Epoch 00033: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2337 - accuracy: 0.4766 - val_loss: 1.7522 - val_accuracy: 0.3173\n",
            "Epoch 34/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.2209 - accuracy: 0.4867\n",
            "Epoch 00034: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2208 - accuracy: 0.4868 - val_loss: 1.7554 - val_accuracy: 0.3045\n",
            "Epoch 35/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.2158 - accuracy: 0.4882\n",
            "Epoch 00035: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2157 - accuracy: 0.4881 - val_loss: 1.7406 - val_accuracy: 0.3174\n",
            "Epoch 36/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.2079 - accuracy: 0.4916\n",
            "Epoch 00036: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.2079 - accuracy: 0.4915 - val_loss: 1.7157 - val_accuracy: 0.3252\n",
            "Epoch 37/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1993 - accuracy: 0.4949\n",
            "Epoch 00037: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.1989 - accuracy: 0.4950 - val_loss: 1.7932 - val_accuracy: 0.2824\n",
            "Epoch 38/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1907 - accuracy: 0.4998\n",
            "Epoch 00038: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.1903 - accuracy: 0.5000 - val_loss: 1.7338 - val_accuracy: 0.3150\n",
            "Epoch 39/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1842 - accuracy: 0.5031\n",
            "Epoch 00039: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1842 - accuracy: 0.5032 - val_loss: 1.7618 - val_accuracy: 0.3030\n",
            "Epoch 40/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1750 - accuracy: 0.5055\n",
            "Epoch 00040: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1750 - accuracy: 0.5055 - val_loss: 1.7260 - val_accuracy: 0.3121\n",
            "Epoch 41/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.1710 - accuracy: 0.5064\n",
            "Epoch 00041: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1707 - accuracy: 0.5063 - val_loss: 1.7773 - val_accuracy: 0.2917\n",
            "Epoch 42/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1644 - accuracy: 0.5086\n",
            "Epoch 00042: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.1643 - accuracy: 0.5088 - val_loss: 1.7372 - val_accuracy: 0.3045\n",
            "Epoch 43/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1540 - accuracy: 0.5121\n",
            "Epoch 00043: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 8ms/step - loss: 1.1540 - accuracy: 0.5121 - val_loss: 1.7278 - val_accuracy: 0.3116\n",
            "Epoch 44/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1526 - accuracy: 0.5121\n",
            "Epoch 00044: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1528 - accuracy: 0.5117 - val_loss: 1.7200 - val_accuracy: 0.3266\n",
            "Epoch 45/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1471 - accuracy: 0.5128\n",
            "Epoch 00045: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1473 - accuracy: 0.5127 - val_loss: 1.7437 - val_accuracy: 0.3038\n",
            "Epoch 46/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1421 - accuracy: 0.5169\n",
            "Epoch 00046: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1424 - accuracy: 0.5166 - val_loss: 1.7327 - val_accuracy: 0.3090\n",
            "Epoch 47/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1378 - accuracy: 0.5157\n",
            "Epoch 00047: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1378 - accuracy: 0.5158 - val_loss: 1.7140 - val_accuracy: 0.3164\n",
            "Epoch 48/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1458 - accuracy: 0.5117\n",
            "Epoch 00048: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1451 - accuracy: 0.5121 - val_loss: 1.7065 - val_accuracy: 0.3247\n",
            "Epoch 49/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1262 - accuracy: 0.5218\n",
            "Epoch 00049: val_accuracy did not improve from 0.33616\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1257 - accuracy: 0.5221 - val_loss: 1.7625 - val_accuracy: 0.2839\n",
            "Epoch 50/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1268 - accuracy: 0.5196\n",
            "Epoch 00050: val_accuracy improved from 0.33616 to 0.34265, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.1271 - accuracy: 0.5194 - val_loss: 1.6699 - val_accuracy: 0.3426\n",
            "Epoch 51/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.1226 - accuracy: 0.5217\n",
            "Epoch 00051: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1226 - accuracy: 0.5217 - val_loss: 1.7014 - val_accuracy: 0.3305\n",
            "Epoch 52/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1155 - accuracy: 0.5248\n",
            "Epoch 00052: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1158 - accuracy: 0.5247 - val_loss: 1.7467 - val_accuracy: 0.3047\n",
            "Epoch 53/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1152 - accuracy: 0.5239\n",
            "Epoch 00053: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1150 - accuracy: 0.5244 - val_loss: 1.7553 - val_accuracy: 0.3008\n",
            "Epoch 54/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.1134 - accuracy: 0.5250\n",
            "Epoch 00054: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1135 - accuracy: 0.5249 - val_loss: 1.8383 - val_accuracy: 0.2650\n",
            "Epoch 55/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1143 - accuracy: 0.5238\n",
            "Epoch 00055: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1139 - accuracy: 0.5240 - val_loss: 1.7425 - val_accuracy: 0.3023\n",
            "Epoch 56/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1091 - accuracy: 0.5251\n",
            "Epoch 00056: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1091 - accuracy: 0.5251 - val_loss: 1.8112 - val_accuracy: 0.2833\n",
            "Epoch 57/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.1087 - accuracy: 0.5247\n",
            "Epoch 00057: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1087 - accuracy: 0.5247 - val_loss: 1.7893 - val_accuracy: 0.2870\n",
            "Epoch 58/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1016 - accuracy: 0.5274\n",
            "Epoch 00058: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1016 - accuracy: 0.5273 - val_loss: 1.7371 - val_accuracy: 0.3287\n",
            "Epoch 59/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1028 - accuracy: 0.5280\n",
            "Epoch 00059: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1025 - accuracy: 0.5282 - val_loss: 1.7039 - val_accuracy: 0.3314\n",
            "Epoch 60/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1065 - accuracy: 0.5256\n",
            "Epoch 00060: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1063 - accuracy: 0.5258 - val_loss: 1.7545 - val_accuracy: 0.3166\n",
            "Epoch 61/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.5305\n",
            "Epoch 00061: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0989 - accuracy: 0.5302 - val_loss: 1.7537 - val_accuracy: 0.3174\n",
            "Epoch 62/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.1012 - accuracy: 0.5273\n",
            "Epoch 00062: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1011 - accuracy: 0.5273 - val_loss: 1.7322 - val_accuracy: 0.3210\n",
            "Epoch 63/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.1086 - accuracy: 0.5229\n",
            "Epoch 00063: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.1086 - accuracy: 0.5226 - val_loss: 1.7256 - val_accuracy: 0.3263\n",
            "Epoch 64/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0950 - accuracy: 0.5307\n",
            "Epoch 00064: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0951 - accuracy: 0.5306 - val_loss: 1.7630 - val_accuracy: 0.3150\n",
            "Epoch 65/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.1000 - accuracy: 0.5272\n",
            "Epoch 00065: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0995 - accuracy: 0.5275 - val_loss: 1.7081 - val_accuracy: 0.3397\n",
            "Epoch 66/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0977 - accuracy: 0.5306\n",
            "Epoch 00066: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0977 - accuracy: 0.5305 - val_loss: 1.8074 - val_accuracy: 0.3050\n",
            "Epoch 67/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0893 - accuracy: 0.5333\n",
            "Epoch 00067: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0893 - accuracy: 0.5334 - val_loss: 1.7802 - val_accuracy: 0.3240\n",
            "Epoch 68/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0888 - accuracy: 0.5319\n",
            "Epoch 00068: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0888 - accuracy: 0.5319 - val_loss: 1.7571 - val_accuracy: 0.3230\n",
            "Epoch 69/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0951 - accuracy: 0.5291\n",
            "Epoch 00069: val_accuracy did not improve from 0.34265\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0951 - accuracy: 0.5291 - val_loss: 1.7035 - val_accuracy: 0.3191\n",
            "Epoch 70/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0878 - accuracy: 0.5354\n",
            "Epoch 00070: val_accuracy improved from 0.34265 to 0.34852, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0878 - accuracy: 0.5355 - val_loss: 1.7183 - val_accuracy: 0.3485\n",
            "Epoch 71/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0894 - accuracy: 0.5331\n",
            "Epoch 00071: val_accuracy did not improve from 0.34852\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0894 - accuracy: 0.5331 - val_loss: 1.7375 - val_accuracy: 0.3448\n",
            "Epoch 72/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0863 - accuracy: 0.5351\n",
            "Epoch 00072: val_accuracy improved from 0.34852 to 0.35331, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0866 - accuracy: 0.5349 - val_loss: 1.7403 - val_accuracy: 0.3533\n",
            "Epoch 73/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0913 - accuracy: 0.5324\n",
            "Epoch 00073: val_accuracy did not improve from 0.35331\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0913 - accuracy: 0.5325 - val_loss: 1.8358 - val_accuracy: 0.2903\n",
            "Epoch 74/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0879 - accuracy: 0.5342\n",
            "Epoch 00074: val_accuracy did not improve from 0.35331\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0878 - accuracy: 0.5343 - val_loss: 1.7928 - val_accuracy: 0.3203\n",
            "Epoch 75/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0849 - accuracy: 0.5358\n",
            "Epoch 00075: val_accuracy did not improve from 0.35331\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0849 - accuracy: 0.5359 - val_loss: 1.7491 - val_accuracy: 0.3360\n",
            "Epoch 76/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0819 - accuracy: 0.5401\n",
            "Epoch 00076: val_accuracy did not improve from 0.35331\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0823 - accuracy: 0.5395 - val_loss: 1.7563 - val_accuracy: 0.3506\n",
            "Epoch 77/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0857 - accuracy: 0.5364\n",
            "Epoch 00077: val_accuracy did not improve from 0.35331\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0857 - accuracy: 0.5363 - val_loss: 1.8557 - val_accuracy: 0.2995\n",
            "Epoch 78/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0810 - accuracy: 0.5376\n",
            "Epoch 00078: val_accuracy did not improve from 0.35331\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0811 - accuracy: 0.5375 - val_loss: 1.8357 - val_accuracy: 0.3152\n",
            "Epoch 79/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0883 - accuracy: 0.5318\n",
            "Epoch 00079: val_accuracy did not improve from 0.35331\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0873 - accuracy: 0.5325 - val_loss: 1.8257 - val_accuracy: 0.3064\n",
            "Epoch 80/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0805 - accuracy: 0.5365\n",
            "Epoch 00080: val_accuracy improved from 0.35331 to 0.36854, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0806 - accuracy: 0.5364 - val_loss: 1.7134 - val_accuracy: 0.3685\n",
            "Epoch 81/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0762 - accuracy: 0.5403\n",
            "Epoch 00081: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0762 - accuracy: 0.5403 - val_loss: 1.7568 - val_accuracy: 0.3483\n",
            "Epoch 82/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0754 - accuracy: 0.5414\n",
            "Epoch 00082: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0749 - accuracy: 0.5416 - val_loss: 1.7635 - val_accuracy: 0.3411\n",
            "Epoch 83/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0763 - accuracy: 0.5388\n",
            "Epoch 00083: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0765 - accuracy: 0.5386 - val_loss: 1.7517 - val_accuracy: 0.3586\n",
            "Epoch 84/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0767 - accuracy: 0.5396\n",
            "Epoch 00084: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0766 - accuracy: 0.5394 - val_loss: 1.7417 - val_accuracy: 0.3583\n",
            "Epoch 85/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0803 - accuracy: 0.5368\n",
            "Epoch 00085: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0799 - accuracy: 0.5370 - val_loss: 1.7338 - val_accuracy: 0.3561\n",
            "Epoch 86/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0729 - accuracy: 0.5396\n",
            "Epoch 00086: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0729 - accuracy: 0.5396 - val_loss: 1.7752 - val_accuracy: 0.3399\n",
            "Epoch 87/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0717 - accuracy: 0.5395\n",
            "Epoch 00087: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0718 - accuracy: 0.5394 - val_loss: 1.7769 - val_accuracy: 0.3349\n",
            "Epoch 88/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0722 - accuracy: 0.5406\n",
            "Epoch 00088: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0727 - accuracy: 0.5404 - val_loss: 1.7318 - val_accuracy: 0.3532\n",
            "Epoch 89/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0766 - accuracy: 0.5367\n",
            "Epoch 00089: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0760 - accuracy: 0.5370 - val_loss: 1.7838 - val_accuracy: 0.3373\n",
            "Epoch 90/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0734 - accuracy: 0.5371\n",
            "Epoch 00090: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0734 - accuracy: 0.5372 - val_loss: 1.8181 - val_accuracy: 0.3196\n",
            "Epoch 91/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0699 - accuracy: 0.5405\n",
            "Epoch 00091: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0696 - accuracy: 0.5408 - val_loss: 1.7810 - val_accuracy: 0.3418\n",
            "Epoch 92/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0730 - accuracy: 0.5402\n",
            "Epoch 00092: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0730 - accuracy: 0.5402 - val_loss: 1.7551 - val_accuracy: 0.3543\n",
            "Epoch 93/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0736 - accuracy: 0.5399\n",
            "Epoch 00093: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0732 - accuracy: 0.5401 - val_loss: 1.7515 - val_accuracy: 0.3511\n",
            "Epoch 94/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0809 - accuracy: 0.5366\n",
            "Epoch 00094: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0811 - accuracy: 0.5362 - val_loss: 1.7576 - val_accuracy: 0.3588\n",
            "Epoch 95/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0721 - accuracy: 0.5427\n",
            "Epoch 00095: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0720 - accuracy: 0.5427 - val_loss: 1.8036 - val_accuracy: 0.3406\n",
            "Epoch 96/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0659 - accuracy: 0.5442\n",
            "Epoch 00096: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0665 - accuracy: 0.5440 - val_loss: 1.7725 - val_accuracy: 0.3424\n",
            "Epoch 97/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0720 - accuracy: 0.5398\n",
            "Epoch 00097: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0718 - accuracy: 0.5399 - val_loss: 1.7779 - val_accuracy: 0.3524\n",
            "Epoch 98/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0652 - accuracy: 0.5432\n",
            "Epoch 00098: val_accuracy did not improve from 0.36854\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0652 - accuracy: 0.5434 - val_loss: 1.8142 - val_accuracy: 0.3297\n",
            "Epoch 99/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0671 - accuracy: 0.5429\n",
            "Epoch 00099: val_accuracy improved from 0.36854 to 0.36888, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0669 - accuracy: 0.5429 - val_loss: 1.7362 - val_accuracy: 0.3689\n",
            "Epoch 100/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0690 - accuracy: 0.5420\n",
            "Epoch 00100: val_accuracy did not improve from 0.36888\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0690 - accuracy: 0.5420 - val_loss: 1.8170 - val_accuracy: 0.3357\n",
            "Epoch 101/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0640 - accuracy: 0.5439\n",
            "Epoch 00101: val_accuracy did not improve from 0.36888\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0638 - accuracy: 0.5440 - val_loss: 1.7265 - val_accuracy: 0.3677\n",
            "Epoch 102/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0642 - accuracy: 0.5455\n",
            "Epoch 00102: val_accuracy improved from 0.36888 to 0.38398, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0632 - accuracy: 0.5460 - val_loss: 1.7328 - val_accuracy: 0.3840\n",
            "Epoch 103/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0720 - accuracy: 0.5412\n",
            "Epoch 00103: val_accuracy did not improve from 0.38398\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0720 - accuracy: 0.5409 - val_loss: 1.7587 - val_accuracy: 0.3646\n",
            "Epoch 104/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0686 - accuracy: 0.5454\n",
            "Epoch 00104: val_accuracy improved from 0.38398 to 0.40537, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0686 - accuracy: 0.5454 - val_loss: 1.7029 - val_accuracy: 0.4054\n",
            "Epoch 105/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0687 - accuracy: 0.5418\n",
            "Epoch 00105: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0687 - accuracy: 0.5417 - val_loss: 1.8023 - val_accuracy: 0.3414\n",
            "Epoch 106/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0633 - accuracy: 0.5451\n",
            "Epoch 00106: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0635 - accuracy: 0.5451 - val_loss: 1.7427 - val_accuracy: 0.3662\n",
            "Epoch 107/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0633 - accuracy: 0.5453\n",
            "Epoch 00107: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0634 - accuracy: 0.5452 - val_loss: 1.8324 - val_accuracy: 0.3267\n",
            "Epoch 108/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0628 - accuracy: 0.5471\n",
            "Epoch 00108: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0627 - accuracy: 0.5472 - val_loss: 1.8067 - val_accuracy: 0.3554\n",
            "Epoch 109/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0675 - accuracy: 0.5453\n",
            "Epoch 00109: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0674 - accuracy: 0.5454 - val_loss: 1.7252 - val_accuracy: 0.3981\n",
            "Epoch 110/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0658 - accuracy: 0.5465\n",
            "Epoch 00110: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0660 - accuracy: 0.5464 - val_loss: 1.7462 - val_accuracy: 0.3737\n",
            "Epoch 111/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0634 - accuracy: 0.5442\n",
            "Epoch 00111: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0633 - accuracy: 0.5442 - val_loss: 1.7767 - val_accuracy: 0.3690\n",
            "Epoch 112/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0678 - accuracy: 0.5446\n",
            "Epoch 00112: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0683 - accuracy: 0.5443 - val_loss: 1.7416 - val_accuracy: 0.3791\n",
            "Epoch 113/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0590 - accuracy: 0.5478\n",
            "Epoch 00113: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0594 - accuracy: 0.5476 - val_loss: 1.7582 - val_accuracy: 0.3754\n",
            "Epoch 114/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0667 - accuracy: 0.5472\n",
            "Epoch 00114: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0667 - accuracy: 0.5472 - val_loss: 1.8326 - val_accuracy: 0.3351\n",
            "Epoch 115/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0600 - accuracy: 0.5480\n",
            "Epoch 00115: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0598 - accuracy: 0.5483 - val_loss: 1.8547 - val_accuracy: 0.3330\n",
            "Epoch 116/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0671 - accuracy: 0.5462\n",
            "Epoch 00116: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0670 - accuracy: 0.5462 - val_loss: 1.7443 - val_accuracy: 0.3829\n",
            "Epoch 117/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0605 - accuracy: 0.5465\n",
            "Epoch 00117: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0607 - accuracy: 0.5466 - val_loss: 1.7617 - val_accuracy: 0.3728\n",
            "Epoch 118/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0603 - accuracy: 0.5481\n",
            "Epoch 00118: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0603 - accuracy: 0.5481 - val_loss: 1.7189 - val_accuracy: 0.3980\n",
            "Epoch 119/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0613 - accuracy: 0.5479\n",
            "Epoch 00119: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0623 - accuracy: 0.5475 - val_loss: 1.7654 - val_accuracy: 0.3569\n",
            "Epoch 120/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0627 - accuracy: 0.5474\n",
            "Epoch 00120: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0625 - accuracy: 0.5474 - val_loss: 1.7971 - val_accuracy: 0.3640\n",
            "Epoch 121/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0548 - accuracy: 0.5494\n",
            "Epoch 00121: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0549 - accuracy: 0.5494 - val_loss: 1.7691 - val_accuracy: 0.3719\n",
            "Epoch 122/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0569 - accuracy: 0.5499\n",
            "Epoch 00122: val_accuracy did not improve from 0.40537\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0568 - accuracy: 0.5499 - val_loss: 1.7609 - val_accuracy: 0.3736\n",
            "Epoch 123/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0672 - accuracy: 0.5411\n",
            "Epoch 00123: val_accuracy improved from 0.40537 to 0.40913, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0672 - accuracy: 0.5411 - val_loss: 1.7162 - val_accuracy: 0.4091\n",
            "Epoch 124/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0520 - accuracy: 0.5509\n",
            "Epoch 00124: val_accuracy did not improve from 0.40913\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0527 - accuracy: 0.5506 - val_loss: 1.7448 - val_accuracy: 0.3871\n",
            "Epoch 125/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0553 - accuracy: 0.5485\n",
            "Epoch 00125: val_accuracy improved from 0.40913 to 0.41617, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0550 - accuracy: 0.5487 - val_loss: 1.6490 - val_accuracy: 0.4162\n",
            "Epoch 126/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0615 - accuracy: 0.5477\n",
            "Epoch 00126: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0615 - accuracy: 0.5477 - val_loss: 1.7714 - val_accuracy: 0.3810\n",
            "Epoch 127/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0520 - accuracy: 0.5530\n",
            "Epoch 00127: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0519 - accuracy: 0.5529 - val_loss: 1.7444 - val_accuracy: 0.3966\n",
            "Epoch 128/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0677 - accuracy: 0.5400\n",
            "Epoch 00128: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0679 - accuracy: 0.5400 - val_loss: 1.7877 - val_accuracy: 0.3558\n",
            "Epoch 129/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0567 - accuracy: 0.5479\n",
            "Epoch 00129: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0573 - accuracy: 0.5478 - val_loss: 1.8114 - val_accuracy: 0.3644\n",
            "Epoch 130/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0621 - accuracy: 0.5445\n",
            "Epoch 00130: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0626 - accuracy: 0.5441 - val_loss: 1.7403 - val_accuracy: 0.3961\n",
            "Epoch 131/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0603 - accuracy: 0.5478\n",
            "Epoch 00131: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0604 - accuracy: 0.5477 - val_loss: 1.7840 - val_accuracy: 0.3690\n",
            "Epoch 132/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0644 - accuracy: 0.5447\n",
            "Epoch 00132: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0643 - accuracy: 0.5448 - val_loss: 1.7642 - val_accuracy: 0.3811\n",
            "Epoch 133/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0576 - accuracy: 0.5476\n",
            "Epoch 00133: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0576 - accuracy: 0.5476 - val_loss: 1.7701 - val_accuracy: 0.3766\n",
            "Epoch 134/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0489 - accuracy: 0.5515\n",
            "Epoch 00134: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0490 - accuracy: 0.5513 - val_loss: 1.7764 - val_accuracy: 0.3779\n",
            "Epoch 135/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0550 - accuracy: 0.5479\n",
            "Epoch 00135: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0553 - accuracy: 0.5476 - val_loss: 1.7572 - val_accuracy: 0.3905\n",
            "Epoch 136/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0596 - accuracy: 0.5471\n",
            "Epoch 00136: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0600 - accuracy: 0.5469 - val_loss: 1.7390 - val_accuracy: 0.3929\n",
            "Epoch 137/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0563 - accuracy: 0.5489\n",
            "Epoch 00137: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0559 - accuracy: 0.5491 - val_loss: 1.7836 - val_accuracy: 0.3682\n",
            "Epoch 138/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0524 - accuracy: 0.5495\n",
            "Epoch 00138: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0523 - accuracy: 0.5495 - val_loss: 1.7499 - val_accuracy: 0.3928\n",
            "Epoch 139/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0569 - accuracy: 0.5493\n",
            "Epoch 00139: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0573 - accuracy: 0.5492 - val_loss: 1.7385 - val_accuracy: 0.3864\n",
            "Epoch 140/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0543 - accuracy: 0.5490\n",
            "Epoch 00140: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0544 - accuracy: 0.5489 - val_loss: 1.7456 - val_accuracy: 0.3981\n",
            "Epoch 141/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0579 - accuracy: 0.5481\n",
            "Epoch 00141: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0580 - accuracy: 0.5480 - val_loss: 1.8201 - val_accuracy: 0.3506\n",
            "Epoch 142/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0524 - accuracy: 0.5494\n",
            "Epoch 00142: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0521 - accuracy: 0.5494 - val_loss: 1.8324 - val_accuracy: 0.3541\n",
            "Epoch 143/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0525 - accuracy: 0.5484\n",
            "Epoch 00143: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0523 - accuracy: 0.5483 - val_loss: 1.7635 - val_accuracy: 0.3774\n",
            "Epoch 144/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0552 - accuracy: 0.5493\n",
            "Epoch 00144: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0553 - accuracy: 0.5493 - val_loss: 1.8543 - val_accuracy: 0.3381\n",
            "Epoch 145/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0537 - accuracy: 0.5495\n",
            "Epoch 00145: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0537 - accuracy: 0.5495 - val_loss: 1.7994 - val_accuracy: 0.3670\n",
            "Epoch 146/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0438 - accuracy: 0.5535\n",
            "Epoch 00146: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0438 - accuracy: 0.5535 - val_loss: 1.7591 - val_accuracy: 0.3887\n",
            "Epoch 147/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0553 - accuracy: 0.5497\n",
            "Epoch 00147: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0558 - accuracy: 0.5490 - val_loss: 1.7819 - val_accuracy: 0.3659\n",
            "Epoch 148/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0570 - accuracy: 0.5454\n",
            "Epoch 00148: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0570 - accuracy: 0.5454 - val_loss: 1.7437 - val_accuracy: 0.3923\n",
            "Epoch 149/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0483 - accuracy: 0.5518\n",
            "Epoch 00149: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0478 - accuracy: 0.5520 - val_loss: 1.7789 - val_accuracy: 0.3763\n",
            "Epoch 150/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0467 - accuracy: 0.5527\n",
            "Epoch 00150: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0468 - accuracy: 0.5524 - val_loss: 1.7814 - val_accuracy: 0.3822\n",
            "Epoch 151/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0507 - accuracy: 0.5512\n",
            "Epoch 00151: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0507 - accuracy: 0.5512 - val_loss: 1.7930 - val_accuracy: 0.3690\n",
            "Epoch 152/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0480 - accuracy: 0.5530\n",
            "Epoch 00152: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0479 - accuracy: 0.5530 - val_loss: 1.8726 - val_accuracy: 0.3334\n",
            "Epoch 153/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0497 - accuracy: 0.5515\n",
            "Epoch 00153: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0497 - accuracy: 0.5515 - val_loss: 1.8625 - val_accuracy: 0.3339\n",
            "Epoch 154/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0494 - accuracy: 0.5509\n",
            "Epoch 00154: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0493 - accuracy: 0.5511 - val_loss: 1.7746 - val_accuracy: 0.3741\n",
            "Epoch 155/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0539 - accuracy: 0.5505\n",
            "Epoch 00155: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0534 - accuracy: 0.5508 - val_loss: 1.8037 - val_accuracy: 0.3755\n",
            "Epoch 156/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0487 - accuracy: 0.5511\n",
            "Epoch 00156: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0484 - accuracy: 0.5512 - val_loss: 1.7963 - val_accuracy: 0.3786\n",
            "Epoch 157/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0502 - accuracy: 0.5506\n",
            "Epoch 00157: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0503 - accuracy: 0.5507 - val_loss: 1.7354 - val_accuracy: 0.4052\n",
            "Epoch 158/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0573 - accuracy: 0.5476\n",
            "Epoch 00158: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0576 - accuracy: 0.5473 - val_loss: 1.7169 - val_accuracy: 0.4022\n",
            "Epoch 159/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0534 - accuracy: 0.5518\n",
            "Epoch 00159: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0536 - accuracy: 0.5518 - val_loss: 1.7434 - val_accuracy: 0.3963\n",
            "Epoch 160/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0497 - accuracy: 0.5508\n",
            "Epoch 00160: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0496 - accuracy: 0.5509 - val_loss: 1.7393 - val_accuracy: 0.3950\n",
            "Epoch 161/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0612 - accuracy: 0.5455\n",
            "Epoch 00161: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0617 - accuracy: 0.5453 - val_loss: 1.7241 - val_accuracy: 0.4018\n",
            "Epoch 162/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0431 - accuracy: 0.5535\n",
            "Epoch 00162: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0434 - accuracy: 0.5535 - val_loss: 1.7920 - val_accuracy: 0.3808\n",
            "Epoch 163/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0550 - accuracy: 0.5475\n",
            "Epoch 00163: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0549 - accuracy: 0.5475 - val_loss: 1.7327 - val_accuracy: 0.4006\n",
            "Epoch 164/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0455 - accuracy: 0.5530\n",
            "Epoch 00164: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0454 - accuracy: 0.5532 - val_loss: 1.8088 - val_accuracy: 0.3686\n",
            "Epoch 165/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0510 - accuracy: 0.5486\n",
            "Epoch 00165: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0512 - accuracy: 0.5488 - val_loss: 1.7365 - val_accuracy: 0.4004\n",
            "Epoch 166/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0391 - accuracy: 0.5560\n",
            "Epoch 00166: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0396 - accuracy: 0.5558 - val_loss: 1.7468 - val_accuracy: 0.3953\n",
            "Epoch 167/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0480 - accuracy: 0.5518\n",
            "Epoch 00167: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0476 - accuracy: 0.5519 - val_loss: 1.7463 - val_accuracy: 0.3987\n",
            "Epoch 168/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0527 - accuracy: 0.5504\n",
            "Epoch 00168: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0531 - accuracy: 0.5503 - val_loss: 1.7674 - val_accuracy: 0.3785\n",
            "Epoch 169/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0502 - accuracy: 0.5526\n",
            "Epoch 00169: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0501 - accuracy: 0.5527 - val_loss: 1.7413 - val_accuracy: 0.3948\n",
            "Epoch 170/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0433 - accuracy: 0.5520\n",
            "Epoch 00170: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0432 - accuracy: 0.5521 - val_loss: 1.7964 - val_accuracy: 0.3711\n",
            "Epoch 171/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0515 - accuracy: 0.5530\n",
            "Epoch 00171: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0516 - accuracy: 0.5531 - val_loss: 1.7543 - val_accuracy: 0.3877\n",
            "Epoch 172/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0468 - accuracy: 0.5546\n",
            "Epoch 00172: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0470 - accuracy: 0.5543 - val_loss: 1.7413 - val_accuracy: 0.3929\n",
            "Epoch 173/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0511 - accuracy: 0.5498\n",
            "Epoch 00173: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0514 - accuracy: 0.5496 - val_loss: 1.6908 - val_accuracy: 0.4117\n",
            "Epoch 174/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0462 - accuracy: 0.5527\n",
            "Epoch 00174: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0465 - accuracy: 0.5527 - val_loss: 1.7469 - val_accuracy: 0.3880\n",
            "Epoch 175/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0499 - accuracy: 0.5512\n",
            "Epoch 00175: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0500 - accuracy: 0.5510 - val_loss: 1.7916 - val_accuracy: 0.3737\n",
            "Epoch 176/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.5553\n",
            "Epoch 00176: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0387 - accuracy: 0.5553 - val_loss: 1.7526 - val_accuracy: 0.3848\n",
            "Epoch 177/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0436 - accuracy: 0.5538\n",
            "Epoch 00177: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0438 - accuracy: 0.5537 - val_loss: 1.7195 - val_accuracy: 0.4054\n",
            "Epoch 178/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0429 - accuracy: 0.5536\n",
            "Epoch 00178: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0428 - accuracy: 0.5535 - val_loss: 1.7150 - val_accuracy: 0.4119\n",
            "Epoch 179/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0415 - accuracy: 0.5556\n",
            "Epoch 00179: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0414 - accuracy: 0.5557 - val_loss: 1.7441 - val_accuracy: 0.3926\n",
            "Epoch 180/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0406 - accuracy: 0.5543\n",
            "Epoch 00180: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0406 - accuracy: 0.5543 - val_loss: 1.7315 - val_accuracy: 0.4074\n",
            "Epoch 181/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.5557\n",
            "Epoch 00181: val_accuracy did not improve from 0.41617\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0383 - accuracy: 0.5558 - val_loss: 1.7231 - val_accuracy: 0.3923\n",
            "Epoch 182/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0498 - accuracy: 0.5497\n",
            "Epoch 00182: val_accuracy improved from 0.41617 to 0.42033, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0501 - accuracy: 0.5494 - val_loss: 1.6837 - val_accuracy: 0.4203\n",
            "Epoch 183/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0498 - accuracy: 0.5504\n",
            "Epoch 00183: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0496 - accuracy: 0.5503 - val_loss: 1.7507 - val_accuracy: 0.3804\n",
            "Epoch 184/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0525 - accuracy: 0.5457\n",
            "Epoch 00184: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0521 - accuracy: 0.5461 - val_loss: 1.7384 - val_accuracy: 0.4015\n",
            "Epoch 185/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0387 - accuracy: 0.5569\n",
            "Epoch 00185: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0386 - accuracy: 0.5569 - val_loss: 1.7598 - val_accuracy: 0.3856\n",
            "Epoch 186/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0417 - accuracy: 0.5530\n",
            "Epoch 00186: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0417 - accuracy: 0.5530 - val_loss: 1.7108 - val_accuracy: 0.4034\n",
            "Epoch 187/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0375 - accuracy: 0.5558\n",
            "Epoch 00187: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0379 - accuracy: 0.5557 - val_loss: 1.7965 - val_accuracy: 0.3718\n",
            "Epoch 188/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0428 - accuracy: 0.5556\n",
            "Epoch 00188: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0427 - accuracy: 0.5559 - val_loss: 1.7558 - val_accuracy: 0.3851\n",
            "Epoch 189/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0500 - accuracy: 0.5515\n",
            "Epoch 00189: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0500 - accuracy: 0.5512 - val_loss: 1.8342 - val_accuracy: 0.3666\n",
            "Epoch 190/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0572 - accuracy: 0.5467\n",
            "Epoch 00190: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0568 - accuracy: 0.5468 - val_loss: 1.7248 - val_accuracy: 0.4081\n",
            "Epoch 191/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0426 - accuracy: 0.5536\n",
            "Epoch 00191: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0426 - accuracy: 0.5537 - val_loss: 1.7340 - val_accuracy: 0.3912\n",
            "Epoch 192/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0390 - accuracy: 0.5555\n",
            "Epoch 00192: val_accuracy did not improve from 0.42033\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0390 - accuracy: 0.5553 - val_loss: 1.7484 - val_accuracy: 0.3923\n",
            "Epoch 193/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0382 - accuracy: 0.5547\n",
            "Epoch 00193: val_accuracy improved from 0.42033 to 0.43058, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0382 - accuracy: 0.5547 - val_loss: 1.6709 - val_accuracy: 0.4306\n",
            "Epoch 194/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0529 - accuracy: 0.5476\n",
            "Epoch 00194: val_accuracy did not improve from 0.43058\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0531 - accuracy: 0.5477 - val_loss: 1.7514 - val_accuracy: 0.3931\n",
            "Epoch 195/1000\n",
            "315/322 [============================>.] - ETA: 0s - loss: 1.0388 - accuracy: 0.5564\n",
            "Epoch 00195: val_accuracy did not improve from 0.43058\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0381 - accuracy: 0.5568 - val_loss: 1.7043 - val_accuracy: 0.4071\n",
            "Epoch 196/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0380 - accuracy: 0.5549\n",
            "Epoch 00196: val_accuracy did not improve from 0.43058\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0380 - accuracy: 0.5549 - val_loss: 1.7890 - val_accuracy: 0.3612\n",
            "Epoch 197/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0517 - accuracy: 0.5533\n",
            "Epoch 00197: val_accuracy did not improve from 0.43058\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0517 - accuracy: 0.5534 - val_loss: 1.7057 - val_accuracy: 0.4118\n",
            "Epoch 198/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0419 - accuracy: 0.5544\n",
            "Epoch 00198: val_accuracy did not improve from 0.43058\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0422 - accuracy: 0.5542 - val_loss: 1.6991 - val_accuracy: 0.4222\n",
            "Epoch 199/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0380 - accuracy: 0.5542\n",
            "Epoch 00199: val_accuracy did not improve from 0.43058\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0381 - accuracy: 0.5542 - val_loss: 1.7374 - val_accuracy: 0.3948\n",
            "Epoch 200/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0435 - accuracy: 0.5523\n",
            "Epoch 00200: val_accuracy improved from 0.43058 to 0.44049, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0436 - accuracy: 0.5522 - val_loss: 1.6504 - val_accuracy: 0.4405\n",
            "Epoch 201/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0375 - accuracy: 0.5567\n",
            "Epoch 00201: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0375 - accuracy: 0.5567 - val_loss: 1.7119 - val_accuracy: 0.4029\n",
            "Epoch 202/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0480 - accuracy: 0.5501\n",
            "Epoch 00202: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0479 - accuracy: 0.5501 - val_loss: 1.7263 - val_accuracy: 0.4035\n",
            "Epoch 203/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0445 - accuracy: 0.5508\n",
            "Epoch 00203: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0445 - accuracy: 0.5508 - val_loss: 1.7228 - val_accuracy: 0.3996\n",
            "Epoch 204/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0423 - accuracy: 0.5521\n",
            "Epoch 00204: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0428 - accuracy: 0.5521 - val_loss: 1.7359 - val_accuracy: 0.3985\n",
            "Epoch 205/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0470 - accuracy: 0.5516\n",
            "Epoch 00205: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0471 - accuracy: 0.5515 - val_loss: 1.7881 - val_accuracy: 0.3690\n",
            "Epoch 206/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.5582\n",
            "Epoch 00206: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0341 - accuracy: 0.5582 - val_loss: 1.8183 - val_accuracy: 0.3570\n",
            "Epoch 207/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0368 - accuracy: 0.5543\n",
            "Epoch 00207: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0373 - accuracy: 0.5540 - val_loss: 1.7445 - val_accuracy: 0.3920\n",
            "Epoch 208/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0458 - accuracy: 0.5490\n",
            "Epoch 00208: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0456 - accuracy: 0.5490 - val_loss: 1.8108 - val_accuracy: 0.3575\n",
            "Epoch 209/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.5547\n",
            "Epoch 00209: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0368 - accuracy: 0.5545 - val_loss: 1.6816 - val_accuracy: 0.4090\n",
            "Epoch 210/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0374 - accuracy: 0.5566\n",
            "Epoch 00210: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0373 - accuracy: 0.5566 - val_loss: 1.7309 - val_accuracy: 0.3962\n",
            "Epoch 211/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0425 - accuracy: 0.5507\n",
            "Epoch 00211: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0425 - accuracy: 0.5507 - val_loss: 1.8309 - val_accuracy: 0.3536\n",
            "Epoch 212/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0408 - accuracy: 0.5570\n",
            "Epoch 00212: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0407 - accuracy: 0.5571 - val_loss: 1.8065 - val_accuracy: 0.3657\n",
            "Epoch 213/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0435 - accuracy: 0.5508\n",
            "Epoch 00213: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0436 - accuracy: 0.5509 - val_loss: 1.7305 - val_accuracy: 0.3951\n",
            "Epoch 214/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0368 - accuracy: 0.5555\n",
            "Epoch 00214: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0370 - accuracy: 0.5554 - val_loss: 1.7961 - val_accuracy: 0.3672\n",
            "Epoch 215/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0357 - accuracy: 0.5565\n",
            "Epoch 00215: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0357 - accuracy: 0.5565 - val_loss: 1.7442 - val_accuracy: 0.3834\n",
            "Epoch 216/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0367 - accuracy: 0.5562\n",
            "Epoch 00216: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0368 - accuracy: 0.5560 - val_loss: 1.7484 - val_accuracy: 0.3864\n",
            "Epoch 217/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0378 - accuracy: 0.5539\n",
            "Epoch 00217: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0375 - accuracy: 0.5540 - val_loss: 1.7206 - val_accuracy: 0.4056\n",
            "Epoch 218/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0424 - accuracy: 0.5525\n",
            "Epoch 00218: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0429 - accuracy: 0.5522 - val_loss: 1.7024 - val_accuracy: 0.3994\n",
            "Epoch 219/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0383 - accuracy: 0.5548\n",
            "Epoch 00219: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 9ms/step - loss: 1.0383 - accuracy: 0.5548 - val_loss: 1.8326 - val_accuracy: 0.3569\n",
            "Epoch 220/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.5528\n",
            "Epoch 00220: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0385 - accuracy: 0.5528 - val_loss: 1.7250 - val_accuracy: 0.3955\n",
            "Epoch 221/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.5555\n",
            "Epoch 00221: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0337 - accuracy: 0.5553 - val_loss: 1.7509 - val_accuracy: 0.3752\n",
            "Epoch 222/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0410 - accuracy: 0.5509\n",
            "Epoch 00222: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0405 - accuracy: 0.5510 - val_loss: 1.7803 - val_accuracy: 0.3674\n",
            "Epoch 223/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0403 - accuracy: 0.5554\n",
            "Epoch 00223: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0407 - accuracy: 0.5553 - val_loss: 1.7217 - val_accuracy: 0.4045\n",
            "Epoch 224/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.5555\n",
            "Epoch 00224: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0370 - accuracy: 0.5554 - val_loss: 1.7900 - val_accuracy: 0.3644\n",
            "Epoch 225/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.5549\n",
            "Epoch 00225: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0352 - accuracy: 0.5549 - val_loss: 1.7551 - val_accuracy: 0.3711\n",
            "Epoch 226/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0334 - accuracy: 0.5572\n",
            "Epoch 00226: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0336 - accuracy: 0.5570 - val_loss: 1.7132 - val_accuracy: 0.3979\n",
            "Epoch 227/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.5584\n",
            "Epoch 00227: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0344 - accuracy: 0.5585 - val_loss: 1.7391 - val_accuracy: 0.3938\n",
            "Epoch 228/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.5565\n",
            "Epoch 00228: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0348 - accuracy: 0.5564 - val_loss: 1.7886 - val_accuracy: 0.3700\n",
            "Epoch 229/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0397 - accuracy: 0.5535\n",
            "Epoch 00229: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0395 - accuracy: 0.5536 - val_loss: 1.7499 - val_accuracy: 0.3940\n",
            "Epoch 230/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0443 - accuracy: 0.5498\n",
            "Epoch 00230: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0442 - accuracy: 0.5497 - val_loss: 1.6990 - val_accuracy: 0.4134\n",
            "Epoch 231/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.5529\n",
            "Epoch 00231: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0389 - accuracy: 0.5529 - val_loss: 1.7333 - val_accuracy: 0.3785\n",
            "Epoch 232/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.5584\n",
            "Epoch 00232: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0345 - accuracy: 0.5587 - val_loss: 1.8802 - val_accuracy: 0.3172\n",
            "Epoch 233/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0431 - accuracy: 0.5506\n",
            "Epoch 00233: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0431 - accuracy: 0.5506 - val_loss: 1.6628 - val_accuracy: 0.4171\n",
            "Epoch 234/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.5566\n",
            "Epoch 00234: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0354 - accuracy: 0.5565 - val_loss: 1.7466 - val_accuracy: 0.3782\n",
            "Epoch 235/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0302 - accuracy: 0.5573\n",
            "Epoch 00235: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0308 - accuracy: 0.5569 - val_loss: 1.7336 - val_accuracy: 0.3897\n",
            "Epoch 236/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.5562\n",
            "Epoch 00236: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0334 - accuracy: 0.5558 - val_loss: 1.7642 - val_accuracy: 0.3734\n",
            "Epoch 237/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0488 - accuracy: 0.5472\n",
            "Epoch 00237: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0489 - accuracy: 0.5472 - val_loss: 1.7937 - val_accuracy: 0.3618\n",
            "Epoch 238/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0325 - accuracy: 0.5542\n",
            "Epoch 00238: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0321 - accuracy: 0.5544 - val_loss: 1.7567 - val_accuracy: 0.3780\n",
            "Epoch 239/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0425 - accuracy: 0.5549\n",
            "Epoch 00239: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0421 - accuracy: 0.5551 - val_loss: 1.7195 - val_accuracy: 0.3948\n",
            "Epoch 240/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.5557\n",
            "Epoch 00240: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0354 - accuracy: 0.5557 - val_loss: 1.7552 - val_accuracy: 0.3728\n",
            "Epoch 241/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0277 - accuracy: 0.5598\n",
            "Epoch 00241: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0277 - accuracy: 0.5598 - val_loss: 1.7152 - val_accuracy: 0.3990\n",
            "Epoch 242/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0421 - accuracy: 0.5545\n",
            "Epoch 00242: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0420 - accuracy: 0.5547 - val_loss: 1.7615 - val_accuracy: 0.3813\n",
            "Epoch 243/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0322 - accuracy: 0.5560\n",
            "Epoch 00243: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0320 - accuracy: 0.5560 - val_loss: 1.8234 - val_accuracy: 0.3546\n",
            "Epoch 244/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0404 - accuracy: 0.5553\n",
            "Epoch 00244: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0405 - accuracy: 0.5552 - val_loss: 1.7402 - val_accuracy: 0.3541\n",
            "Epoch 245/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0409 - accuracy: 0.5562\n",
            "Epoch 00245: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0407 - accuracy: 0.5562 - val_loss: 1.7432 - val_accuracy: 0.3700\n",
            "Epoch 246/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0374 - accuracy: 0.5553\n",
            "Epoch 00246: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0373 - accuracy: 0.5554 - val_loss: 1.6927 - val_accuracy: 0.4052\n",
            "Epoch 247/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.5565\n",
            "Epoch 00247: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0343 - accuracy: 0.5565 - val_loss: 1.6968 - val_accuracy: 0.4141\n",
            "Epoch 248/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0378 - accuracy: 0.5560\n",
            "Epoch 00248: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0378 - accuracy: 0.5560 - val_loss: 1.7117 - val_accuracy: 0.3884\n",
            "Epoch 249/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.5596\n",
            "Epoch 00249: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0306 - accuracy: 0.5597 - val_loss: 1.7548 - val_accuracy: 0.3772\n",
            "Epoch 250/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0320 - accuracy: 0.5575\n",
            "Epoch 00250: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0319 - accuracy: 0.5576 - val_loss: 1.8081 - val_accuracy: 0.3718\n",
            "Epoch 251/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.5551\n",
            "Epoch 00251: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0363 - accuracy: 0.5552 - val_loss: 1.7434 - val_accuracy: 0.3711\n",
            "Epoch 252/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0414 - accuracy: 0.5508\n",
            "Epoch 00252: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0412 - accuracy: 0.5508 - val_loss: 1.7103 - val_accuracy: 0.3922\n",
            "Epoch 253/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.5537\n",
            "Epoch 00253: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0351 - accuracy: 0.5537 - val_loss: 1.6812 - val_accuracy: 0.4048\n",
            "Epoch 254/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.5601\n",
            "Epoch 00254: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0289 - accuracy: 0.5600 - val_loss: 1.6848 - val_accuracy: 0.4145\n",
            "Epoch 255/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.5529\n",
            "Epoch 00255: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0377 - accuracy: 0.5531 - val_loss: 1.7867 - val_accuracy: 0.3619\n",
            "Epoch 256/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.5602\n",
            "Epoch 00256: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0262 - accuracy: 0.5600 - val_loss: 1.6812 - val_accuracy: 0.4027\n",
            "Epoch 257/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0324 - accuracy: 0.5556\n",
            "Epoch 00257: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0324 - accuracy: 0.5557 - val_loss: 1.7345 - val_accuracy: 0.3873\n",
            "Epoch 258/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0309 - accuracy: 0.5557\n",
            "Epoch 00258: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0307 - accuracy: 0.5555 - val_loss: 1.7131 - val_accuracy: 0.4051\n",
            "Epoch 259/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0314 - accuracy: 0.5556\n",
            "Epoch 00259: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0310 - accuracy: 0.5558 - val_loss: 1.7622 - val_accuracy: 0.3676\n",
            "Epoch 260/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.5539\n",
            "Epoch 00260: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0358 - accuracy: 0.5538 - val_loss: 1.8334 - val_accuracy: 0.3414\n",
            "Epoch 261/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0361 - accuracy: 0.5565\n",
            "Epoch 00261: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0360 - accuracy: 0.5566 - val_loss: 1.8144 - val_accuracy: 0.3533\n",
            "Epoch 262/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0303 - accuracy: 0.5575\n",
            "Epoch 00262: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0295 - accuracy: 0.5577 - val_loss: 1.7261 - val_accuracy: 0.3938\n",
            "Epoch 263/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0385 - accuracy: 0.5534\n",
            "Epoch 00263: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0381 - accuracy: 0.5536 - val_loss: 1.7295 - val_accuracy: 0.3854\n",
            "Epoch 264/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0389 - accuracy: 0.5539\n",
            "Epoch 00264: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0387 - accuracy: 0.5539 - val_loss: 1.8134 - val_accuracy: 0.3533\n",
            "Epoch 265/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0295 - accuracy: 0.5562\n",
            "Epoch 00265: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0293 - accuracy: 0.5561 - val_loss: 1.7830 - val_accuracy: 0.3644\n",
            "Epoch 266/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0293 - accuracy: 0.5586\n",
            "Epoch 00266: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0293 - accuracy: 0.5585 - val_loss: 1.7106 - val_accuracy: 0.3934\n",
            "Epoch 267/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.5538\n",
            "Epoch 00267: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0339 - accuracy: 0.5538 - val_loss: 1.8278 - val_accuracy: 0.3483\n",
            "Epoch 268/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0324 - accuracy: 0.5550\n",
            "Epoch 00268: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0325 - accuracy: 0.5550 - val_loss: 1.6870 - val_accuracy: 0.4130\n",
            "Epoch 269/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.5555\n",
            "Epoch 00269: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0327 - accuracy: 0.5555 - val_loss: 1.7176 - val_accuracy: 0.3858\n",
            "Epoch 270/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.5541\n",
            "Epoch 00270: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0348 - accuracy: 0.5541 - val_loss: 1.6872 - val_accuracy: 0.4072\n",
            "Epoch 271/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.5577\n",
            "Epoch 00271: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0287 - accuracy: 0.5576 - val_loss: 1.7379 - val_accuracy: 0.3763\n",
            "Epoch 272/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0298 - accuracy: 0.5598\n",
            "Epoch 00272: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0298 - accuracy: 0.5597 - val_loss: 1.8051 - val_accuracy: 0.3542\n",
            "Epoch 273/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0292 - accuracy: 0.5558\n",
            "Epoch 00273: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0289 - accuracy: 0.5559 - val_loss: 1.7626 - val_accuracy: 0.3659\n",
            "Epoch 274/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.5548\n",
            "Epoch 00274: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0345 - accuracy: 0.5548 - val_loss: 1.6677 - val_accuracy: 0.4094\n",
            "Epoch 275/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.5555\n",
            "Epoch 00275: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0291 - accuracy: 0.5556 - val_loss: 1.6622 - val_accuracy: 0.4103\n",
            "Epoch 276/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0317 - accuracy: 0.5546\n",
            "Epoch 00276: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0317 - accuracy: 0.5546 - val_loss: 1.6879 - val_accuracy: 0.3979\n",
            "Epoch 277/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.5575\n",
            "Epoch 00277: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0346 - accuracy: 0.5575 - val_loss: 1.7145 - val_accuracy: 0.3998\n",
            "Epoch 278/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0328 - accuracy: 0.5559\n",
            "Epoch 00278: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0327 - accuracy: 0.5559 - val_loss: 1.7717 - val_accuracy: 0.3627\n",
            "Epoch 279/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0266 - accuracy: 0.5588\n",
            "Epoch 00279: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0266 - accuracy: 0.5588 - val_loss: 1.7715 - val_accuracy: 0.3628\n",
            "Epoch 280/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.5570\n",
            "Epoch 00280: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0287 - accuracy: 0.5570 - val_loss: 1.6680 - val_accuracy: 0.4030\n",
            "Epoch 281/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0274 - accuracy: 0.5583\n",
            "Epoch 00281: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0277 - accuracy: 0.5577 - val_loss: 1.7074 - val_accuracy: 0.3927\n",
            "Epoch 282/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0316 - accuracy: 0.5545\n",
            "Epoch 00282: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0313 - accuracy: 0.5547 - val_loss: 1.7543 - val_accuracy: 0.3705\n",
            "Epoch 283/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0249 - accuracy: 0.5587\n",
            "Epoch 00283: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0254 - accuracy: 0.5585 - val_loss: 1.7537 - val_accuracy: 0.3829\n",
            "Epoch 284/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0277 - accuracy: 0.5580\n",
            "Epoch 00284: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0278 - accuracy: 0.5580 - val_loss: 1.7402 - val_accuracy: 0.3806\n",
            "Epoch 285/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.5569\n",
            "Epoch 00285: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0307 - accuracy: 0.5570 - val_loss: 1.7454 - val_accuracy: 0.3780\n",
            "Epoch 286/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0272 - accuracy: 0.5549\n",
            "Epoch 00286: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0274 - accuracy: 0.5547 - val_loss: 1.6908 - val_accuracy: 0.3916\n",
            "Epoch 287/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.5540\n",
            "Epoch 00287: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0340 - accuracy: 0.5540 - val_loss: 1.7178 - val_accuracy: 0.3888\n",
            "Epoch 288/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0271 - accuracy: 0.5584\n",
            "Epoch 00288: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0271 - accuracy: 0.5584 - val_loss: 1.7840 - val_accuracy: 0.3630\n",
            "Epoch 289/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0227 - accuracy: 0.5601\n",
            "Epoch 00289: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0227 - accuracy: 0.5601 - val_loss: 1.7417 - val_accuracy: 0.3770\n",
            "Epoch 290/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0295 - accuracy: 0.5582\n",
            "Epoch 00290: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0296 - accuracy: 0.5582 - val_loss: 1.7059 - val_accuracy: 0.3910\n",
            "Epoch 291/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0244 - accuracy: 0.5600\n",
            "Epoch 00291: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0245 - accuracy: 0.5600 - val_loss: 1.7272 - val_accuracy: 0.3963\n",
            "Epoch 292/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0292 - accuracy: 0.5568\n",
            "Epoch 00292: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0292 - accuracy: 0.5569 - val_loss: 1.7356 - val_accuracy: 0.3772\n",
            "Epoch 293/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0333 - accuracy: 0.5515\n",
            "Epoch 00293: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0332 - accuracy: 0.5516 - val_loss: 1.7005 - val_accuracy: 0.3989\n",
            "Epoch 294/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.5573\n",
            "Epoch 00294: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0256 - accuracy: 0.5575 - val_loss: 1.7043 - val_accuracy: 0.3994\n",
            "Epoch 295/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0275 - accuracy: 0.5584\n",
            "Epoch 00295: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0275 - accuracy: 0.5585 - val_loss: 1.6894 - val_accuracy: 0.3996\n",
            "Epoch 296/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.5541\n",
            "Epoch 00296: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0288 - accuracy: 0.5544 - val_loss: 1.6754 - val_accuracy: 0.4121\n",
            "Epoch 297/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0279 - accuracy: 0.5581\n",
            "Epoch 00297: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0278 - accuracy: 0.5582 - val_loss: 1.6921 - val_accuracy: 0.3948\n",
            "Epoch 298/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0272 - accuracy: 0.5610\n",
            "Epoch 00298: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0275 - accuracy: 0.5610 - val_loss: 1.7805 - val_accuracy: 0.3631\n",
            "Epoch 299/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0236 - accuracy: 0.5593\n",
            "Epoch 00299: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0238 - accuracy: 0.5592 - val_loss: 1.7896 - val_accuracy: 0.3547\n",
            "Epoch 300/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.5606\n",
            "Epoch 00300: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0260 - accuracy: 0.5605 - val_loss: 1.7024 - val_accuracy: 0.3987\n",
            "Epoch 301/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0265 - accuracy: 0.5569\n",
            "Epoch 00301: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0265 - accuracy: 0.5569 - val_loss: 1.7532 - val_accuracy: 0.3772\n",
            "Epoch 302/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0321 - accuracy: 0.5555\n",
            "Epoch 00302: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0320 - accuracy: 0.5555 - val_loss: 1.6728 - val_accuracy: 0.4097\n",
            "Epoch 303/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.5565\n",
            "Epoch 00303: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0263 - accuracy: 0.5565 - val_loss: 1.7123 - val_accuracy: 0.3949\n",
            "Epoch 304/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.5581\n",
            "Epoch 00304: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0256 - accuracy: 0.5581 - val_loss: 1.7295 - val_accuracy: 0.3813\n",
            "Epoch 305/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0280 - accuracy: 0.5574\n",
            "Epoch 00305: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0278 - accuracy: 0.5573 - val_loss: 1.7525 - val_accuracy: 0.3780\n",
            "Epoch 306/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0235 - accuracy: 0.5589\n",
            "Epoch 00306: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0234 - accuracy: 0.5589 - val_loss: 1.6562 - val_accuracy: 0.4158\n",
            "Epoch 307/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.5548\n",
            "Epoch 00307: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0260 - accuracy: 0.5551 - val_loss: 1.7735 - val_accuracy: 0.3664\n",
            "Epoch 308/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0262 - accuracy: 0.5565\n",
            "Epoch 00308: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0262 - accuracy: 0.5565 - val_loss: 1.7034 - val_accuracy: 0.3901\n",
            "Epoch 309/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0208 - accuracy: 0.5606\n",
            "Epoch 00309: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0206 - accuracy: 0.5606 - val_loss: 1.7076 - val_accuracy: 0.3980\n",
            "Epoch 310/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0362 - accuracy: 0.5516\n",
            "Epoch 00310: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0368 - accuracy: 0.5517 - val_loss: 1.7596 - val_accuracy: 0.3705\n",
            "Epoch 311/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0188 - accuracy: 0.5608\n",
            "Epoch 00311: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0191 - accuracy: 0.5607 - val_loss: 1.7668 - val_accuracy: 0.3632\n",
            "Epoch 312/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0241 - accuracy: 0.5612\n",
            "Epoch 00312: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0249 - accuracy: 0.5605 - val_loss: 1.7192 - val_accuracy: 0.3942\n",
            "Epoch 313/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0249 - accuracy: 0.5575\n",
            "Epoch 00313: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 13ms/step - loss: 1.0249 - accuracy: 0.5576 - val_loss: 1.7088 - val_accuracy: 0.3938\n",
            "Epoch 314/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.5570\n",
            "Epoch 00314: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 5s 17ms/step - loss: 1.0263 - accuracy: 0.5570 - val_loss: 1.7572 - val_accuracy: 0.3643\n",
            "Epoch 315/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0240 - accuracy: 0.5583\n",
            "Epoch 00315: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0243 - accuracy: 0.5581 - val_loss: 1.7497 - val_accuracy: 0.3756\n",
            "Epoch 316/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.5595\n",
            "Epoch 00316: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0239 - accuracy: 0.5598 - val_loss: 1.7524 - val_accuracy: 0.3741\n",
            "Epoch 317/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.5569\n",
            "Epoch 00317: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0248 - accuracy: 0.5571 - val_loss: 1.8082 - val_accuracy: 0.3465\n",
            "Epoch 318/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0242 - accuracy: 0.5585\n",
            "Epoch 00318: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0240 - accuracy: 0.5586 - val_loss: 1.7135 - val_accuracy: 0.3923\n",
            "Epoch 319/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.5586\n",
            "Epoch 00319: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0228 - accuracy: 0.5585 - val_loss: 1.6872 - val_accuracy: 0.3929\n",
            "Epoch 320/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.5578\n",
            "Epoch 00320: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0255 - accuracy: 0.5579 - val_loss: 1.7738 - val_accuracy: 0.3699\n",
            "Epoch 321/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.5571\n",
            "Epoch 00321: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0232 - accuracy: 0.5570 - val_loss: 1.6747 - val_accuracy: 0.4043\n",
            "Epoch 322/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.5577\n",
            "Epoch 00322: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0258 - accuracy: 0.5578 - val_loss: 1.7346 - val_accuracy: 0.3774\n",
            "Epoch 323/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0288 - accuracy: 0.5539\n",
            "Epoch 00323: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0283 - accuracy: 0.5544 - val_loss: 1.6855 - val_accuracy: 0.3971\n",
            "Epoch 324/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0214 - accuracy: 0.5585\n",
            "Epoch 00324: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0214 - accuracy: 0.5585 - val_loss: 1.6723 - val_accuracy: 0.4123\n",
            "Epoch 325/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.5603\n",
            "Epoch 00325: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0220 - accuracy: 0.5604 - val_loss: 1.7507 - val_accuracy: 0.3586\n",
            "Epoch 326/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0239 - accuracy: 0.5618\n",
            "Epoch 00326: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0238 - accuracy: 0.5617 - val_loss: 1.7020 - val_accuracy: 0.3936\n",
            "Epoch 327/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.5628\n",
            "Epoch 00327: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0191 - accuracy: 0.5626 - val_loss: 1.7763 - val_accuracy: 0.3558\n",
            "Epoch 328/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0257 - accuracy: 0.5586\n",
            "Epoch 00328: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0257 - accuracy: 0.5586 - val_loss: 1.7142 - val_accuracy: 0.3870\n",
            "Epoch 329/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.5643\n",
            "Epoch 00329: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0206 - accuracy: 0.5638 - val_loss: 1.7612 - val_accuracy: 0.3753\n",
            "Epoch 330/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0188 - accuracy: 0.5652\n",
            "Epoch 00330: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0189 - accuracy: 0.5650 - val_loss: 1.6578 - val_accuracy: 0.4132\n",
            "Epoch 331/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.5647\n",
            "Epoch 00331: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0197 - accuracy: 0.5646 - val_loss: 1.7446 - val_accuracy: 0.3823\n",
            "Epoch 332/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0237 - accuracy: 0.5607\n",
            "Epoch 00332: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0238 - accuracy: 0.5605 - val_loss: 1.7088 - val_accuracy: 0.3898\n",
            "Epoch 333/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.5596\n",
            "Epoch 00333: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0262 - accuracy: 0.5596 - val_loss: 1.7590 - val_accuracy: 0.3594\n",
            "Epoch 334/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.5632\n",
            "Epoch 00334: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0223 - accuracy: 0.5634 - val_loss: 1.7339 - val_accuracy: 0.3776\n",
            "Epoch 335/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0265 - accuracy: 0.5614\n",
            "Epoch 00335: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0267 - accuracy: 0.5615 - val_loss: 1.8614 - val_accuracy: 0.3365\n",
            "Epoch 336/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.5627\n",
            "Epoch 00336: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0220 - accuracy: 0.5629 - val_loss: 1.7082 - val_accuracy: 0.4058\n",
            "Epoch 337/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0401 - accuracy: 0.5498\n",
            "Epoch 00337: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0404 - accuracy: 0.5497 - val_loss: 1.7759 - val_accuracy: 0.3504\n",
            "Epoch 338/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0194 - accuracy: 0.5631\n",
            "Epoch 00338: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0195 - accuracy: 0.5631 - val_loss: 1.6930 - val_accuracy: 0.4043\n",
            "Epoch 339/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.5610\n",
            "Epoch 00339: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0239 - accuracy: 0.5615 - val_loss: 1.6718 - val_accuracy: 0.4052\n",
            "Epoch 340/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0210 - accuracy: 0.5638\n",
            "Epoch 00340: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0210 - accuracy: 0.5637 - val_loss: 1.7040 - val_accuracy: 0.3992\n",
            "Epoch 341/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.5645\n",
            "Epoch 00341: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0188 - accuracy: 0.5646 - val_loss: 1.7293 - val_accuracy: 0.3806\n",
            "Epoch 342/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0293 - accuracy: 0.5577\n",
            "Epoch 00342: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0294 - accuracy: 0.5576 - val_loss: 1.7100 - val_accuracy: 0.3789\n",
            "Epoch 343/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.5626\n",
            "Epoch 00343: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0250 - accuracy: 0.5627 - val_loss: 1.6354 - val_accuracy: 0.4175\n",
            "Epoch 344/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.5647\n",
            "Epoch 00344: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0220 - accuracy: 0.5646 - val_loss: 1.7878 - val_accuracy: 0.3667\n",
            "Epoch 345/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.5631\n",
            "Epoch 00345: val_accuracy did not improve from 0.44049\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0243 - accuracy: 0.5632 - val_loss: 1.6417 - val_accuracy: 0.4268\n",
            "Epoch 346/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0216 - accuracy: 0.5634\n",
            "Epoch 00346: val_accuracy improved from 0.44049 to 0.44780, saving model to /content/drive/MyDrive/data分析/Mediapipe/Medaipipeweight2.h5\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0216 - accuracy: 0.5634 - val_loss: 1.5787 - val_accuracy: 0.4478\n",
            "Epoch 347/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0273 - accuracy: 0.5618\n",
            "Epoch 00347: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0266 - accuracy: 0.5622 - val_loss: 1.7067 - val_accuracy: 0.3959\n",
            "Epoch 348/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0210 - accuracy: 0.5636\n",
            "Epoch 00348: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0209 - accuracy: 0.5636 - val_loss: 1.6797 - val_accuracy: 0.4027\n",
            "Epoch 349/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0267 - accuracy: 0.5621\n",
            "Epoch 00349: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0268 - accuracy: 0.5621 - val_loss: 1.6599 - val_accuracy: 0.4008\n",
            "Epoch 350/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0174 - accuracy: 0.5678\n",
            "Epoch 00350: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0170 - accuracy: 0.5680 - val_loss: 1.8242 - val_accuracy: 0.3486\n",
            "Epoch 351/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0285 - accuracy: 0.5612\n",
            "Epoch 00351: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0285 - accuracy: 0.5612 - val_loss: 1.6315 - val_accuracy: 0.4239\n",
            "Epoch 352/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0192 - accuracy: 0.5657\n",
            "Epoch 00352: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0191 - accuracy: 0.5658 - val_loss: 1.7927 - val_accuracy: 0.3498\n",
            "Epoch 353/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0300 - accuracy: 0.5599\n",
            "Epoch 00353: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0307 - accuracy: 0.5589 - val_loss: 1.6652 - val_accuracy: 0.4127\n",
            "Epoch 354/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0204 - accuracy: 0.5654\n",
            "Epoch 00354: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0204 - accuracy: 0.5654 - val_loss: 1.6804 - val_accuracy: 0.3985\n",
            "Epoch 355/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0204 - accuracy: 0.5646\n",
            "Epoch 00355: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0204 - accuracy: 0.5646 - val_loss: 1.7036 - val_accuracy: 0.3941\n",
            "Epoch 356/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0162 - accuracy: 0.5684\n",
            "Epoch 00356: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0162 - accuracy: 0.5684 - val_loss: 1.7242 - val_accuracy: 0.3808\n",
            "Epoch 357/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.5663\n",
            "Epoch 00357: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0198 - accuracy: 0.5661 - val_loss: 1.6926 - val_accuracy: 0.3981\n",
            "Epoch 358/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0180 - accuracy: 0.5687\n",
            "Epoch 00358: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0177 - accuracy: 0.5689 - val_loss: 1.6697 - val_accuracy: 0.4031\n",
            "Epoch 359/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.5639\n",
            "Epoch 00359: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0223 - accuracy: 0.5639 - val_loss: 1.6909 - val_accuracy: 0.4009\n",
            "Epoch 360/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0277 - accuracy: 0.5606\n",
            "Epoch 00360: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0283 - accuracy: 0.5604 - val_loss: 1.6303 - val_accuracy: 0.4320\n",
            "Epoch 361/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0165 - accuracy: 0.5680\n",
            "Epoch 00361: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0165 - accuracy: 0.5680 - val_loss: 1.7227 - val_accuracy: 0.3897\n",
            "Epoch 362/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0135 - accuracy: 0.5686\n",
            "Epoch 00362: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0135 - accuracy: 0.5686 - val_loss: 1.7370 - val_accuracy: 0.3818\n",
            "Epoch 363/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0165 - accuracy: 0.5673\n",
            "Epoch 00363: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0166 - accuracy: 0.5674 - val_loss: 1.7412 - val_accuracy: 0.3800\n",
            "Epoch 364/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.5634\n",
            "Epoch 00364: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0201 - accuracy: 0.5634 - val_loss: 1.6524 - val_accuracy: 0.4133\n",
            "Epoch 365/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0215 - accuracy: 0.5651\n",
            "Epoch 00365: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0215 - accuracy: 0.5651 - val_loss: 1.7594 - val_accuracy: 0.3751\n",
            "Epoch 366/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.5674\n",
            "Epoch 00366: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0184 - accuracy: 0.5676 - val_loss: 1.7152 - val_accuracy: 0.3923\n",
            "Epoch 367/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0267 - accuracy: 0.5619\n",
            "Epoch 00367: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0267 - accuracy: 0.5618 - val_loss: 1.7533 - val_accuracy: 0.3663\n",
            "Epoch 368/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0182 - accuracy: 0.5677\n",
            "Epoch 00368: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0184 - accuracy: 0.5675 - val_loss: 1.7369 - val_accuracy: 0.3739\n",
            "Epoch 369/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0213 - accuracy: 0.5656\n",
            "Epoch 00369: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0211 - accuracy: 0.5656 - val_loss: 1.6593 - val_accuracy: 0.4177\n",
            "Epoch 370/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0169 - accuracy: 0.5677\n",
            "Epoch 00370: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0171 - accuracy: 0.5675 - val_loss: 1.7601 - val_accuracy: 0.3588\n",
            "Epoch 371/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0172 - accuracy: 0.5697\n",
            "Epoch 00371: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0172 - accuracy: 0.5697 - val_loss: 1.7029 - val_accuracy: 0.4033\n",
            "Epoch 372/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0219 - accuracy: 0.5644\n",
            "Epoch 00372: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0219 - accuracy: 0.5644 - val_loss: 1.6959 - val_accuracy: 0.3927\n",
            "Epoch 373/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.5639\n",
            "Epoch 00373: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0246 - accuracy: 0.5638 - val_loss: 1.5998 - val_accuracy: 0.4457\n",
            "Epoch 374/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.5673\n",
            "Epoch 00374: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0184 - accuracy: 0.5674 - val_loss: 1.7137 - val_accuracy: 0.3948\n",
            "Epoch 375/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0162 - accuracy: 0.5668\n",
            "Epoch 00375: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0162 - accuracy: 0.5668 - val_loss: 1.6937 - val_accuracy: 0.4032\n",
            "Epoch 376/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0159 - accuracy: 0.5653\n",
            "Epoch 00376: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0160 - accuracy: 0.5653 - val_loss: 1.7371 - val_accuracy: 0.3705\n",
            "Epoch 377/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.5657\n",
            "Epoch 00377: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0173 - accuracy: 0.5659 - val_loss: 1.7154 - val_accuracy: 0.3890\n",
            "Epoch 378/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0138 - accuracy: 0.5690\n",
            "Epoch 00378: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0138 - accuracy: 0.5690 - val_loss: 1.7161 - val_accuracy: 0.3857\n",
            "Epoch 379/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0156 - accuracy: 0.5675\n",
            "Epoch 00379: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0156 - accuracy: 0.5674 - val_loss: 1.7934 - val_accuracy: 0.3592\n",
            "Epoch 380/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0166 - accuracy: 0.5675\n",
            "Epoch 00380: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0166 - accuracy: 0.5675 - val_loss: 1.6922 - val_accuracy: 0.4030\n",
            "Epoch 381/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0171 - accuracy: 0.5677\n",
            "Epoch 00381: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0171 - accuracy: 0.5677 - val_loss: 1.7067 - val_accuracy: 0.4013\n",
            "Epoch 382/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0154 - accuracy: 0.5679\n",
            "Epoch 00382: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0151 - accuracy: 0.5679 - val_loss: 1.7203 - val_accuracy: 0.3870\n",
            "Epoch 383/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.5672\n",
            "Epoch 00383: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0190 - accuracy: 0.5671 - val_loss: 1.6815 - val_accuracy: 0.4017\n",
            "Epoch 384/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0148 - accuracy: 0.5692\n",
            "Epoch 00384: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0148 - accuracy: 0.5692 - val_loss: 1.7469 - val_accuracy: 0.3717\n",
            "Epoch 385/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.5663\n",
            "Epoch 00385: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0194 - accuracy: 0.5664 - val_loss: 1.6803 - val_accuracy: 0.4081\n",
            "Epoch 386/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0162 - accuracy: 0.5674\n",
            "Epoch 00386: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0167 - accuracy: 0.5672 - val_loss: 1.7597 - val_accuracy: 0.3768\n",
            "Epoch 387/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0194 - accuracy: 0.5662\n",
            "Epoch 00387: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0194 - accuracy: 0.5662 - val_loss: 1.6533 - val_accuracy: 0.4138\n",
            "Epoch 388/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0192 - accuracy: 0.5656\n",
            "Epoch 00388: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0192 - accuracy: 0.5657 - val_loss: 1.6745 - val_accuracy: 0.4132\n",
            "Epoch 389/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0115 - accuracy: 0.5703\n",
            "Epoch 00389: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0116 - accuracy: 0.5702 - val_loss: 1.7231 - val_accuracy: 0.3928\n",
            "Epoch 390/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0247 - accuracy: 0.5619\n",
            "Epoch 00390: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0250 - accuracy: 0.5616 - val_loss: 1.7372 - val_accuracy: 0.3766\n",
            "Epoch 391/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0128 - accuracy: 0.5692\n",
            "Epoch 00391: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0133 - accuracy: 0.5690 - val_loss: 1.7569 - val_accuracy: 0.3802\n",
            "Epoch 392/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.5709\n",
            "Epoch 00392: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0174 - accuracy: 0.5710 - val_loss: 1.7130 - val_accuracy: 0.3872\n",
            "Epoch 393/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0129 - accuracy: 0.5688\n",
            "Epoch 00393: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0133 - accuracy: 0.5687 - val_loss: 1.7063 - val_accuracy: 0.3886\n",
            "Epoch 394/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0171 - accuracy: 0.5686\n",
            "Epoch 00394: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0168 - accuracy: 0.5688 - val_loss: 1.8875 - val_accuracy: 0.3239\n",
            "Epoch 395/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0163 - accuracy: 0.5662\n",
            "Epoch 00395: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0165 - accuracy: 0.5661 - val_loss: 1.6559 - val_accuracy: 0.4180\n",
            "Epoch 396/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0155 - accuracy: 0.5681\n",
            "Epoch 00396: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0155 - accuracy: 0.5681 - val_loss: 1.6469 - val_accuracy: 0.4175\n",
            "Epoch 397/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0142 - accuracy: 0.5680\n",
            "Epoch 00397: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0142 - accuracy: 0.5679 - val_loss: 1.7592 - val_accuracy: 0.3595\n",
            "Epoch 398/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0157 - accuracy: 0.5692\n",
            "Epoch 00398: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0150 - accuracy: 0.5694 - val_loss: 1.7052 - val_accuracy: 0.3965\n",
            "Epoch 399/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0214 - accuracy: 0.5648\n",
            "Epoch 00399: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0215 - accuracy: 0.5648 - val_loss: 1.7598 - val_accuracy: 0.3819\n",
            "Epoch 400/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0141 - accuracy: 0.5701\n",
            "Epoch 00400: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0136 - accuracy: 0.5704 - val_loss: 1.6955 - val_accuracy: 0.3987\n",
            "Epoch 401/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0177 - accuracy: 0.5679\n",
            "Epoch 00401: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0177 - accuracy: 0.5679 - val_loss: 1.6924 - val_accuracy: 0.3968\n",
            "Epoch 402/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0153 - accuracy: 0.5675\n",
            "Epoch 00402: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0153 - accuracy: 0.5675 - val_loss: 1.7576 - val_accuracy: 0.3720\n",
            "Epoch 403/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0168 - accuracy: 0.5667\n",
            "Epoch 00403: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0169 - accuracy: 0.5666 - val_loss: 1.6862 - val_accuracy: 0.4046\n",
            "Epoch 404/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0139 - accuracy: 0.5700\n",
            "Epoch 00404: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0139 - accuracy: 0.5700 - val_loss: 1.7591 - val_accuracy: 0.3761\n",
            "Epoch 405/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0158 - accuracy: 0.5680\n",
            "Epoch 00405: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0160 - accuracy: 0.5681 - val_loss: 1.6936 - val_accuracy: 0.4013\n",
            "Epoch 406/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0161 - accuracy: 0.5669\n",
            "Epoch 00406: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0159 - accuracy: 0.5667 - val_loss: 1.7021 - val_accuracy: 0.4017\n",
            "Epoch 407/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0165 - accuracy: 0.5668\n",
            "Epoch 00407: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0165 - accuracy: 0.5668 - val_loss: 1.6897 - val_accuracy: 0.4042\n",
            "Epoch 408/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0231 - accuracy: 0.5639\n",
            "Epoch 00408: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0232 - accuracy: 0.5639 - val_loss: 1.7427 - val_accuracy: 0.3761\n",
            "Epoch 409/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0155 - accuracy: 0.5658\n",
            "Epoch 00409: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0156 - accuracy: 0.5657 - val_loss: 1.6112 - val_accuracy: 0.4384\n",
            "Epoch 410/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0163 - accuracy: 0.5669\n",
            "Epoch 00410: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0163 - accuracy: 0.5669 - val_loss: 1.7071 - val_accuracy: 0.3934\n",
            "Epoch 411/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0142 - accuracy: 0.5705\n",
            "Epoch 00411: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0142 - accuracy: 0.5705 - val_loss: 1.6909 - val_accuracy: 0.3783\n",
            "Epoch 412/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0117 - accuracy: 0.5699\n",
            "Epoch 00412: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0121 - accuracy: 0.5698 - val_loss: 1.7112 - val_accuracy: 0.3914\n",
            "Epoch 413/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0117 - accuracy: 0.5697\n",
            "Epoch 00413: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0118 - accuracy: 0.5697 - val_loss: 1.7090 - val_accuracy: 0.3910\n",
            "Epoch 414/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0197 - accuracy: 0.5657\n",
            "Epoch 00414: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0195 - accuracy: 0.5657 - val_loss: 1.6653 - val_accuracy: 0.4123\n",
            "Epoch 415/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.5658\n",
            "Epoch 00415: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0197 - accuracy: 0.5661 - val_loss: 1.8321 - val_accuracy: 0.3431\n",
            "Epoch 416/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0109 - accuracy: 0.5721\n",
            "Epoch 00416: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0108 - accuracy: 0.5721 - val_loss: 1.7590 - val_accuracy: 0.3694\n",
            "Epoch 417/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0396 - accuracy: 0.5620\n",
            "Epoch 00417: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0396 - accuracy: 0.5620 - val_loss: 1.7577 - val_accuracy: 0.3721\n",
            "Epoch 418/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0163 - accuracy: 0.5676\n",
            "Epoch 00418: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0161 - accuracy: 0.5677 - val_loss: 1.7955 - val_accuracy: 0.3457\n",
            "Epoch 419/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0130 - accuracy: 0.5709\n",
            "Epoch 00419: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0127 - accuracy: 0.5712 - val_loss: 1.7478 - val_accuracy: 0.3810\n",
            "Epoch 420/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0149 - accuracy: 0.5666\n",
            "Epoch 00420: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0151 - accuracy: 0.5665 - val_loss: 1.7245 - val_accuracy: 0.3953\n",
            "Epoch 421/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0101 - accuracy: 0.5713\n",
            "Epoch 00421: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0101 - accuracy: 0.5713 - val_loss: 1.6792 - val_accuracy: 0.3996\n",
            "Epoch 422/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0147 - accuracy: 0.5682\n",
            "Epoch 00422: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0147 - accuracy: 0.5682 - val_loss: 1.7423 - val_accuracy: 0.3875\n",
            "Epoch 423/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0113 - accuracy: 0.5705\n",
            "Epoch 00423: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0113 - accuracy: 0.5705 - val_loss: 1.7301 - val_accuracy: 0.3827\n",
            "Epoch 424/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0166 - accuracy: 0.5669\n",
            "Epoch 00424: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0172 - accuracy: 0.5669 - val_loss: 1.6962 - val_accuracy: 0.3978\n",
            "Epoch 425/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0142 - accuracy: 0.5696\n",
            "Epoch 00425: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0141 - accuracy: 0.5696 - val_loss: 1.8104 - val_accuracy: 0.3511\n",
            "Epoch 426/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0144 - accuracy: 0.5697\n",
            "Epoch 00426: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0144 - accuracy: 0.5697 - val_loss: 1.7335 - val_accuracy: 0.3739\n",
            "Epoch 427/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0131 - accuracy: 0.5684\n",
            "Epoch 00427: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0129 - accuracy: 0.5685 - val_loss: 1.7850 - val_accuracy: 0.3533\n",
            "Epoch 428/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0118 - accuracy: 0.5697\n",
            "Epoch 00428: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0117 - accuracy: 0.5698 - val_loss: 1.6824 - val_accuracy: 0.4056\n",
            "Epoch 429/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0138 - accuracy: 0.5689\n",
            "Epoch 00429: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0135 - accuracy: 0.5691 - val_loss: 1.6979 - val_accuracy: 0.3961\n",
            "Epoch 430/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0109 - accuracy: 0.5710\n",
            "Epoch 00430: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0109 - accuracy: 0.5710 - val_loss: 1.7024 - val_accuracy: 0.3891\n",
            "Epoch 431/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0111 - accuracy: 0.5712\n",
            "Epoch 00431: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0111 - accuracy: 0.5712 - val_loss: 1.6736 - val_accuracy: 0.4061\n",
            "Epoch 432/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.5737\n",
            "Epoch 00432: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0082 - accuracy: 0.5735 - val_loss: 1.7499 - val_accuracy: 0.3776\n",
            "Epoch 433/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.5626\n",
            "Epoch 00433: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0198 - accuracy: 0.5629 - val_loss: 1.6875 - val_accuracy: 0.4052\n",
            "Epoch 434/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0087 - accuracy: 0.5720\n",
            "Epoch 00434: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0083 - accuracy: 0.5722 - val_loss: 1.7159 - val_accuracy: 0.3921\n",
            "Epoch 435/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0141 - accuracy: 0.5672\n",
            "Epoch 00435: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0140 - accuracy: 0.5675 - val_loss: 1.6862 - val_accuracy: 0.3976\n",
            "Epoch 436/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0110 - accuracy: 0.5711\n",
            "Epoch 00436: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0110 - accuracy: 0.5710 - val_loss: 1.8067 - val_accuracy: 0.3459\n",
            "Epoch 437/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0191 - accuracy: 0.5675\n",
            "Epoch 00437: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0195 - accuracy: 0.5672 - val_loss: 1.6801 - val_accuracy: 0.4059\n",
            "Epoch 438/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0126 - accuracy: 0.5703\n",
            "Epoch 00438: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0126 - accuracy: 0.5703 - val_loss: 1.8539 - val_accuracy: 0.3538\n",
            "Epoch 439/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0468 - accuracy: 0.5519\n",
            "Epoch 00439: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0467 - accuracy: 0.5520 - val_loss: 1.7450 - val_accuracy: 0.3837\n",
            "Epoch 440/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0077 - accuracy: 0.5732\n",
            "Epoch 00440: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0079 - accuracy: 0.5730 - val_loss: 1.7406 - val_accuracy: 0.3851\n",
            "Epoch 441/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0142 - accuracy: 0.5683\n",
            "Epoch 00441: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0140 - accuracy: 0.5682 - val_loss: 1.7096 - val_accuracy: 0.3895\n",
            "Epoch 442/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0133 - accuracy: 0.5704\n",
            "Epoch 00442: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0131 - accuracy: 0.5704 - val_loss: 1.8220 - val_accuracy: 0.3407\n",
            "Epoch 443/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0088 - accuracy: 0.5721\n",
            "Epoch 00443: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0092 - accuracy: 0.5718 - val_loss: 1.7081 - val_accuracy: 0.3981\n",
            "Epoch 444/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0156 - accuracy: 0.5694\n",
            "Epoch 00444: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0155 - accuracy: 0.5697 - val_loss: 1.6786 - val_accuracy: 0.4178\n",
            "Epoch 445/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0147 - accuracy: 0.5694\n",
            "Epoch 00445: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0147 - accuracy: 0.5693 - val_loss: 1.6929 - val_accuracy: 0.3982\n",
            "Epoch 446/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0138 - accuracy: 0.5695\n",
            "Epoch 00446: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0135 - accuracy: 0.5698 - val_loss: 1.6646 - val_accuracy: 0.4158\n",
            "Epoch 447/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0197 - accuracy: 0.5663\n",
            "Epoch 00447: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0198 - accuracy: 0.5663 - val_loss: 1.7506 - val_accuracy: 0.3752\n",
            "Epoch 448/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0132 - accuracy: 0.5702\n",
            "Epoch 00448: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0131 - accuracy: 0.5704 - val_loss: 1.7078 - val_accuracy: 0.3948\n",
            "Epoch 449/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.5674\n",
            "Epoch 00449: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0189 - accuracy: 0.5674 - val_loss: 1.6772 - val_accuracy: 0.4069\n",
            "Epoch 450/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0150 - accuracy: 0.5692\n",
            "Epoch 00450: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0150 - accuracy: 0.5693 - val_loss: 1.7724 - val_accuracy: 0.3523\n",
            "Epoch 451/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0158 - accuracy: 0.5689\n",
            "Epoch 00451: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0154 - accuracy: 0.5691 - val_loss: 1.7188 - val_accuracy: 0.3987\n",
            "Epoch 452/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0101 - accuracy: 0.5714\n",
            "Epoch 00452: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0100 - accuracy: 0.5714 - val_loss: 1.6934 - val_accuracy: 0.3966\n",
            "Epoch 453/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0116 - accuracy: 0.5689\n",
            "Epoch 00453: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0115 - accuracy: 0.5690 - val_loss: 1.6086 - val_accuracy: 0.4330\n",
            "Epoch 454/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0124 - accuracy: 0.5687\n",
            "Epoch 00454: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0123 - accuracy: 0.5687 - val_loss: 1.7570 - val_accuracy: 0.3746\n",
            "Epoch 455/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0101 - accuracy: 0.5724\n",
            "Epoch 00455: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0099 - accuracy: 0.5726 - val_loss: 1.7358 - val_accuracy: 0.3762\n",
            "Epoch 456/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0179 - accuracy: 0.5665\n",
            "Epoch 00456: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0177 - accuracy: 0.5665 - val_loss: 1.8398 - val_accuracy: 0.3430\n",
            "Epoch 457/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0112 - accuracy: 0.5719\n",
            "Epoch 00457: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0112 - accuracy: 0.5719 - val_loss: 1.6476 - val_accuracy: 0.4251\n",
            "Epoch 458/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0097 - accuracy: 0.5705\n",
            "Epoch 00458: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0095 - accuracy: 0.5706 - val_loss: 1.7020 - val_accuracy: 0.4002\n",
            "Epoch 459/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0085 - accuracy: 0.5711\n",
            "Epoch 00459: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0086 - accuracy: 0.5711 - val_loss: 1.7350 - val_accuracy: 0.3700\n",
            "Epoch 460/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0132 - accuracy: 0.5686\n",
            "Epoch 00460: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0131 - accuracy: 0.5686 - val_loss: 1.7521 - val_accuracy: 0.3712\n",
            "Epoch 461/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0110 - accuracy: 0.5704\n",
            "Epoch 00461: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0113 - accuracy: 0.5703 - val_loss: 1.7188 - val_accuracy: 0.3798\n",
            "Epoch 462/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0135 - accuracy: 0.5681\n",
            "Epoch 00462: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0133 - accuracy: 0.5679 - val_loss: 1.7471 - val_accuracy: 0.3648\n",
            "Epoch 463/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0063 - accuracy: 0.5727\n",
            "Epoch 00463: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0066 - accuracy: 0.5726 - val_loss: 1.7660 - val_accuracy: 0.3693\n",
            "Epoch 464/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0048 - accuracy: 0.5750\n",
            "Epoch 00464: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0048 - accuracy: 0.5750 - val_loss: 1.7969 - val_accuracy: 0.3595\n",
            "Epoch 465/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0107 - accuracy: 0.5693\n",
            "Epoch 00465: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0106 - accuracy: 0.5694 - val_loss: 1.7518 - val_accuracy: 0.3676\n",
            "Epoch 466/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.5754\n",
            "Epoch 00466: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0068 - accuracy: 0.5754 - val_loss: 1.7178 - val_accuracy: 0.4076\n",
            "Epoch 467/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0071 - accuracy: 0.5713\n",
            "Epoch 00467: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0070 - accuracy: 0.5714 - val_loss: 1.6630 - val_accuracy: 0.4187\n",
            "Epoch 468/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0072 - accuracy: 0.5721\n",
            "Epoch 00468: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0072 - accuracy: 0.5721 - val_loss: 1.6669 - val_accuracy: 0.4076\n",
            "Epoch 469/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0090 - accuracy: 0.5710\n",
            "Epoch 00469: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 10ms/step - loss: 1.0090 - accuracy: 0.5710 - val_loss: 1.6236 - val_accuracy: 0.4365\n",
            "Epoch 470/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0142 - accuracy: 0.5684\n",
            "Epoch 00470: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0137 - accuracy: 0.5686 - val_loss: 1.6793 - val_accuracy: 0.4170\n",
            "Epoch 471/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0145 - accuracy: 0.5685\n",
            "Epoch 00471: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0149 - accuracy: 0.5683 - val_loss: 1.6581 - val_accuracy: 0.4285\n",
            "Epoch 472/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0092 - accuracy: 0.5721\n",
            "Epoch 00472: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0094 - accuracy: 0.5719 - val_loss: 1.7536 - val_accuracy: 0.3739\n",
            "Epoch 473/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0095 - accuracy: 0.5711\n",
            "Epoch 00473: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0094 - accuracy: 0.5711 - val_loss: 1.7619 - val_accuracy: 0.3667\n",
            "Epoch 474/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0077 - accuracy: 0.5703\n",
            "Epoch 00474: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0078 - accuracy: 0.5703 - val_loss: 1.7140 - val_accuracy: 0.3981\n",
            "Epoch 475/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0074 - accuracy: 0.5735\n",
            "Epoch 00475: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0070 - accuracy: 0.5736 - val_loss: 1.7546 - val_accuracy: 0.3814\n",
            "Epoch 476/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0111 - accuracy: 0.5692\n",
            "Epoch 00476: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0111 - accuracy: 0.5692 - val_loss: 1.6508 - val_accuracy: 0.4140\n",
            "Epoch 477/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0094 - accuracy: 0.5709\n",
            "Epoch 00477: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0094 - accuracy: 0.5709 - val_loss: 1.7947 - val_accuracy: 0.3512\n",
            "Epoch 478/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0100 - accuracy: 0.5713\n",
            "Epoch 00478: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0101 - accuracy: 0.5713 - val_loss: 1.7717 - val_accuracy: 0.3700\n",
            "Epoch 479/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0117 - accuracy: 0.5680\n",
            "Epoch 00479: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0117 - accuracy: 0.5679 - val_loss: 1.8592 - val_accuracy: 0.3278\n",
            "Epoch 480/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0086 - accuracy: 0.5716\n",
            "Epoch 00480: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0083 - accuracy: 0.5719 - val_loss: 1.7108 - val_accuracy: 0.3976\n",
            "Epoch 481/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0150 - accuracy: 0.5670\n",
            "Epoch 00481: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0149 - accuracy: 0.5670 - val_loss: 1.6997 - val_accuracy: 0.3995\n",
            "Epoch 482/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0072 - accuracy: 0.5711\n",
            "Epoch 00482: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0073 - accuracy: 0.5711 - val_loss: 1.7833 - val_accuracy: 0.3581\n",
            "Epoch 483/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0135 - accuracy: 0.5680\n",
            "Epoch 00483: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0134 - accuracy: 0.5679 - val_loss: 1.7511 - val_accuracy: 0.3765\n",
            "Epoch 484/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0093 - accuracy: 0.5715\n",
            "Epoch 00484: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0093 - accuracy: 0.5715 - val_loss: 1.6807 - val_accuracy: 0.4005\n",
            "Epoch 485/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0133 - accuracy: 0.5673\n",
            "Epoch 00485: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0133 - accuracy: 0.5672 - val_loss: 1.6651 - val_accuracy: 0.4022\n",
            "Epoch 486/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0097 - accuracy: 0.5722\n",
            "Epoch 00486: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0098 - accuracy: 0.5722 - val_loss: 1.7105 - val_accuracy: 0.3972\n",
            "Epoch 487/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0078 - accuracy: 0.5723\n",
            "Epoch 00487: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0072 - accuracy: 0.5725 - val_loss: 1.6876 - val_accuracy: 0.4070\n",
            "Epoch 488/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0092 - accuracy: 0.5713\n",
            "Epoch 00488: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0092 - accuracy: 0.5713 - val_loss: 1.7842 - val_accuracy: 0.3520\n",
            "Epoch 489/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0111 - accuracy: 0.5689\n",
            "Epoch 00489: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0109 - accuracy: 0.5690 - val_loss: 1.8086 - val_accuracy: 0.3484\n",
            "Epoch 490/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0089 - accuracy: 0.5728\n",
            "Epoch 00490: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0089 - accuracy: 0.5728 - val_loss: 1.7765 - val_accuracy: 0.3623\n",
            "Epoch 491/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0054 - accuracy: 0.5720\n",
            "Epoch 00491: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0059 - accuracy: 0.5717 - val_loss: 1.6897 - val_accuracy: 0.3999\n",
            "Epoch 492/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0093 - accuracy: 0.5698\n",
            "Epoch 00492: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0094 - accuracy: 0.5697 - val_loss: 1.7194 - val_accuracy: 0.3845\n",
            "Epoch 493/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0023 - accuracy: 0.5718\n",
            "Epoch 00493: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0028 - accuracy: 0.5721 - val_loss: 1.7022 - val_accuracy: 0.3890\n",
            "Epoch 494/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0081 - accuracy: 0.5713\n",
            "Epoch 00494: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0081 - accuracy: 0.5713 - val_loss: 1.7647 - val_accuracy: 0.3590\n",
            "Epoch 495/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.5715\n",
            "Epoch 00495: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0082 - accuracy: 0.5717 - val_loss: 1.7630 - val_accuracy: 0.3792\n",
            "Epoch 496/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0080 - accuracy: 0.5731\n",
            "Epoch 00496: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0078 - accuracy: 0.5733 - val_loss: 1.6696 - val_accuracy: 0.4052\n",
            "Epoch 497/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.5707\n",
            "Epoch 00497: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0081 - accuracy: 0.5707 - val_loss: 1.7444 - val_accuracy: 0.3724\n",
            "Epoch 498/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0040 - accuracy: 0.5750\n",
            "Epoch 00498: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0042 - accuracy: 0.5749 - val_loss: 1.6603 - val_accuracy: 0.4022\n",
            "Epoch 499/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0088 - accuracy: 0.5715\n",
            "Epoch 00499: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0089 - accuracy: 0.5714 - val_loss: 1.7605 - val_accuracy: 0.3709\n",
            "Epoch 500/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0076 - accuracy: 0.5732\n",
            "Epoch 00500: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0074 - accuracy: 0.5734 - val_loss: 1.6937 - val_accuracy: 0.3945\n",
            "Epoch 501/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0087 - accuracy: 0.5709\n",
            "Epoch 00501: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0085 - accuracy: 0.5710 - val_loss: 1.7096 - val_accuracy: 0.3830\n",
            "Epoch 502/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.5736\n",
            "Epoch 00502: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0081 - accuracy: 0.5736 - val_loss: 1.7514 - val_accuracy: 0.3767\n",
            "Epoch 503/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0058 - accuracy: 0.5742\n",
            "Epoch 00503: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0056 - accuracy: 0.5744 - val_loss: 1.6764 - val_accuracy: 0.3987\n",
            "Epoch 504/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0083 - accuracy: 0.5709\n",
            "Epoch 00504: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0083 - accuracy: 0.5709 - val_loss: 1.8632 - val_accuracy: 0.3191\n",
            "Epoch 505/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0269 - accuracy: 0.5649\n",
            "Epoch 00505: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0269 - accuracy: 0.5649 - val_loss: 1.6509 - val_accuracy: 0.4176\n",
            "Epoch 506/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0062 - accuracy: 0.5740\n",
            "Epoch 00506: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0059 - accuracy: 0.5741 - val_loss: 1.7618 - val_accuracy: 0.3588\n",
            "Epoch 507/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0108 - accuracy: 0.5711\n",
            "Epoch 00507: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0108 - accuracy: 0.5711 - val_loss: 1.7276 - val_accuracy: 0.3754\n",
            "Epoch 508/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0045 - accuracy: 0.5728\n",
            "Epoch 00508: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0043 - accuracy: 0.5729 - val_loss: 1.7304 - val_accuracy: 0.3814\n",
            "Epoch 509/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0059 - accuracy: 0.5716\n",
            "Epoch 00509: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0058 - accuracy: 0.5717 - val_loss: 1.7425 - val_accuracy: 0.3754\n",
            "Epoch 510/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0074 - accuracy: 0.5735\n",
            "Epoch 00510: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0074 - accuracy: 0.5735 - val_loss: 1.7255 - val_accuracy: 0.3830\n",
            "Epoch 511/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0065 - accuracy: 0.5741\n",
            "Epoch 00511: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0065 - accuracy: 0.5740 - val_loss: 1.7009 - val_accuracy: 0.3948\n",
            "Epoch 512/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.5705\n",
            "Epoch 00512: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0082 - accuracy: 0.5705 - val_loss: 1.6665 - val_accuracy: 0.4046\n",
            "Epoch 513/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0103 - accuracy: 0.5686\n",
            "Epoch 00513: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0103 - accuracy: 0.5686 - val_loss: 1.7253 - val_accuracy: 0.3916\n",
            "Epoch 514/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0083 - accuracy: 0.5704\n",
            "Epoch 00514: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0087 - accuracy: 0.5702 - val_loss: 1.7019 - val_accuracy: 0.4083\n",
            "Epoch 515/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0054 - accuracy: 0.5731\n",
            "Epoch 00515: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0054 - accuracy: 0.5731 - val_loss: 1.7765 - val_accuracy: 0.3526\n",
            "Epoch 516/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0100 - accuracy: 0.5692\n",
            "Epoch 00516: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0100 - accuracy: 0.5692 - val_loss: 1.6931 - val_accuracy: 0.3962\n",
            "Epoch 517/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0151 - accuracy: 0.5691\n",
            "Epoch 00517: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0143 - accuracy: 0.5693 - val_loss: 1.6151 - val_accuracy: 0.4384\n",
            "Epoch 518/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0117 - accuracy: 0.5694\n",
            "Epoch 00518: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0117 - accuracy: 0.5694 - val_loss: 1.7783 - val_accuracy: 0.3654\n",
            "Epoch 519/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0100 - accuracy: 0.5711\n",
            "Epoch 00519: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0100 - accuracy: 0.5711 - val_loss: 1.7981 - val_accuracy: 0.3492\n",
            "Epoch 520/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0044 - accuracy: 0.5750\n",
            "Epoch 00520: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0046 - accuracy: 0.5751 - val_loss: 1.7679 - val_accuracy: 0.3780\n",
            "Epoch 521/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0072 - accuracy: 0.5723\n",
            "Epoch 00521: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0075 - accuracy: 0.5722 - val_loss: 1.6434 - val_accuracy: 0.4208\n",
            "Epoch 522/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0074 - accuracy: 0.5703\n",
            "Epoch 00522: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0071 - accuracy: 0.5702 - val_loss: 1.7152 - val_accuracy: 0.3731\n",
            "Epoch 523/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0041 - accuracy: 0.5726\n",
            "Epoch 00523: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0038 - accuracy: 0.5731 - val_loss: 1.7562 - val_accuracy: 0.3630\n",
            "Epoch 524/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0052 - accuracy: 0.5734\n",
            "Epoch 00524: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0052 - accuracy: 0.5734 - val_loss: 1.7136 - val_accuracy: 0.3897\n",
            "Epoch 525/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0033 - accuracy: 0.5741\n",
            "Epoch 00525: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0034 - accuracy: 0.5741 - val_loss: 1.6293 - val_accuracy: 0.4112\n",
            "Epoch 526/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0010 - accuracy: 0.5744\n",
            "Epoch 00526: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0008 - accuracy: 0.5745 - val_loss: 1.7905 - val_accuracy: 0.3502\n",
            "Epoch 527/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0085 - accuracy: 0.5715\n",
            "Epoch 00527: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0083 - accuracy: 0.5717 - val_loss: 1.7214 - val_accuracy: 0.3895\n",
            "Epoch 528/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0092 - accuracy: 0.5720\n",
            "Epoch 00528: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0093 - accuracy: 0.5719 - val_loss: 1.7184 - val_accuracy: 0.3823\n",
            "Epoch 529/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0052 - accuracy: 0.5733\n",
            "Epoch 00529: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0052 - accuracy: 0.5733 - val_loss: 1.8534 - val_accuracy: 0.3111\n",
            "Epoch 530/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0058 - accuracy: 0.5740\n",
            "Epoch 00530: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0056 - accuracy: 0.5742 - val_loss: 1.6568 - val_accuracy: 0.4324\n",
            "Epoch 531/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0111 - accuracy: 0.5684\n",
            "Epoch 00531: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0109 - accuracy: 0.5686 - val_loss: 1.6954 - val_accuracy: 0.3884\n",
            "Epoch 532/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0109 - accuracy: 0.5705\n",
            "Epoch 00532: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0108 - accuracy: 0.5706 - val_loss: 1.8274 - val_accuracy: 0.3219\n",
            "Epoch 533/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0016 - accuracy: 0.5748\n",
            "Epoch 00533: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 3s 11ms/step - loss: 1.0017 - accuracy: 0.5749 - val_loss: 1.6798 - val_accuracy: 0.3989\n",
            "Epoch 534/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0111 - accuracy: 0.5688\n",
            "Epoch 00534: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0111 - accuracy: 0.5688 - val_loss: 1.7169 - val_accuracy: 0.3864\n",
            "Epoch 535/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0087 - accuracy: 0.5707\n",
            "Epoch 00535: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0087 - accuracy: 0.5707 - val_loss: 1.7500 - val_accuracy: 0.3741\n",
            "Epoch 536/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0062 - accuracy: 0.5715\n",
            "Epoch 00536: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0069 - accuracy: 0.5710 - val_loss: 1.7699 - val_accuracy: 0.3542\n",
            "Epoch 537/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0090 - accuracy: 0.5705\n",
            "Epoch 00537: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0091 - accuracy: 0.5705 - val_loss: 1.6936 - val_accuracy: 0.4025\n",
            "Epoch 538/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0090 - accuracy: 0.5705\n",
            "Epoch 00538: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0088 - accuracy: 0.5706 - val_loss: 1.7341 - val_accuracy: 0.3817\n",
            "Epoch 539/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0077 - accuracy: 0.5704\n",
            "Epoch 00539: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0078 - accuracy: 0.5704 - val_loss: 1.6604 - val_accuracy: 0.4061\n",
            "Epoch 540/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0085 - accuracy: 0.5709\n",
            "Epoch 00540: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0081 - accuracy: 0.5715 - val_loss: 1.6960 - val_accuracy: 0.3985\n",
            "Epoch 541/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0028 - accuracy: 0.5749\n",
            "Epoch 00541: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0029 - accuracy: 0.5749 - val_loss: 1.6587 - val_accuracy: 0.4139\n",
            "Epoch 542/1000\n",
            "316/322 [============================>.] - ETA: 0s - loss: 1.0010 - accuracy: 0.5749\n",
            "Epoch 00542: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0015 - accuracy: 0.5749 - val_loss: 1.7430 - val_accuracy: 0.3787\n",
            "Epoch 543/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0043 - accuracy: 0.5734\n",
            "Epoch 00543: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0042 - accuracy: 0.5734 - val_loss: 1.7350 - val_accuracy: 0.3746\n",
            "Epoch 544/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0050 - accuracy: 0.5721\n",
            "Epoch 00544: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0050 - accuracy: 0.5721 - val_loss: 1.6584 - val_accuracy: 0.3971\n",
            "Epoch 545/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0043 - accuracy: 0.5751\n",
            "Epoch 00545: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0042 - accuracy: 0.5751 - val_loss: 1.6771 - val_accuracy: 0.4028\n",
            "Epoch 546/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0021 - accuracy: 0.5733\n",
            "Epoch 00546: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0018 - accuracy: 0.5734 - val_loss: 1.6788 - val_accuracy: 0.3990\n",
            "Epoch 547/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0069 - accuracy: 0.5728\n",
            "Epoch 00547: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0070 - accuracy: 0.5728 - val_loss: 1.6803 - val_accuracy: 0.3989\n",
            "Epoch 548/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0042 - accuracy: 0.5728\n",
            "Epoch 00548: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0039 - accuracy: 0.5729 - val_loss: 1.6997 - val_accuracy: 0.3954\n",
            "Epoch 549/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0056 - accuracy: 0.5726\n",
            "Epoch 00549: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0056 - accuracy: 0.5725 - val_loss: 1.6980 - val_accuracy: 0.3898\n",
            "Epoch 550/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0036 - accuracy: 0.5750\n",
            "Epoch 00550: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0036 - accuracy: 0.5751 - val_loss: 1.7901 - val_accuracy: 0.3443\n",
            "Epoch 551/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0011 - accuracy: 0.5751\n",
            "Epoch 00551: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0009 - accuracy: 0.5751 - val_loss: 1.6955 - val_accuracy: 0.4024\n",
            "Epoch 552/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0038 - accuracy: 0.5740\n",
            "Epoch 00552: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0038 - accuracy: 0.5740 - val_loss: 1.7484 - val_accuracy: 0.3664\n",
            "Epoch 553/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0043 - accuracy: 0.5741\n",
            "Epoch 00553: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0043 - accuracy: 0.5740 - val_loss: 1.7197 - val_accuracy: 0.3740\n",
            "Epoch 554/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.5717\n",
            "Epoch 00554: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0077 - accuracy: 0.5718 - val_loss: 1.8126 - val_accuracy: 0.3410\n",
            "Epoch 555/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0183 - accuracy: 0.5692\n",
            "Epoch 00555: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0179 - accuracy: 0.5693 - val_loss: 1.7517 - val_accuracy: 0.3765\n",
            "Epoch 556/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0112 - accuracy: 0.5677\n",
            "Epoch 00556: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0107 - accuracy: 0.5683 - val_loss: 1.7243 - val_accuracy: 0.3783\n",
            "Epoch 557/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0041 - accuracy: 0.5716\n",
            "Epoch 00557: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0038 - accuracy: 0.5716 - val_loss: 1.7118 - val_accuracy: 0.3791\n",
            "Epoch 558/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0011 - accuracy: 0.5743\n",
            "Epoch 00558: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0012 - accuracy: 0.5742 - val_loss: 1.7570 - val_accuracy: 0.3546\n",
            "Epoch 559/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0020 - accuracy: 0.5743\n",
            "Epoch 00559: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0026 - accuracy: 0.5740 - val_loss: 1.7303 - val_accuracy: 0.3739\n",
            "Epoch 560/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0133 - accuracy: 0.5686\n",
            "Epoch 00560: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0132 - accuracy: 0.5687 - val_loss: 1.7478 - val_accuracy: 0.3639\n",
            "Epoch 561/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0028 - accuracy: 0.5746\n",
            "Epoch 00561: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0028 - accuracy: 0.5746 - val_loss: 1.7717 - val_accuracy: 0.3690\n",
            "Epoch 562/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0009 - accuracy: 0.5758\n",
            "Epoch 00562: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0008 - accuracy: 0.5758 - val_loss: 1.7947 - val_accuracy: 0.3500\n",
            "Epoch 563/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 0.9975 - accuracy: 0.5788\n",
            "Epoch 00563: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 0.9975 - accuracy: 0.5787 - val_loss: 1.7196 - val_accuracy: 0.3772\n",
            "Epoch 564/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0030 - accuracy: 0.5764\n",
            "Epoch 00564: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0030 - accuracy: 0.5764 - val_loss: 1.7904 - val_accuracy: 0.3512\n",
            "Epoch 565/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0077 - accuracy: 0.5705\n",
            "Epoch 00565: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0080 - accuracy: 0.5703 - val_loss: 1.7544 - val_accuracy: 0.3661\n",
            "Epoch 566/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0040 - accuracy: 0.5731\n",
            "Epoch 00566: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0042 - accuracy: 0.5731 - val_loss: 1.7131 - val_accuracy: 0.3888\n",
            "Epoch 567/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 0.9985 - accuracy: 0.5767\n",
            "Epoch 00567: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 0.9985 - accuracy: 0.5767 - val_loss: 1.7980 - val_accuracy: 0.3320\n",
            "Epoch 568/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0049 - accuracy: 0.5732\n",
            "Epoch 00568: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0047 - accuracy: 0.5730 - val_loss: 1.6726 - val_accuracy: 0.4001\n",
            "Epoch 569/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0065 - accuracy: 0.5738\n",
            "Epoch 00569: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 11ms/step - loss: 1.0066 - accuracy: 0.5738 - val_loss: 1.7113 - val_accuracy: 0.3818\n",
            "Epoch 570/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 0.9994 - accuracy: 0.5769\n",
            "Epoch 00570: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 13ms/step - loss: 0.9989 - accuracy: 0.5772 - val_loss: 1.7380 - val_accuracy: 0.3634\n",
            "Epoch 571/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0048 - accuracy: 0.5724\n",
            "Epoch 00571: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 13ms/step - loss: 1.0047 - accuracy: 0.5724 - val_loss: 1.7008 - val_accuracy: 0.3968\n",
            "Epoch 572/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0053 - accuracy: 0.5726\n",
            "Epoch 00572: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0052 - accuracy: 0.5726 - val_loss: 1.6669 - val_accuracy: 0.3961\n",
            "Epoch 573/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0023 - accuracy: 0.5740\n",
            "Epoch 00573: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0025 - accuracy: 0.5739 - val_loss: 1.7783 - val_accuracy: 0.3589\n",
            "Epoch 574/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0037 - accuracy: 0.5721\n",
            "Epoch 00574: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0038 - accuracy: 0.5718 - val_loss: 1.7219 - val_accuracy: 0.3746\n",
            "Epoch 575/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0039 - accuracy: 0.5747\n",
            "Epoch 00575: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0038 - accuracy: 0.5748 - val_loss: 1.6923 - val_accuracy: 0.3821\n",
            "Epoch 576/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0042 - accuracy: 0.5727\n",
            "Epoch 00576: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0044 - accuracy: 0.5726 - val_loss: 1.7984 - val_accuracy: 0.3455\n",
            "Epoch 577/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0075 - accuracy: 0.5718\n",
            "Epoch 00577: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0075 - accuracy: 0.5718 - val_loss: 1.7962 - val_accuracy: 0.3453\n",
            "Epoch 578/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 0.9987 - accuracy: 0.5768\n",
            "Epoch 00578: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 0.9988 - accuracy: 0.5767 - val_loss: 1.7482 - val_accuracy: 0.3760\n",
            "Epoch 579/1000\n",
            "319/322 [============================>.] - ETA: 0s - loss: 1.0097 - accuracy: 0.5689\n",
            "Epoch 00579: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0097 - accuracy: 0.5690 - val_loss: 1.7274 - val_accuracy: 0.3667\n",
            "Epoch 580/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0017 - accuracy: 0.5740\n",
            "Epoch 00580: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0018 - accuracy: 0.5740 - val_loss: 1.6897 - val_accuracy: 0.3815\n",
            "Epoch 581/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0057 - accuracy: 0.5740\n",
            "Epoch 00581: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0057 - accuracy: 0.5740 - val_loss: 1.7951 - val_accuracy: 0.3327\n",
            "Epoch 582/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0011 - accuracy: 0.5771\n",
            "Epoch 00582: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0011 - accuracy: 0.5772 - val_loss: 1.7107 - val_accuracy: 0.3954\n",
            "Epoch 583/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0015 - accuracy: 0.5750\n",
            "Epoch 00583: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0015 - accuracy: 0.5750 - val_loss: 1.7120 - val_accuracy: 0.3882\n",
            "Epoch 584/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0024 - accuracy: 0.5724\n",
            "Epoch 00584: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0027 - accuracy: 0.5727 - val_loss: 1.6947 - val_accuracy: 0.3955\n",
            "Epoch 585/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0004 - accuracy: 0.5760\n",
            "Epoch 00585: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 13ms/step - loss: 1.0005 - accuracy: 0.5759 - val_loss: 1.6730 - val_accuracy: 0.4039\n",
            "Epoch 586/1000\n",
            "318/322 [============================>.] - ETA: 0s - loss: 1.0089 - accuracy: 0.5719\n",
            "Epoch 00586: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0090 - accuracy: 0.5720 - val_loss: 1.6745 - val_accuracy: 0.3996\n",
            "Epoch 587/1000\n",
            "320/322 [============================>.] - ETA: 0s - loss: 1.0016 - accuracy: 0.5753\n",
            "Epoch 00587: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0019 - accuracy: 0.5752 - val_loss: 1.7559 - val_accuracy: 0.3618\n",
            "Epoch 588/1000\n",
            "321/322 [============================>.] - ETA: 0s - loss: 1.0053 - accuracy: 0.5739\n",
            "Epoch 00588: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0053 - accuracy: 0.5739 - val_loss: 1.8080 - val_accuracy: 0.3400\n",
            "Epoch 589/1000\n",
            "322/322 [==============================] - ETA: 0s - loss: 1.0044 - accuracy: 0.5734\n",
            "Epoch 00589: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0044 - accuracy: 0.5734 - val_loss: 1.6941 - val_accuracy: 0.3968\n",
            "Epoch 590/1000\n",
            "317/322 [============================>.] - ETA: 0s - loss: 1.0005 - accuracy: 0.5736\n",
            "Epoch 00590: val_accuracy did not improve from 0.44780\n",
            "322/322 [==============================] - 4s 12ms/step - loss: 1.0006 - accuracy: 0.5736 - val_loss: 1.7670 - val_accuracy: 0.3563\n",
            "Epoch 591/1000\n",
            " 35/322 [==>...........................] - ETA: 2s - loss: 0.9960 - accuracy: 0.5778"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e57bbe19e626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0;31m#validation_steps=30,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelCheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDSyKZrgXdaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec213cd-49ef-4dd6-ad51-1221c5444020"
      },
      "source": [
        "score = model.evaluate(x_test,y_test,verbose=1)\n",
        "print(\"\\n\")\n",
        "print(\"Test loss:\",score[0])\n",
        "print(\"Test accuracy:\",score[1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "461/461 [==============================] - 1s 2ms/step - loss: 1.6393 - accuracy: 0.4195\n",
            "\n",
            "\n",
            "Test loss: 1.639326810836792\n",
            "Test accuracy: 0.41949498653411865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhG1k3kVXkO9"
      },
      "source": [
        "def plot_history(history):\n",
        "    # print(history.history.keys())\n",
        "\n",
        "    # 精度の履歴をプロット\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['accuracy', 'val_accuracy'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # 損失の履歴をプロット\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['loss', 'val_loss'], loc='lower right')\n",
        "    plt.show()\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gyxnkxyXlr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "db73c239-bace-4fe9-d7e2-a7f09b7c55ea"
      },
      "source": [
        "# 学習履歴をプロット\n",
        "plot_history(history)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6565d037e197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 学習履歴をプロット\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}